{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c19884-5476-4152-9c6d-c3a7464732ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PIL\n",
    "import torch\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torchvision import transforms\n",
    "from torchmetrics.functional import accuracy as tm_acc\n",
    "from PIL import Image\n",
    "from fastai.vision.all import *\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18f47fa-113a-4c76-896e-0631a56a1723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fastai to load data with dataloader\n",
    "class CustomTransform(Transform):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        # import dinov2 for embedding creation\n",
    "        dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14') # dinov2_vits14, dinov2_vitb14, dinov2_vitl14, dinov2_vitg14\n",
    "        self.dinov2 = dinov2.to(device)\n",
    "\n",
    "    def encodes(self, x: PIL.Image.Image):\n",
    "        if random.random() < self.p:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(280),\n",
    "                transforms.CenterCrop(280),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(*imagenet_stats)# mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            x = transform(x) # this has to be applied first\n",
    "            x = x.to(device)\n",
    "            # apply your custom transformation here\n",
    "            with torch.no_grad():\n",
    "                embedding = self.dinov2(x.unsqueeze(0))\n",
    "            # x = x\n",
    "        return embedding.squeeze(0)\n",
    "\n",
    "def get_images(dataset_path, batch_size, img_size, seed, subfolders=('train','valid')):\n",
    "    \"The beetles dataset\"\n",
    "    files = get_image_files(path=dataset_path, recurse=True, folders=subfolders)\n",
    "    item_tfms = [CustomTransform(p=1)]\n",
    "    \n",
    "    dblock = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                       get_items = get_image_files,\n",
    "                       splitter = GrandparentSplitter(train_name=subfolders[0], valid_name=subfolders[1]),\n",
    "                       get_y = parent_label,\n",
    "                       item_tfms = item_tfms, # resize trasnformation is applied during inference too                                    \n",
    "                      )\n",
    "    dls = dblock.dataloaders(dataset_path, bs = batch_size, num_workers=0)\n",
    "    return dls\n",
    "\n",
    "class FastaiDataModule(LightningDataModule):\n",
    "    def __init__(self, train_loader, val_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "class MLP(LightningModule):\n",
    "    def __init__(self, input_size, num_classes, weight, hidden_layers=None, learning_rate=1e-3, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_layers[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            for i in range(len(hidden_layers)-1):\n",
    "                layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_layers[-1], num_classes))\n",
    "        else:\n",
    "            layers.append(nn.Linear(input_size, num_classes))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.weight = weight\n",
    "        self.predictions = []\n",
    "        self.labels = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, reduction='mean', label_smoothing=self.label_smoothing, weight=self.weight)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = tm_acc(preds, y)\n",
    "        self.log('train_acc', acc)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, reduction='mean', label_smoothing=self.label_smoothing, weight=self.weight)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = tm_acc(preds, y)\n",
    "        self.predictions.append(preds)\n",
    "        self.labels.append(y)\n",
    "        \n",
    "        # Log loss and metric\n",
    "        self.log('val_acc', acc)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def get_predictions_and_labels(self):\n",
    "        return self.predictions, self.labels\n",
    "\n",
    "def sweep_function():\n",
    "    # initialize wandb run\n",
    "    run = wandb.init()\n",
    "    # Load data\n",
    "    dls = get_images(dataset_path=r\"/blue/hulcr/gmarais/Beetle_data/selected_images/train_data\", batch_size=wandb.config.batch_size, img_size=280, seed=42, subfolders=('train','valid'))\n",
    "    embedding_size = dls.one_batch()[0].shape[1]\n",
    "    labels = np.array([re.split(r'/|\\\\', str(x))[-2] for x in dls.items])\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weights = {c: w for c, w in zip(classes, weights)}\n",
    "    weights = tensor([class_weights[c] for c in dls.vocab]).to(dls.device)\n",
    "    # Create a wandb logger\n",
    "    wandb_logger = WandbLogger(project='DINOv2_sweep')\n",
    "    # Create a trainer and pass the wandb logger\n",
    "    trainer = Trainer(max_epochs=wandb.config.epochs, logger=wandb_logger)\n",
    "    # Get the dataloaders in the correct format\n",
    "    data_module = FastaiDataModule(train_loader=dls.train, val_loader=dls.valid, test_loader=dls.valid)\n",
    "    # Define linear NN model\n",
    "    model = MLP(input_size=embedding_size, num_classes=len(classes), weight=weights, hidden_layers=None, learning_rate=wandb.config.learning_rate, label_smoothing=wandb.config.label_smoothing)\n",
    "    # Fit the model\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    # finish wandb run\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b35b4c-d5e4-4efd-b113-28944d23ec22",
   "metadata": {},
   "source": [
    "# Sweep to tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28abb8d-d1d6-44fe-a471-457fcd4a9b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: v46lz29w\n",
      "Sweep URL: https://wandb.ai/christopher-marais/uncategorized/sweeps/v46lz29w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: up63omd0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlabel_smoothing: 0.6626195003940363\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005711904853087127\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/blue/hulcr/gmarais/Beetle_classifier/DINOv2/wandb/run-20230425_132630-up63omd0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/uncategorized/runs/up63omd0' target=\"_blank\">sparkling-sweep-1</a></strong> to <a href='https://wandb.ai/christopher-marais/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/christopher-marais/uncategorized/sweeps/v46lz29w' target=\"_blank\">https://wandb.ai/christopher-marais/uncategorized/sweeps/v46lz29w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/christopher-marais/uncategorized' target=\"_blank\">https://wandb.ai/christopher-marais/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/christopher-marais/uncategorized/sweeps/v46lz29w' target=\"_blank\">https://wandb.ai/christopher-marais/uncategorized/sweeps/v46lz29w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/christopher-marais/uncategorized/runs/up63omd0' target=\"_blank\">https://wandb.ai/christopher-marais/uncategorized/runs/up63omd0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/gmarais/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/blue/hulcr/gmarais/conda/envs/dinov2/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [7]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 12.3 K\n",
      "--------------------------------------\n",
      "12.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.3 K    Total params\n",
      "0.049     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 41.04it/s]"
     ]
    }
   ],
   "source": [
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "  'method': 'bayes',\n",
    "  'metric': {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'  \n",
    "  },\n",
    "  'parameters': {\n",
    "      'learning_rate': {'min':1e-8,'max': 1e-1},\n",
    "      'batch_size': {'values': [8, 16, 32, 64, 128, 256, 512, 1024, 2048]},\n",
    "      'label_smoothing': {'min': 0.01, 'max': 0.99},\n",
    "      'epochs': {'min': 1, 'max': 10}\n",
    "  }\n",
    "}\n",
    "\n",
    "# Initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config)\n",
    "\n",
    "# Run sweep agent\n",
    "wandb.agent(sweep_id, function=sweep_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02f197-7a5f-41ae-a785-3ffbad38750b",
   "metadata": {},
   "source": [
    "# Single training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9245c3-eccc-464d-9256-270d3f813985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define parameters\n",
    "batch_size=512\n",
    "learning_rate=1e-3\n",
    "label_smoothing=0.1\n",
    "max_epochs=5\n",
    "\n",
    "# prepare datalaodersby extracting features using DINOv2\n",
    "dls = get_images(dataset_path=r\"/blue/hulcr/gmarais/Beetle_data/selected_images/train_data\", batch_size=batch_size, img_size=280, seed=42, subfolders=('train','valid'))\n",
    "embedding_size = dls.one_batch()[0].shape[1]\n",
    "labels = np.array([re.split(r'/|\\\\', str(x))[-2] for x in dls.items])\n",
    "classes = np.unique(labels)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "class_weights = {c: w for c, w in zip(classes, weights)}\n",
    "weights = tensor([class_weights[c] for c in dls.vocab]).to(dls.device)\n",
    "\n",
    "# create a wandb logger\n",
    "wandb_logger = WandbLogger(project='DINOv2_single')\n",
    "\n",
    "# create a trainer and pass the wandb logger\n",
    "trainer = Trainer(logger=wandb_logger)\n",
    "\n",
    "# get the dataloaders in the correct format\n",
    "data_module = FastaiDataModule(train_loader=dls.train, val_loader=dls.valid, test_loader=dls.valid)\n",
    "\n",
    "# define linear NN model\n",
    "model = MLP(input_size=embedding_size, num_classes=len(classes), weight=weights, hidden_layers=None, learning_rate=learning_rate, label_smoothing=label_smoothing) # the dinov2 output is of shape 384\n",
    "\n",
    "# give details on trainer\n",
    "trainer = Trainer(max_epochs=max_epochs, logger=wandb_logger, log_every_n_steps=8)\n",
    "\n",
    "# fit the model\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4551593f-062f-489f-9250-28f5a99e08c1",
   "metadata": {},
   "source": [
    "# 5-Fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca8579-1da2-4b0e-867b-bfd7a5643bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "batch_size=512\n",
    "learning_rate=1e-3\n",
    "label_smoothing=0.1\n",
    "max_epochs=5\n",
    "\n",
    "# CV training\n",
    "predictions_lst = []\n",
    "labels_lst = []\n",
    "for i in range(5):\n",
    "    dls = get_images(dataset_path=r\"/blue/hulcr/gmarais/Beetle_data/selected_images/train_data\", batch_size=batch_size, img_size=280, seed=42, subfolders=('train_'+str(i),'valid_'+str(i)))\n",
    "    embedding_size = dls.one_batch()[0].shape[1]\n",
    "    labels = np.array([re.split(r'/|\\\\', str(x))[-2] for x in dls.items])\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weights = {c: w for c, w in zip(classes, weights)}\n",
    "    weights = tensor([class_weights[c] for c in dls.vocab]).to(dls.device)\n",
    "\n",
    "    # create a wandb logger\n",
    "    wandb_logger = WandbLogger(project='DINOv2_CV')\n",
    "\n",
    "    # create a trainer and pass the wandb logger\n",
    "    trainer = Trainer(logger=wandb_logger)\n",
    "\n",
    "    # get the dataloaders in the correct format\n",
    "    data_module = FastaiDataModule(train_loader=dls.train, val_loader=dls.valid, test_loader=dls.valid)\n",
    "\n",
    "    # define linear NN model\n",
    "    model = MLP(input_size=embedding_size, num_classes=len(classes), weight=weights, hidden_layers=None, learning_rate=learning_rate, label_smoothing=label_smoothing) # the dinov2 output is of shape 384\n",
    "\n",
    "    # give details on trainer\n",
    "    trainer = Trainer(max_epochs=max_epochs, logger=wandb_logger)\n",
    "\n",
    "    # fit the model\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    predictions, labels = model.get_predictions_and_labels()\n",
    "    predictions_lst.append(predictions)\n",
    "    labels_lst.append(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "dinov2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
