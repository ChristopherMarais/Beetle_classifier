{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c403f8d-3cfd-48f5-8a4a-500a03fff3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import wandb\n",
    "import fastai\n",
    "import dill\n",
    "import re\n",
    "import random\n",
    "import PIL\n",
    "import numpy as np\n",
    "from fastai.vision.augment import cutout_gaussian\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.core import *\n",
    "from fastai.text.core import RegexLabeller\n",
    "from fastai.vision.utils import get_image_files\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.data.core import *\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import notebook_login, push_to_hub_fastai, from_pretrained_fastai\n",
    "from torchvision.transforms import GaussianBlur\n",
    "# os.environ['WANDB_WATCH'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2baae5-bf1c-4f7a-bae0-98bb038d313f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    batch_size=128,  #16, #256,\n",
    "    epochs=2,\n",
    "    lr=2e-3,\n",
    "    img_size=256, # 224\n",
    "    seed=42,\n",
    "    pretrained=True,\n",
    "    top_k_losses=5,\n",
    "    model_name=\"maxvit_nano_rw_256.sw_in1k\",# \"maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k\",# try with maxvit_nano_rw_256.sw_in1k # regnetx_040 coatnet_bn_0_rw_224.sw_in1k\n",
    "    wandb_project=\"Beetle_classifier\", \n",
    "    wandb_group=\"ambrosia_symbiosis\",\n",
    "    job_type=\"training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e633d2-718e-4940-9e6e-94ea772337e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a custom transform for Gaussian blur\n",
    "def gaussian_blur(x, p=0.5, kernel_size_min=3, kernel_size_max=20, sigma_min=0.1, sigma_max=3):\n",
    "    if x.ndim == 4:\n",
    "        for i in range(x.shape[0]):\n",
    "            if random.random() < p:\n",
    "                kernel_size = random.randrange(kernel_size_min, kernel_size_max + 1, 2)\n",
    "                sigma = random.uniform(sigma_min, sigma_max)\n",
    "                x[i] = GaussianBlur(kernel_size=kernel_size, sigma=sigma)(x[i])\n",
    "    return x\n",
    "\n",
    "def get_images(dataset_path, batch_size, img_size, seed, subfolders=('train','valid'), exclude_folder=None):\n",
    "    \"The beetles dataset\"\n",
    "    files = get_image_files(path=dataset_path, recurse=True, folders=subfolders)\n",
    "    if exclude_folder:\n",
    "            files = [f for f in files if (exclude_folder not in str(f))]\n",
    "    transforms = aug_transforms(    # transformatiosn that are only applied ot training and not inference\n",
    "                           batch=False,\n",
    "                           pad_mode='zeros',\n",
    "                           size=img_size,\n",
    "                           p_affine=0.8,\n",
    "                           p_lighting=0.8,\n",
    "                           max_rotate=360.0,\n",
    "                           mult=1.0, \n",
    "                           do_flip=True, \n",
    "                           flip_vert=False,\n",
    "                           min_zoom=1.0,\n",
    "                           max_zoom=1.1, \n",
    "                           max_lighting=0.75,\n",
    "                           max_warp=0.2, \n",
    "                           mode='bilinear', \n",
    "                           align_corners=True,\n",
    "                           min_scale=1.0,\n",
    "                           xtra_tfms=[RandomErasing(p=0.8, max_count=5, sh=0.25)]) # this adds random erasing to entire batches\n",
    "    transforms.append(partial(gaussian_blur, p=0.8))\n",
    "    transforms.append(Normalize.from_stats(*imagenet_stats))\n",
    "    dblock = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                       get_items = get_image_files,\n",
    "                       splitter = GrandparentSplitter(train_name=subfolders[0], valid_name=subfolders[1]),\n",
    "                       get_y = parent_label,\n",
    "                       item_tfms = Resize(img_size, ResizeMethod.Pad, pad_mode='zeros'), # resize trasnformation is applied during inference too                                    \n",
    "                       batch_tfms = transforms)\n",
    "    dls = dblock.dataloaders(dataset_path, bs = batch_size)\n",
    "    return dls\n",
    "\n",
    "def train(config, dataset_path, subfolders=('train','valid'), exclude_folder=None):\n",
    "    \"Train the model using the supplied config\"\n",
    "    dls = get_images(dataset_path=dataset_path, \n",
    "                     batch_size=config.batch_size, \n",
    "                     img_size=config.img_size, \n",
    "                     seed=config.seed, \n",
    "                     subfolders=subfolders, \n",
    "                     exclude_folder=exclude_folder)\n",
    "    labels = np.array([re.split(r'/|\\\\', str(x))[-2] for x in dls.items])\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weights = {c: w for c, w in zip(classes, weights)}\n",
    "    weights = tensor([class_weights[c] for c in dls.vocab]).to(dls.device)\n",
    "    # wandb.init(project=config.wandb_project, group=config.wandb_group, job_type=config.job_type, config=config) # it is a good idea to keep these functions out of the training function due to some exporting issues\n",
    "    cbs = [MixedPrecision(), ShowGraphCallback(), SaveModelCallback(), WandbCallback(log='gradients')] # (all, parameters, gradients or None) parameters and all does nto work currently wandb needs to be updated\n",
    "    learn = vision_learner(dls, \n",
    "                           config.model_name, \n",
    "                           loss_func=LabelSmoothingCrossEntropy(weight=weights), # this fucntion is used for class imbalance it is a regularization technique # LabelSmoothingCrossEntropyFlat is used for multi dimensional data\n",
    "                           metrics=[error_rate, \n",
    "                                    accuracy, \n",
    "                                    top_k_accuracy], \n",
    "                           cbs=cbs, \n",
    "                           pretrained=config.pretrained)\n",
    "    learn.fine_tune(config.epochs, base_lr=config.lr)\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    interp.plot_confusion_matrix()\n",
    "    interp.plot_top_losses(config.top_k_losses, nrows=config.top_k_losses)\n",
    "    # wandb.finish() # it is a good idea to keep these functions out of the training function due to some exporting issues\n",
    "    return learn\n",
    "\n",
    "# this function only describes how much a singular value in al ist stands out.\n",
    "# if all values in the lsit are high or low this is 1\n",
    "# the smaller the proportiopn of number of disimilar vlaues are to other more similar values the lower this number\n",
    "# the larger the gap between the dissimilar numbers and the simialr number the smaller this number\n",
    "# only able to interpret probabilities or values between 0 and 1\n",
    "# this function outputs an estimate an inverse of the classification confidence based on the probabilities of all the classes.\n",
    "# the wedge threshold splits the data on a threshold with a magnitude of a positive int to force a ledge/peak in the data\n",
    "def unkown_prob_calc(probs, wedge_threshold, wedge_magnitude=1, wedge='strict'):\n",
    "    if wedge =='strict':\n",
    "        increase_var = (1/(wedge_magnitude))\n",
    "        decrease_var = (wedge_magnitude)\n",
    "    if wedge =='dynamic': # this allows pointsthat are furhter from the threshold ot be moved less and points clsoer to be moved more\n",
    "        increase_var = (1/(wedge_magnitude*((1-np.abs(probs-wedge_threshold)))))\n",
    "        decrease_var = (wedge_magnitude*((1-np.abs(probs-wedge_threshold))))\n",
    "    # else:\n",
    "    #     print(\"Error: use 'strict' (default) or 'dynamic' as options for the wedge parameter!\")\n",
    "    probs = np.where(probs>=wedge_threshold , probs**increase_var, probs)\n",
    "    probs = np.where(probs<=wedge_threshold , probs**decrease_var, probs)\n",
    "    diff_matrix = np.abs(probs[:, np.newaxis] - probs)\n",
    "    diff_matrix_sum = np.sum(diff_matrix)\n",
    "    probs_sum = np.sum(probs)\n",
    "    class_val = (diff_matrix_sum/probs_sum)\n",
    "    max_class_val = ((len(probs)-1)*2)\n",
    "    kown_prob = class_val/max_class_val\n",
    "    unknown_prob = 1-kown_prob\n",
    "    return(unknown_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f127d-ce29-44cb-8f02-19e5454ef5cb",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d677984-84cd-4344-8929-6deb33e8aac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/blue/hulcr/gmarais/Beetle_classifier/Train/wandb/run-20230506_031324-xxkn6srr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/Beetle_classifier/runs/xxkn6srr' target=\"_blank\">sage-vortex-49</a></strong> to <a href='https://wandb.ai/christopher-marais/Beetle_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/christopher-marais/Beetle_classifier' target=\"_blank\">https://wandb.ai/christopher-marais/Beetle_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/christopher-marais/Beetle_classifier/runs/xxkn6srr' target=\"_blank\">https://wandb.ai/christopher-marais/Beetle_classifier/runs/xxkn6srr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/hulcr/gmarais/conda/envs/BC_310/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>top_k_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='164' class='' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      84.10% [164/195 01:08&lt;00:13 1.9923]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Model\n",
    "dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/selected_images/train_data\"\n",
    "# dataset_path = r\"F:\\Beetle_data\\selected_images\\train_data\"\n",
    "wandb.init(project=config.wandb_project, group=config.wandb_group, job_type=config.job_type, config=config)\n",
    "learn = train(config=config, dataset_path=dataset_path, exclude_folder=None)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba4082-af8d-432d-9c55-372a77332a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn.remove_cb(WandbCallback)\n",
    "lrs = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n",
    "plt.savefig('Learning_rate_find.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450bc27-0910-4b5e-b6df-95cc93085dea",
   "metadata": {},
   "source": [
    "Training images: 32.469 (~25000 training and ~7500 validation [80/20 split])\n",
    "Testing Images: 4610"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045802a9-049d-4be7-9c10-105e30a7c5ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save Model Locally and to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec139e7c-a844-49fe-b507-c77931492ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model to disk for inference\n",
    "learn.path = Path(\"/blue/hulcr/gmarais/Beetle_classifier/Models\")\n",
    "# learn.path = Path(\"F:\\Beetle_data\\Models\")\n",
    "learn.remove_cb(WandbCallback) # remove WandbCallbacks to allow prediction model to be applied without wandb\n",
    "# wandb.unwatch(learn.model)\n",
    "learn.export('beetle_classifier.pkl')#, pickle_module=dill) # use learn.save to save model and continue training later\n",
    "\n",
    "# load to huggingface hub\n",
    "repo_id=\"ChristopherMarais/beetle-model\"\n",
    "push_to_hub_fastai(learner=learn, repo_id=repo_id, token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6928116-f083-4831-8120-5995715ef049",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd667977-042e-4100-a8bf-922e12c202d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from huggingface Hub\n",
    "repo_id=\"ChristopherMarais/beetle-model\"\n",
    "learn = from_pretrained_fastai(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e80506-d4f8-428f-94e4-563865cfdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import testing data\n",
    "dataset_path=r\"/blue/hulcr/gmarais/Beetle_data/selected_images\"\n",
    "# dataset_path=r\"F:\\Beetle_data\\selected_images\"\n",
    "files = get_image_files(path=dataset_path, recurse=True, folders=('test_data')) # get files from directory\n",
    "test_dl = learn.dls.test_dl(files, with_labels=True) # load data as a dataloader\n",
    "preds, targets = learn.get_preds(dl=test_dl)\n",
    "\n",
    "# get names of classes\n",
    "class_lst = list(learn.dls.vocab)\n",
    "class_lst.append(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f948b-a7ed-4d75-96d7-efb760771d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate ROC curve for classifier\n",
    "threshold_resolution = 0.001\n",
    "wedge_magnitude=2\n",
    "fpr_lst = []\n",
    "tpr_lst = []\n",
    "threshold_arr = np.arange(0,1+threshold_resolution,threshold_resolution)\n",
    "for i in threshold_arr:\n",
    "    # add unknown class probability\n",
    "    unknown_preds = np.apply_along_axis(unkown_prob_calc, axis=1, arr=np.array(preds), wedge_threshold=i, wedge_magnitude=wedge_magnitude) # calculate unknown class probability\n",
    "    full_preds = torch.from_numpy(np.concatenate((np.array(preds), unknown_preds[:, np.newaxis]), axis=1)) # add probability to estimates\n",
    "    preds_probs, preds_class = torch.max(full_preds, axis=1)\n",
    "    cnf_matrix = confusion_matrix(y_true=np.array(targets), y_pred=np.array(preds_class))\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    FPR = FP/(FP+TN)\n",
    "    fpr_lst.append(FPR)\n",
    "    TPR = TP/(TP+FN + 1e-6) # add 1e-6 to avoid the Nan value that happends when class not in targets\n",
    "    tpr_lst.append(TPR)\n",
    "\n",
    "# create dataframe with information in it\n",
    "# get column names for each dataframe\n",
    "FPR_class_lst = [x + \"_FPR\" for x in class_lst]\n",
    "TPR_class_lst = [x + \"_TPR\" for x in class_lst]\n",
    "df_fpr = pd.DataFrame(fpr_lst, columns=FPR_class_lst)\n",
    "df_tpr = pd.DataFrame(tpr_lst, columns=TPR_class_lst)\n",
    "df_ROC = pd.concat([df_tpr, df_fpr], axis=1) # get all data in one df\n",
    "df_ROC['threshold'] = threshold_arr # add threshold column\n",
    "ROC_lst = FPR_class_lst + TPR_class_lst\n",
    "df_ROC = df_ROC.drop_duplicates(subset=ROC_lst) # get rid of all unecessary duplicates\n",
    "df_ROC = df_ROC.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bea8da-6ab3-4981-b256-0388dadded23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100c53b-702b-4f53-8585-cfcb84b065eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize ROC curve\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Create an empty figure, and iteratively add new lines\n",
    "# every time we compute a new class\n",
    "fig = go.Figure()\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "\n",
    "for i in class_lst:\n",
    "    # get data from dataframe\n",
    "    fpr = df_ROC[i+\"_FPR\"].tolist()\n",
    "    fpr.insert(0, 0)\n",
    "    fpr.insert(-1, 1)\n",
    "    fpr.sort()\n",
    "    tpr = df_ROC[i+\"_TPR\"].tolist()\n",
    "    tpr.insert(0, 0)\n",
    "    tpr.insert(-1, 1)\n",
    "    tpr.sort()\n",
    "    \n",
    "    auc_score = auc(fpr, tpr)\n",
    "\n",
    "    name = f\"{i} (AUC={auc_score:.2f})\"\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=i, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    width=1000, height=1000\n",
    ")\n",
    "fig.show()\n",
    "# Save the figure as an HTML file\n",
    "pyo.plot(fig, filename='ROC_curve.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c44a7-5730-4116-9444-969c16a5a5e6",
   "metadata": {},
   "source": [
    "selecting a threshold is practically irrelevant but os far the bst one is at ~0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ee2f2-e70a-46f0-8c65-13504865dc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import model again to test\n",
    "# learn = load_learner(Path(\"/blue/hulcr/gmarais/Beetle_classifier/Models\") / 'beetle_classifier.pkl', cpu=False, pickle_module=dill)\n",
    "# print(learn.dls.vocab) # print all possible classes of model\n",
    "\n",
    "# import testing data\n",
    "dataset_path=r\"/blue/hulcr/gmarais/Beetle_data/selected_images\"\n",
    "# dataset_path=r\"F:\\Beetle_data\\selected_images\"\n",
    "files = get_image_files(path=dataset_path, recurse=True, folders=('test_data')) # get files from directory\n",
    "test_dl = learn.dls.test_dl(files, with_labels=True) # load data as a dataloader\n",
    "\n",
    "preds, targets = learn.get_preds(dl=test_dl)\n",
    "val_out = learn.validate(dl=test_dl)\n",
    "print(\" Loss: \"+str(val_out[0])+\"\\n\",\n",
    "      \"Error Rate: \"+str(val_out[1])+\"\\n\",\n",
    "      \"Accuracy: \"+str(val_out[2])+\"\\n\",\n",
    "      \"Top k(5) Accuracy: \"+str(val_out[3])+\"\\n\")\n",
    "\n",
    "# add unknown class probability\n",
    "unknown_preds = np.apply_along_axis(unkown_prob_calc, axis=1, arr=np.array(preds), wedge_threshold=0.5) # calculate unknown class probability\n",
    "full_preds = torch.from_numpy(np.concatenate((np.array(preds), unknown_preds[:, np.newaxis]), axis=1)) # add probability to estimates\n",
    "preds_probs, preds_class = torch.max(full_preds, axis=1)\n",
    "\n",
    "\n",
    "# plot confusion matrix\n",
    "arr_cm = confusion_matrix(y_true=np.array(targets), y_pred=np.array(preds_class))\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(arr_cm, cmap='Blues')\n",
    "ax.set_xticks(range(0,11))\n",
    "ax.set_xticklabels(class_lst)\n",
    "ax.set_yticks(range(0,11))\n",
    "ax.set_yticklabels(class_lst)\n",
    "# Get the colormap colors\n",
    "my_cmap = im.cmap(im.norm(arr_cm))\n",
    "for i in range(arr_cm.shape[0]):\n",
    "    for j in range(arr_cm.shape[1]):\n",
    "        # Get the RGB color of the cell\n",
    "        rgba = my_cmap[i, j]\n",
    "        # If the cell is dark, use white text; otherwise, use black text\n",
    "        text_color = 'w' if rgba[:3].mean() < 0.5 else 'k'\n",
    "        ax.text(j, i, arr_cm[i, j], ha='center', va='center', color=text_color)\n",
    "plt.title(\"Test Confusion Matrix\")        \n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf7583-9122-437a-b976-cc572e89cbd3",
   "metadata": {},
   "source": [
    "# 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb477d-1dc2-4319-88d0-39f6e7ac4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV training\n",
    "valid_targets = []\n",
    "valid_preds = []\n",
    "test_targets = []\n",
    "test_preds = []\n",
    "for i in range(5):\n",
    "    # Train Model\n",
    "    dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/selected_images/train_data\"\n",
    "    # dataset_path = r\"F:\\Beetle_data\\selected_images\\train_data\"\n",
    "    wandb.init(project=config.wandb_project, group=config.wandb_group, job_type=config.job_type, config=config)\n",
    "    learn = train(config=config, dataset_path=dataset_path, subfolders=('train_'+str(i),'valid_'+str(i)))\n",
    "    wandb.finish()\n",
    "    # get predicstions and labels\n",
    "    # Get validation labels and predictions\n",
    "    files = get_image_files(path=dataset_path, recurse=True, folders=('valid_'+str(i))) # get files from directory\n",
    "    test_dl = learn.dls.test_dl(files, with_labels=True) # load data as a dataloader\n",
    "    preds, targets = learn.get_preds(dl=test_dl)\n",
    "    valid_targets.append(targets)\n",
    "    valid_preds.append(preds)\n",
    "    \n",
    "    # testing data\n",
    "    files = get_image_files(path=r\"/blue/hulcr/gmarais/Beetle_data/selected_images\", recurse=True, folders=('test_data')) # get files from directory\n",
    "    test_dl = learn.dls.test_dl(files, with_labels=True) # load data as a dataloader\n",
    "    preds, targets = learn.get_preds(dl=test_dl)\n",
    "    test_targets.append(targets)\n",
    "    test_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d16b0-7e56-47a0-a34d-751a8696432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save CV results to dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269f759-4929-4f6f-a3d7-9107d41e21d3",
   "metadata": {},
   "source": [
    "# Out of Class classification evaluation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157b480-ceba-4998-af7d-81c914e1055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/selected_images/train_data\"\n",
    "# dataset_path = r\"F:\\Beetle_data\\selected_images\\train_data\"\n",
    "wandb.init(project=config.wandb_project, group=config.wandb_group, job_type=config.job_type, config=config)\n",
    "learn = train(config=config, dataset_path=dataset_path, exclude_folder='Coccotypes_dactyliperda')\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BC_310",
   "language": "python",
   "name": "bc_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
