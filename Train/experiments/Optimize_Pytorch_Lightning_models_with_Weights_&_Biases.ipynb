{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmv7xMP7SuB8"
   },
   "source": [
    "Pytorch Lightning is a lightweight wrapper for organizing your PyTorch code and easily adding advanced features such as distributed training, 16-bit precision or gradient accumulation.\n",
    "\n",
    "Coupled with the [Weights & Biases integration](https://docs.wandb.com/library/integrations/lightning), you can quickly train and monitor models for full traceability and reproducibility with only 2 extra lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T02:53:44.392567Z",
     "iopub.status.busy": "2023-03-03T02:53:44.392068Z",
     "iopub.status.idle": "2023-03-03T02:53:52.065564Z",
     "shell.execute_reply": "2023-03-03T02:53:52.062565Z",
     "shell.execute_reply.started": "2023-03-03T02:53:44.392567Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1129\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1128\u001b[0m wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[1;32m-> 1129\u001b[0m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m wi\u001b[38;5;241m.\u001b[39msettings\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:164\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprinter\u001b[38;5;241m.\u001b[39mdisplay(line, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl \u001b[38;5;241m=\u001b[39m \u001b[43mwandb_setup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Make sure we have a logger setup (might be an early logger)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py:307\u001b[0m, in \u001b[0;36msetup\u001b[1;34m(settings)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_WandbSetup\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 307\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py:302\u001b[0m, in \u001b[0;36m_setup\u001b[1;34m(settings, _reset)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m wl \u001b[38;5;241m=\u001b[39m \u001b[43m_WandbSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wl\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py:288\u001b[0m, in \u001b[0;36m_WandbSetup.__init__\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m _WandbSetup\u001b[38;5;241m.\u001b[39m_instance \u001b[38;5;241m=\u001b[39m \u001b[43m_WandbSetup__WandbSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py:100\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup.__init__\u001b[1;34m(self, pid, settings, environ)\u001b[0m\n\u001b[0;32m     98\u001b[0m _set_logger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_logger)\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_early_logger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# self._settings.freeze()\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py:128\u001b[0m, in \u001b[0;36m_WandbSetup__WandbSetup._settings_setup\u001b[1;34m(self, settings, early_logger)\u001b[0m\n\u001b[0;32m    126\u001b[0m     s\u001b[38;5;241m.\u001b[39m_apply_setup(settings, _logger\u001b[38;5;241m=\u001b[39mearly_logger)\n\u001b[1;32m--> 128\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_settings_from_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s\u001b[38;5;241m.\u001b[39m_cli_only_mode:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\sdk\\wandb_settings.py:1390\u001b[0m, in \u001b[0;36mSettings._infer_settings_from_environment\u001b[1;34m(self, _logger)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotebook_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1390\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjupyter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m     settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jupyter_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\jupyter.py:228\u001b[0m, in \u001b[0;36mnotebook_metadata\u001b[1;34m(silent)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: ipynb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: ipynb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    226\u001b[0m         }\n\u001b[1;32m--> 228\u001b[0m jupyter_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mnotebook_metadata_from_jupyter_servers_and_kernel_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jupyter_metadata:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\jupyter.py:160\u001b[0m, in \u001b[0;36mnotebook_metadata_from_jupyter_servers_and_kernel_id\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnotebook_metadata_from_jupyter_servers_and_kernel_id\u001b[39m():\n\u001b[1;32m--> 160\u001b[0m     servers, kernel_id \u001b[38;5;241m=\u001b[39m \u001b[43mjupyter_servers_and_kernel_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m servers:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\jupyter.py:257\u001b[0m, in \u001b[0;36mjupyter_servers_and_kernel_id\u001b[1;34m()\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m serverapp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 257\u001b[0m     servers\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(\u001b[43mserverapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_running_servers\u001b[49m()))\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m notebookapp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\importlib\\util.py:247\u001b[0m, in \u001b[0;36m_LazyModule.__getattribute__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    246\u001b[0m         attrs_updated[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__spec__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# If exec_module() was used directly there is no guarantee the module\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# object was put into sys.modules.\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\jupyter_server\\serverapp.py:116\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyter_server\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilemanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    113\u001b[0m     AsyncFileContentsManager,\n\u001b[0;32m    114\u001b[0m     FileContentsManager,\n\u001b[0;32m    115\u001b[0m )\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyter_server\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlargefilemanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LargeFileManager\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyter_server\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    118\u001b[0m     AsyncContentsManager,\n\u001b[0;32m    119\u001b[0m     ContentsManager,\n\u001b[0;32m    120\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WandbLogger\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m----> 4\u001b[0m wandb_logger \u001b[38;5;241m=\u001b[39m \u001b[43mWandbLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(logger\u001b[38;5;241m=\u001b[39mwandb_logger)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:357\u001b[0m, in \u001b[0;36mWandbLogger.__init__\u001b[1;34m(self, name, save_dir, version, offline, dir, id, anonymous, project, log_model, experiment, prefix, checkpoint_name, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _WANDB_GREATER_EQUAL_0_12_10:\n\u001b[0;32m    356\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mrequire(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 357\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_name \u001b[38;5;241m=\u001b[39m checkpoint_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\lightning_fabric\\loggers\\logger.py:117\u001b[0m, in \u001b[0;36mrank_zero_experiment.<locals>.experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@rank_zero_only\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m _DummyExperiment()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\lightning_utilities\\core\\rank_zero.py:24\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\lightning_fabric\\loggers\\logger.py:115\u001b[0m, in \u001b[0;36mrank_zero_experiment.<locals>.experiment.<locals>.get_experiment\u001b[1;34m()\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@rank_zero_only\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_experiment\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:405\u001b[0m, in \u001b[0;36mWandbLogger.experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39m_attach(attach_id)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# create new wandb process\u001b[39;00m\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb_init)\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# define default x-axis\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment, (Run, RunDisabled)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefine_metric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1153\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1153\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n\u001b[0;32m   1154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "wandb_logger = WandbLogger()\n",
    "trainer = Trainer(logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDIZdR8qSuB8"
   },
   "source": [
    "W&B integration with Pytorch-Lightning can automatically:\n",
    "* log your configuration parameters\n",
    "* log your losses and metrics\n",
    "* log your model\n",
    "* keep track of your code\n",
    "* log your system metrics (GPU, CPU, memory, temperature, etc)\n",
    "\n",
    "### üìö Docs\n",
    "You can find the PyTorch Lightning WandbLogger docs [here](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.loggers.WandbLogger.html?highlight=wandblogger) and the Weights & Biases docs [here](https://docs.wandb.com/library/integrations/lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOM58ImkSuB_"
   },
   "source": [
    "### üõ†Ô∏è Installation and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.067063Z",
     "iopub.status.idle": "2023-03-03T02:53:52.068064Z",
     "shell.execute_reply": "2023-03-03T02:53:52.068064Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.068064Z"
    },
    "id": "zIOoAOVrSuB_",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q pytorch-lightning wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPKgb9R6SuCB"
   },
   "source": [
    "We make sure we're logged into W&B so that our experiments can be associated with our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.071064Z",
     "iopub.status.idle": "2023-03-03T02:53:52.072064Z",
     "shell.execute_reply": "2023-03-03T02:53:52.071564Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.071564Z"
    },
    "id": "9K3LS0PlSuCC",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTRkr3AoSuCC"
   },
   "source": [
    "## üìä Setting up the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVHMiAYoSuCD"
   },
   "source": [
    "For the context of this tutorial we use vanilla pytorch dataloaders on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.073563Z",
     "iopub.status.idle": "2023-03-03T02:53:52.074565Z",
     "shell.execute_reply": "2023-03-03T02:53:52.074063Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.074063Z"
    },
    "id": "LtC3tqgLSuCD",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "dataset = MNIST(root=\"./MNIST\", download=True, transform=transform)\n",
    "training_set, validation_set = random_split(dataset, [55000, 5000])\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.076069Z",
     "iopub.status.idle": "2023-03-03T02:53:52.077066Z",
     "shell.execute_reply": "2023-03-03T02:53:52.076567Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.076567Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir='./', batch_size=256):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        '''called only once and on 1 GPU'''\n",
    "        # download data\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''called on each GPU separately - stage defines if we are at fit or test step'''\n",
    "        # we set up only relevant datasets when stage is specified (automatically set by Pytorch-Lightning)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_train = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        '''returns training dataloader'''\n",
    "        mnist_train = DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle = True)#, num_workers = cpu_count)\n",
    "        return mnist_train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        '''returns validation dataloader'''\n",
    "        mnist_val = DataLoader(self.mnist_val, batch_size=self.batch_size, shuffle = True)#, num_workers = cpu_count)\n",
    "        return mnist_val\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        '''returns test dataloader'''\n",
    "        mnist_test = DataLoader(self.mnist_test, batch_size=self.batch_size, shuffle = True)#, num_workers = cpu_count)\n",
    "        return mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.079064Z",
     "iopub.status.idle": "2023-03-03T02:53:52.079564Z",
     "shell.execute_reply": "2023-03-03T02:53:52.079564Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.079564Z"
    }
   },
   "outputs": [],
   "source": [
    "# declaring the path of the train and test folders\n",
    "train_path = \"DATASET/TRAIN\"\n",
    "test_path = \"DATASET/TEST\"\n",
    "classes_dir_data = os.listdir(train_path)\n",
    "num_of_classes = len(classes_dir_data)\n",
    "print(\"Total Number of Classes :\" , num_of_classes)\n",
    "num = 0\n",
    "classes_dict = {}\n",
    "classes_lst = []\n",
    "num_dict = {}\n",
    "for c in  classes_dir_data:\n",
    "    classes_dict[c] = num\n",
    "    num_dict[num] = c\n",
    "    classes_lst.append(c)\n",
    "    num = num +1\n",
    "\"\"\"\n",
    "num_dict contains a dictionary of the classes numerically and it's corresponding classes.\n",
    "classes_dict contains a dictionary of the classes and the coresponding values numerically.\n",
    "\"\"\"\n",
    "num_of_classes = len(classes_dir_data)\n",
    "\n",
    "classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.081564Z",
     "iopub.status.idle": "2023-03-03T02:53:52.082062Z",
     "shell.execute_reply": "2023-03-03T02:53:52.082062Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.082062Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating the dataset\n",
    "\n",
    "#dataset\n",
    "\n",
    "class Image_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self,classes,image_base_dir,transform = None, target_transform = None):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        classes:The classes in the dataset\n",
    "\n",
    "        image_base_dir:The directory of the folders containing the images\n",
    "\n",
    "        transform:The trasformations for the Images\n",
    "\n",
    "        Target_transform:The trasformations for the target\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.img_labels = classes\n",
    "\n",
    "        self.imge_base_dir = image_base_dir\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        img_dir_list = os.listdir(os.path.join(self.imge_base_dir,self.img_labels[idx]))\n",
    "\n",
    "        image_path = img_dir_list[randint(0,len(img_dir_list)-1)]\n",
    "\n",
    "        #print(image_path)\n",
    "\n",
    "        image_path = os.path.join(self.imge_base_dir,self.img_labels[idx],image_path)\n",
    "\n",
    "        image = skimage.io.imread(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            label = self.target_transform(self.img_labels[idx])\n",
    "\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.084064Z",
     "iopub.status.idle": "2023-03-03T02:53:52.085064Z",
     "shell.execute_reply": "2023-03-03T02:53:52.084564Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.084564Z"
    }
   },
   "outputs": [],
   "source": [
    "size = 50 # need to be the same as what is used in layer_5/ input layer ot the cnn\n",
    "\n",
    "basic_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((size,size)),\n",
    "        transforms.Grayscale(1),\n",
    "    transforms.ToTensor()])\n",
    "training_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((size,size)),\n",
    "    transforms.RandomRotation(degrees = 45),\n",
    "    transforms.RandomHorizontalFlip(p = 0.005),\n",
    "        transforms.Grayscale(1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def target_transformations(x):\n",
    "    return torch.tensor(classes_dict.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL03mttlSuCE"
   },
   "source": [
    "## ü§ì Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZ2mDCQ-SuCE"
   },
   "source": [
    "**Tips**:\n",
    "* Call `self.save_hyperparameters()` in `__init__` to automatically log your hyperparameters to **W&B**\n",
    "* Call self.log in `training_step` and `validation_step` to log the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.086563Z",
     "iopub.status.idle": "2023-03-03T02:53:52.087066Z",
     "shell.execute_reply": "2023-03-03T02:53:52.086563Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.086563Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, CrossEntropyLoss, functional as F\n",
    "from torch.optim import Adam\n",
    "from torchmetrics.functional import accuracy\n",
    "from pytorch_lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.088563Z",
     "iopub.status.idle": "2023-03-03T02:53:52.089564Z",
     "shell.execute_reply": "2023-03-03T02:53:52.089065Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.089065Z"
    },
    "id": "xL_JWvg9SuCE",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class MNIST_LitModule(LightningModule): \n",
    "\n",
    "    def __init__(self, n_classes=10, n_layer_1=128, n_layer_2=256, lr=1e-3): #############################################################################\n",
    "        '''method used to define our model parameters'''\n",
    "        super().__init__()\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)#############################################################################\n",
    "        self.layer_1 = Linear(28 * 28, n_layer_1)#############################################################################\n",
    "        self.layer_2 = Linear(n_layer_1, n_layer_2)#############################################################################\n",
    "        self.layer_3 = Linear(n_layer_2, n_classes)#############################################################################\n",
    "\n",
    "        # loss\n",
    "        self.loss = CrossEntropyLoss()#############################################################################\n",
    "\n",
    "        # optimizer parameters\n",
    "        self.lr = lr#############################################################################\n",
    "\n",
    "        # save hyper-parameters to self.hparams (auto-logged by W&B)\n",
    "        self.save_hyperparameters() \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''method used for inference input -> output'''\n",
    "\n",
    "        batch_size, channels, width, height = x.size()\n",
    "\n",
    "        # (b, 1, 28, 28) -> (b, 1*28*28)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # let's do 3 x (linear + relu)\n",
    "        x = self.layer_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''needs to return a loss from a single batch'''\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        # Log loss and metric\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_accuracy', acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        # Log loss and metric\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_accuracy', acc)\n",
    "\n",
    "        # Let's return preds to use it in a custom callback\n",
    "        return preds\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        # Log loss and metric\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_accuracy', acc)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        '''defines model optimizer'''\n",
    "        return Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def _get_preds_loss_accuracy(self, batch):\n",
    "        '''convenience function since train/valid/test steps are similar'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        loss = self.loss(logits, y)\n",
    "        acc = accuracy(preds, y, 'multiclass', num_classes=10)\n",
    "        return preds, loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3wR351_SuCF"
   },
   "source": [
    "The model is now ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.091063Z",
     "iopub.status.idle": "2023-03-03T02:53:52.091562Z",
     "shell.execute_reply": "2023-03-03T02:53:52.091063Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.091063Z"
    },
    "id": "vNGDhqpnSuCF",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = MNIST_LitModule(n_layer_1=128, n_layer_2=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0t6MpopSuCG"
   },
   "source": [
    "## üíæ Save Model Checkpoints\n",
    "\n",
    "The `ModelCheckpoint` callback is required along with the `WandbLogger` argument to log model checkpoints to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.093065Z",
     "iopub.status.idle": "2023-03-03T02:53:52.094071Z",
     "shell.execute_reply": "2023-03-03T02:53:52.093564Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.093564Z"
    },
    "id": "d84LZk-6SuCG",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_accuracy', mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G69Pss7VSuCG"
   },
   "source": [
    "##¬†üí° Tracking Experiments with WandbLogger\n",
    "\n",
    "PyTorch Lightning has a `WandbLogger` to easily log your experiments with Wights & Biases. Just pass it to your `Trainer` to log to W&B. See the [WandbLogger docs](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html#pytorch_lightning.loggers.WandbLogger) for all parameters. Note, to log the metrics to a specific W&B Team, pass your Team name to the `entity` argument in `WandbLogger`\n",
    "\n",
    "#### `pytorch_lightning.loggers.WandbLogger()`\n",
    "\n",
    "| Functionality | Argument/Function | PS |\n",
    "| ------ | ------ | ------ |\n",
    "| Logging models | `WandbLogger(... ,log_model='all')` or `WandbLogger(... ,log_model=True`) | Log all models if `log_model=\"all\"` and at end of training if `log_model=True`\n",
    "| Set custom run names | `WandbLogger(... ,name='my_run_name'`) | |\n",
    "| Organize runs by project | `WandbLogger(... ,project='my_project')` | |\n",
    "| Log histograms of gradients and parameters | `WandbLogger.watch(model)`  | `WandbLogger.watch(model, log='all')` to log parameter histograms  |\n",
    "| Log hyperparameters | Call `self.save_hyperparameters()` within `LightningModule.__init__()` |\n",
    "| Log custom objects (images, audio, video, molecules‚Ä¶) | Use `WandbLogger.log_text`, `WandbLogger.log_image` and `WandbLogger.log_table` |\n",
    "\n",
    "See the [WandbLogger docs](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html#pytorch_lightning.loggers.WandbLogger) here for all parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.096065Z",
     "iopub.status.idle": "2023-03-03T02:53:52.097064Z",
     "shell.execute_reply": "2023-03-03T02:53:52.096563Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.096563Z"
    },
    "id": "8zNfGYR7SuCH",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "wandb_logger = WandbLogger(project='MNIST', # group runs in \"MNIST\" project\n",
    "                           log_model='all') # log all new checkpoints during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyquRSkhSuCH"
   },
   "source": [
    "## ‚öôÔ∏è Using WandbLogger to log Images, Text and More\n",
    "Pytorch Lightning is extensible through its callback system. We can create a custom callback to automatically log sample predictions during validation. `WandbLogger` provides convenient media logging functions:\n",
    "* `WandbLogger.log_text` for text data\n",
    "* `WandbLogger.log_image` for images\n",
    "* `WandbLogger.log_table` for [W&B Tables](https://docs.wandb.ai/guides/data-vis).\n",
    "\n",
    "An alternate to `self.log` in the Model class is directly using `wandb.log({dict})` or `trainer.logger.experiment.log({dict})`\n",
    "\n",
    "In this case we log the first 20 images in the first batch of the validation dataset along with the predicted and ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.098564Z",
     "iopub.status.idle": "2023-03-03T02:53:52.099065Z",
     "shell.execute_reply": "2023-03-03T02:53:52.099065Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.099065Z"
    },
    "id": "M9-321mOSuCH",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    " \n",
    "class LogPredictionsCallback(Callback):\n",
    "    \n",
    "    def on_validation_batch_end(\n",
    "        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        \"\"\"Called when the validation batch ends.\"\"\"\n",
    " \n",
    "        # `outputs` comes from `LightningModule.validation_step`\n",
    "        # which corresponds to our model predictions in this case\n",
    "        \n",
    "        # Let's log 20 sample image predictions from first batch\n",
    "        if batch_idx == 0:\n",
    "            n = 20\n",
    "            x, y = batch\n",
    "            images = [img for img in x[:n]]\n",
    "            captions = [f'Ground Truth: {y_i} - Prediction: {y_pred}' for y_i, y_pred in zip(y[:n], outputs[:n])]\n",
    "            \n",
    "            # Option 1: log images with `WandbLogger.log_image`\n",
    "            wandb_logger.log_image(key='sample_images', images=images, caption=captions)\n",
    "\n",
    "            # Option 2: log predictions as a Table\n",
    "            columns = ['image', 'ground truth', 'prediction']\n",
    "            data = [[wandb.Image(x_i), y_i, y_pred] for x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]\n",
    "            wandb_logger.log_table(key='sample_table', columns=columns, data=data)\n",
    "\n",
    "log_predictions_callback = LogPredictionsCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7spLwBaSuCI"
   },
   "source": [
    "## üèãÔ∏è‚Äç Train Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.101065Z",
     "iopub.status.idle": "2023-03-03T02:53:52.101565Z",
     "shell.execute_reply": "2023-03-03T02:53:52.101565Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.101565Z"
    },
    "id": "JT4s-GKeSuCI",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    logger=wandb_logger,                    # W&B integration\n",
    "    callbacks=[log_predictions_callback,    # logging of sample predictions\n",
    "               checkpoint_callback],        # our model checkpoint callback\n",
    "    accelerator=\"gpu\",                      # use GPU\n",
    "    max_epochs=5)                           # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.103569Z",
     "iopub.status.idle": "2023-03-03T02:53:52.104564Z",
     "shell.execute_reply": "2023-03-03T02:53:52.104064Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.104064Z"
    },
    "id": "wnaXag_aSuCI",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, training_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWmc1aDeSuCI"
   },
   "source": [
    "When we want to close our W&B run, we call `wandb.finish()` (mainly useful in notebooks, called automatically in scripts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-03T02:53:52.106563Z",
     "iopub.status.idle": "2023-03-03T02:53:52.107566Z",
     "shell.execute_reply": "2023-03-03T02:53:52.107065Z",
     "shell.execute_reply.started": "2023-03-03T02:53:52.107065Z"
    },
    "id": "DCngWZ_cSuCI",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t33OzfUISuCM"
   },
   "source": [
    "## üìö Resources\n",
    "\n",
    "* [Pytorch Lightning and W&B integration documentation](https://docs.wandb.ai/integrations/lightning) contains a few tips for taking most advantage of W&B\n",
    "* [Pytorch Lightning documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/loggers.html#weights-and-biases) is extremely thorough and full of examples"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
