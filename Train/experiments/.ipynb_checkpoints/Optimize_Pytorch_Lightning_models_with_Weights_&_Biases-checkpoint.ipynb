{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmv7xMP7SuB8"
   },
   "source": [
    "Pytorch Lightning is a lightweight wrapper for organizing your PyTorch code and easily adding advanced features such as distributed training, 16-bit precision or gradient accumulation.\n",
    "\n",
    "Coupled with the [Weights & Biases integration](https://docs.wandb.com/library/integrations/lightning), you can quickly train and monitor models for full traceability and reproducibility with only 2 extra lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:03.262717Z",
     "iopub.status.busy": "2023-03-01T20:17:03.261711Z",
     "iopub.status.idle": "2023-03-01T20:17:11.004364Z",
     "shell.execute_reply": "2023-03-01T20:17:11.003361Z",
     "shell.execute_reply.started": "2023-03-01T20:17:03.262717Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20230301_151709-r72pnsup</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/lightning_logs/runs/r72pnsup' target=\"_blank\">proud-blaze-1</a></strong> to <a href='https://wandb.ai/christopher-marais/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/christopher-marais/lightning_logs' target=\"_blank\">https://wandb.ai/christopher-marais/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/christopher-marais/lightning_logs/runs/r72pnsup' target=\"_blank\">https://wandb.ai/christopher-marais/lightning_logs/runs/r72pnsup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\gcmar\\.conda\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "wandb_logger = WandbLogger()\n",
    "trainer = Trainer(logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDIZdR8qSuB8"
   },
   "source": [
    "W&B integration with Pytorch-Lightning can automatically:\n",
    "* log your configuration parameters\n",
    "* log your losses and metrics\n",
    "* log your model\n",
    "* keep track of your code\n",
    "* log your system metrics (GPU, CPU, memory, temperature, etc)\n",
    "\n",
    "### 📚 Docs\n",
    "You can find the PyTorch Lightning WandbLogger docs [here](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.loggers.WandbLogger.html?highlight=wandblogger) and the Weights & Biases docs [here](https://docs.wandb.com/library/integrations/lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOM58ImkSuB_"
   },
   "source": [
    "### 🛠️ Installation and set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:11.005879Z",
     "iopub.status.busy": "2023-03-01T20:17:11.004364Z",
     "iopub.status.idle": "2023-03-01T20:17:11.020491Z",
     "shell.execute_reply": "2023-03-01T20:17:11.019572Z",
     "shell.execute_reply.started": "2023-03-01T20:17:11.005879Z"
    },
    "id": "zIOoAOVrSuB_",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q pytorch-lightning wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPKgb9R6SuCB"
   },
   "source": [
    "We make sure we're logged into W&B so that our experiments can be associated with our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:11.022487Z",
     "iopub.status.busy": "2023-03-01T20:17:11.022487Z",
     "iopub.status.idle": "2023-03-01T20:17:11.050756Z",
     "shell.execute_reply": "2023-03-01T20:17:11.049229Z",
     "shell.execute_reply.started": "2023-03-01T20:17:11.022487Z"
    },
    "id": "9K3LS0PlSuCC",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTRkr3AoSuCC"
   },
   "source": [
    "## 📊 Setting up the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVHMiAYoSuCD"
   },
   "source": [
    "For the context of this tutorial we use vanilla pytorch dataloaders on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:11.052280Z",
     "iopub.status.busy": "2023-03-01T20:17:11.051770Z",
     "iopub.status.idle": "2023-03-01T20:17:11.158906Z",
     "shell.execute_reply": "2023-03-01T20:17:11.157375Z",
     "shell.execute_reply.started": "2023-03-01T20:17:11.052280Z"
    },
    "id": "LtC3tqgLSuCD",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "dataset = MNIST(root=\"./MNIST\", download=True, transform=transform)\n",
    "training_set, validation_set = random_split(dataset, [55000, 5000])\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:11.161941Z",
     "iopub.status.busy": "2023-03-01T20:17:11.160940Z",
     "iopub.status.idle": "2023-03-01T20:17:11.466147Z",
     "shell.execute_reply": "2023-03-01T20:17:11.464630Z",
     "shell.execute_reply.started": "2023-03-01T20:17:11.161941Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LightningDataModule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMNISTDataModule\u001b[39;00m(\u001b[43mLightningDataModule\u001b[49m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LightningDataModule' is not defined"
     ]
    }
   ],
   "source": [
    "class MNISTDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir='./', batch_size=256):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        '''called only once and on 1 GPU'''\n",
    "        # download data\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''called on each GPU separately - stage defines if we are at fit or test step'''\n",
    "        # we set up only relevant datasets when stage is specified (automatically set by Pytorch-Lightning)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_train = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        '''returns training dataloader'''\n",
    "        mnist_train = DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle = True)#, num_workers = cpu_count)\n",
    "        return mnist_train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        '''returns validation dataloader'''\n",
    "        mnist_val = DataLoader(self.mnist_val, batch_size=self.batch_size, shuffle = True)#, num_workers = cpu_count)\n",
    "        return mnist_val\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        '''returns test dataloader'''\n",
    "        mnist_test = DataLoader(self.mnist_test, batch_size=self.batch_size, shuffle = True)#, num_workers = cpu_count)\n",
    "        return mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-01T20:17:11.467672Z",
     "iopub.status.idle": "2023-03-01T20:17:11.468686Z",
     "shell.execute_reply": "2023-03-01T20:17:11.468686Z",
     "shell.execute_reply.started": "2023-03-01T20:17:11.468686Z"
    }
   },
   "outputs": [],
   "source": [
    "# declaring the path of the train and test folders\n",
    "train_path = \"DATASET/TRAIN\"\n",
    "test_path = \"DATASET/TEST\"\n",
    "classes_dir_data = os.listdir(train_path)\n",
    "num_of_classes = len(classes_dir_data)\n",
    "print(\"Total Number of Classes :\" , num_of_classes)\n",
    "num = 0\n",
    "classes_dict = {}\n",
    "classes_lst = []\n",
    "num_dict = {}\n",
    "for c in  classes_dir_data:\n",
    "    classes_dict[c] = num\n",
    "    num_dict[num] = c\n",
    "    classes_lst.append(c)\n",
    "    num = num +1\n",
    "\"\"\"\n",
    "num_dict contains a dictionary of the classes numerically and it's corresponding classes.\n",
    "classes_dict contains a dictionary of the classes and the coresponding values numerically.\n",
    "\"\"\"\n",
    "num_of_classes = len(classes_dir_data)\n",
    "\n",
    "classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-01T20:17:11.470707Z",
     "iopub.status.idle": "2023-03-01T20:17:11.470707Z",
     "shell.execute_reply": "2023-03-01T20:17:11.470707Z",
     "shell.execute_reply.started": "2023-03-01T20:17:11.470707Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating the dataset\n",
    "\n",
    "#dataset\n",
    "\n",
    "class Image_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self,classes,image_base_dir,transform = None, target_transform = None):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        classes:The classes in the dataset\n",
    "\n",
    "        image_base_dir:The directory of the folders containing the images\n",
    "\n",
    "        transform:The trasformations for the Images\n",
    "\n",
    "        Target_transform:The trasformations for the target\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.img_labels = classes\n",
    "\n",
    "        self.imge_base_dir = image_base_dir\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        img_dir_list = os.listdir(os.path.join(self.imge_base_dir,self.img_labels[idx]))\n",
    "\n",
    "        image_path = img_dir_list[randint(0,len(img_dir_list)-1)]\n",
    "\n",
    "        #print(image_path)\n",
    "\n",
    "        image_path = os.path.join(self.imge_base_dir,self.img_labels[idx],image_path)\n",
    "\n",
    "        image = skimage.io.imread(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            label = self.target_transform(self.img_labels[idx])\n",
    "\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-01T20:17:11.472711Z",
     "iopub.status.idle": "2023-03-01T20:17:11.473706Z",
     "shell.execute_reply": "2023-03-01T20:17:11.473706Z",
     "shell.execute_reply.started": "2023-03-01T20:17:11.473706Z"
    }
   },
   "outputs": [],
   "source": [
    "size = 50 # need to be the same as what is used in layer_5/ input layer ot the cnn\n",
    "\n",
    "basic_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((size,size)),\n",
    "        transforms.Grayscale(1),\n",
    "    transforms.ToTensor()])\n",
    "training_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((size,size)),\n",
    "    transforms.RandomRotation(degrees = 45),\n",
    "    transforms.RandomHorizontalFlip(p = 0.005),\n",
    "        transforms.Grayscale(1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def target_transformations(x):\n",
    "    return torch.tensor(classes_dict.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL03mttlSuCE"
   },
   "source": [
    "## 🤓 Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZ2mDCQ-SuCE"
   },
   "source": [
    "**Tips**:\n",
    "* Call `self.save_hyperparameters()` in `__init__` to automatically log your hyperparameters to **W&B**\n",
    "* Call self.log in `training_step` and `validation_step` to log the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:30.870268Z",
     "iopub.status.busy": "2023-03-01T20:17:30.870268Z",
     "iopub.status.idle": "2023-03-01T20:17:30.884286Z",
     "shell.execute_reply": "2023-03-01T20:17:30.883284Z",
     "shell.execute_reply.started": "2023-03-01T20:17:30.870268Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, CrossEntropyLoss, functional as F\n",
    "from torch.optim import Adam\n",
    "from torchmetrics.functional import accuracy\n",
    "from pytorch_lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:31.002409Z",
     "iopub.status.busy": "2023-03-01T20:17:31.002409Z",
     "iopub.status.idle": "2023-03-01T20:17:31.024668Z",
     "shell.execute_reply": "2023-03-01T20:17:31.023665Z",
     "shell.execute_reply.started": "2023-03-01T20:17:31.002409Z"
    },
    "id": "xL_JWvg9SuCE",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class MNIST_LitModule(LightningModule): \n",
    "\n",
    "    def __init__(self, n_classes=10, n_layer_1=128, n_layer_2=256, lr=1e-3): #############################################################################\n",
    "        '''method used to define our model parameters'''\n",
    "        super().__init__()\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)#############################################################################\n",
    "        self.layer_1 = Linear(28 * 28, n_layer_1)#############################################################################\n",
    "        self.layer_2 = Linear(n_layer_1, n_layer_2)#############################################################################\n",
    "        self.layer_3 = Linear(n_layer_2, n_classes)#############################################################################\n",
    "\n",
    "        # loss\n",
    "        self.loss = CrossEntropyLoss()#############################################################################\n",
    "\n",
    "        # optimizer parameters\n",
    "        self.lr = lr#############################################################################\n",
    "\n",
    "        # save hyper-parameters to self.hparams (auto-logged by W&B)\n",
    "        self.save_hyperparameters() \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''method used for inference input -> output'''\n",
    "\n",
    "        batch_size, channels, width, height = x.size()\n",
    "\n",
    "        # (b, 1, 28, 28) -> (b, 1*28*28)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # let's do 3 x (linear + relu)\n",
    "        x = self.layer_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''needs to return a loss from a single batch'''\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        # Log loss and metric\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_accuracy', acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        # Log loss and metric\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_accuracy', acc)\n",
    "\n",
    "        # Let's return preds to use it in a custom callback\n",
    "        return preds\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "        # Log loss and metric\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_accuracy', acc)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        '''defines model optimizer'''\n",
    "        return Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def _get_preds_loss_accuracy(self, batch):\n",
    "        '''convenience function since train/valid/test steps are similar'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        loss = self.loss(logits, y)\n",
    "        acc = accuracy(preds, y, 'multiclass', num_classes=10)\n",
    "        return preds, loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3wR351_SuCF"
   },
   "source": [
    "The model is now ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:31.281922Z",
     "iopub.status.busy": "2023-03-01T20:17:31.280940Z",
     "iopub.status.idle": "2023-03-01T20:17:31.301566Z",
     "shell.execute_reply": "2023-03-01T20:17:31.300589Z",
     "shell.execute_reply.started": "2023-03-01T20:17:31.281922Z"
    },
    "id": "vNGDhqpnSuCF",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = MNIST_LitModule(n_layer_1=128, n_layer_2=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0t6MpopSuCG"
   },
   "source": [
    "## 💾 Save Model Checkpoints\n",
    "\n",
    "The `ModelCheckpoint` callback is required along with the `WandbLogger` argument to log model checkpoints to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:32.148038Z",
     "iopub.status.busy": "2023-03-01T20:17:32.147050Z",
     "iopub.status.idle": "2023-03-01T20:17:32.157129Z",
     "shell.execute_reply": "2023-03-01T20:17:32.156231Z",
     "shell.execute_reply.started": "2023-03-01T20:17:32.148038Z"
    },
    "id": "d84LZk-6SuCG",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_accuracy', mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G69Pss7VSuCG"
   },
   "source": [
    "## 💡 Tracking Experiments with WandbLogger\n",
    "\n",
    "PyTorch Lightning has a `WandbLogger` to easily log your experiments with Wights & Biases. Just pass it to your `Trainer` to log to W&B. See the [WandbLogger docs](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html#pytorch_lightning.loggers.WandbLogger) for all parameters. Note, to log the metrics to a specific W&B Team, pass your Team name to the `entity` argument in `WandbLogger`\n",
    "\n",
    "#### `pytorch_lightning.loggers.WandbLogger()`\n",
    "\n",
    "| Functionality | Argument/Function | PS |\n",
    "| ------ | ------ | ------ |\n",
    "| Logging models | `WandbLogger(... ,log_model='all')` or `WandbLogger(... ,log_model=True`) | Log all models if `log_model=\"all\"` and at end of training if `log_model=True`\n",
    "| Set custom run names | `WandbLogger(... ,name='my_run_name'`) | |\n",
    "| Organize runs by project | `WandbLogger(... ,project='my_project')` | |\n",
    "| Log histograms of gradients and parameters | `WandbLogger.watch(model)`  | `WandbLogger.watch(model, log='all')` to log parameter histograms  |\n",
    "| Log hyperparameters | Call `self.save_hyperparameters()` within `LightningModule.__init__()` |\n",
    "| Log custom objects (images, audio, video, molecules…) | Use `WandbLogger.log_text`, `WandbLogger.log_image` and `WandbLogger.log_table` |\n",
    "\n",
    "See the [WandbLogger docs](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html#pytorch_lightning.loggers.WandbLogger) here for all parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:32.896735Z",
     "iopub.status.busy": "2023-03-01T20:17:32.895127Z",
     "iopub.status.idle": "2023-03-01T20:17:32.920379Z",
     "shell.execute_reply": "2023-03-01T20:17:32.918524Z",
     "shell.execute_reply.started": "2023-03-01T20:17:32.896735Z"
    },
    "id": "8zNfGYR7SuCH",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gcmar\\.conda\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "wandb_logger = WandbLogger(project='MNIST', # group runs in \"MNIST\" project\n",
    "                           log_model='all') # log all new checkpoints during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyquRSkhSuCH"
   },
   "source": [
    "## ⚙️ Using WandbLogger to log Images, Text and More\n",
    "Pytorch Lightning is extensible through its callback system. We can create a custom callback to automatically log sample predictions during validation. `WandbLogger` provides convenient media logging functions:\n",
    "* `WandbLogger.log_text` for text data\n",
    "* `WandbLogger.log_image` for images\n",
    "* `WandbLogger.log_table` for [W&B Tables](https://docs.wandb.ai/guides/data-vis).\n",
    "\n",
    "An alternate to `self.log` in the Model class is directly using `wandb.log({dict})` or `trainer.logger.experiment.log({dict})`\n",
    "\n",
    "In this case we log the first 20 images in the first batch of the validation dataset along with the predicted and ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:33.746922Z",
     "iopub.status.busy": "2023-03-01T20:17:33.746922Z",
     "iopub.status.idle": "2023-03-01T20:17:33.768774Z",
     "shell.execute_reply": "2023-03-01T20:17:33.767773Z",
     "shell.execute_reply.started": "2023-03-01T20:17:33.746922Z"
    },
    "id": "M9-321mOSuCH",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    " \n",
    "class LogPredictionsCallback(Callback):\n",
    "    \n",
    "    def on_validation_batch_end(\n",
    "        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        \"\"\"Called when the validation batch ends.\"\"\"\n",
    " \n",
    "        # `outputs` comes from `LightningModule.validation_step`\n",
    "        # which corresponds to our model predictions in this case\n",
    "        \n",
    "        # Let's log 20 sample image predictions from first batch\n",
    "        if batch_idx == 0:\n",
    "            n = 20\n",
    "            x, y = batch\n",
    "            images = [img for img in x[:n]]\n",
    "            captions = [f'Ground Truth: {y_i} - Prediction: {y_pred}' for y_i, y_pred in zip(y[:n], outputs[:n])]\n",
    "            \n",
    "            # Option 1: log images with `WandbLogger.log_image`\n",
    "            wandb_logger.log_image(key='sample_images', images=images, caption=captions)\n",
    "\n",
    "            # Option 2: log predictions as a Table\n",
    "            columns = ['image', 'ground truth', 'prediction']\n",
    "            data = [[wandb.Image(x_i), y_i, y_pred] for x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]\n",
    "            wandb_logger.log_table(key='sample_table', columns=columns, data=data)\n",
    "\n",
    "log_predictions_callback = LogPredictionsCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7spLwBaSuCI"
   },
   "source": [
    "## 🏋️‍ Train Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:34.531650Z",
     "iopub.status.busy": "2023-03-01T20:17:34.531650Z",
     "iopub.status.idle": "2023-03-01T20:17:34.566933Z",
     "shell.execute_reply": "2023-03-01T20:17:34.565396Z",
     "shell.execute_reply.started": "2023-03-01T20:17:34.531650Z"
    },
    "id": "JT4s-GKeSuCI",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    logger=wandb_logger,                    # W&B integration\n",
    "    callbacks=[log_predictions_callback,    # logging of sample predictions\n",
    "               checkpoint_callback],        # our model checkpoint callback\n",
    "    accelerator=\"gpu\",                      # use GPU\n",
    "    max_epochs=5)                           # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:17:34.893588Z",
     "iopub.status.busy": "2023-03-01T20:17:34.892578Z",
     "iopub.status.idle": "2023-03-01T20:19:35.616443Z",
     "shell.execute_reply": "2023-03-01T20:19:35.615932Z",
     "shell.execute_reply.started": "2023-03-01T20:17:34.893588Z"
    },
    "id": "wnaXag_aSuCI",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | layer_1 | Linear           | 100 K \n",
      "1 | layer_2 | Linear           | 16.5 K\n",
      "2 | layer_3 | Linear           | 1.3 K \n",
      "3 | loss    | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "118 K     Trainable params\n",
      "0         Non-trainable params\n",
      "118 K     Total params\n",
      "0.473     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gcmar\\.conda\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\gcmar\\.conda\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521627599b1643cfa6fc84fae85d0da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, training_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWmc1aDeSuCI"
   },
   "source": [
    "When we want to close our W&B run, we call `wandb.finish()` (mainly useful in notebooks, called automatically in scripts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:19:35.618462Z",
     "iopub.status.busy": "2023-03-01T20:19:35.618462Z"
    },
    "id": "DCngWZ_cSuCI",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t33OzfUISuCM"
   },
   "source": [
    "## 📚 Resources\n",
    "\n",
    "* [Pytorch Lightning and W&B integration documentation](https://docs.wandb.ai/integrations/lightning) contains a few tips for taking most advantage of W&B\n",
    "* [Pytorch Lightning documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/loggers.html#weights-and-biases) is extremely thorough and full of examples"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
