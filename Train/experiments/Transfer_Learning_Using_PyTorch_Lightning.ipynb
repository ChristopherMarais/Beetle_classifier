{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T19:32:38.945977Z",
     "iopub.status.busy": "2023-03-08T19:32:38.945977Z",
     "iopub.status.idle": "2023-03-08T19:32:42.643740Z",
     "shell.execute_reply": "2023-03-08T19:32:42.643740Z",
     "shell.execute_reply.started": "2023-03-08T19:32:38.945977Z"
    },
    "id": "S9pJ49LSTh-V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "# your favorite machine learning tracking tool\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "# from torchmetrics.functional import accuracy\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import StanfordCars\n",
    "from torchvision.datasets.utils import download_url\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lU_xxpiIUEX3"
   },
   "source": [
    "Now you'll need to login to you wandb account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T19:32:42.645741Z",
     "iopub.status.busy": "2023-03-08T19:32:42.644730Z",
     "iopub.status.idle": "2023-03-08T19:32:45.532967Z",
     "shell.execute_reply": "2023-03-08T19:32:45.531962Z",
     "shell.execute_reply.started": "2023-03-08T19:32:42.645741Z"
    },
    "id": "cj2a1F0yTsqx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlOc1qJqULTF"
   },
   "source": [
    "## The Dataset 💿\n",
    "\n",
    "We will be using the StanfordCars dataset to train our image classifier. It contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T19:32:45.535962Z",
     "iopub.status.busy": "2023-03-08T19:32:45.534962Z",
     "iopub.status.idle": "2023-03-08T19:32:45.548966Z",
     "shell.execute_reply": "2023-03-08T19:32:45.547963Z",
     "shell.execute_reply.started": "2023-03-08T19:32:45.535962Z"
    },
    "id": "yaF_kykKUFpk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StanfordCarsDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, data_dir: str = './'):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Augmentation policy for training set\n",
    "        self.augmentation = transforms.Compose([\n",
    "              transforms.RandomResizedCrop(size=(224,224), scale=(0.8, 1.0)),\n",
    "              transforms.RandomRotation(degrees=15),\n",
    "              transforms.RandomHorizontalFlip(),\n",
    "              transforms.CenterCrop(size=224),\n",
    "              transforms.ToTensor(),\n",
    "              transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # Preprocessing steps applied to validation and test set.\n",
    "        self.transform = transforms.Compose([\n",
    "              transforms.Resize(size=(224,224)),\n",
    "              transforms.CenterCrop(size=224),\n",
    "              transforms.ToTensor(),\n",
    "              transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.num_classes = 196\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # build dataset\n",
    "        dataset = StanfordCars(root=self.data_dir, download=True, split=\"train\")\n",
    "        # split dataset\n",
    "        self.train, self.val = random_split(dataset, [6500, 1644])\n",
    "\n",
    "        self.test = StanfordCars(root=self.data_dir, download=True, split=\"test\")\n",
    "        \n",
    "        self.test = random_split(self.test, [len(self.test)])[0]\n",
    "\n",
    "        self.train.dataset.transform = self.augmentation\n",
    "        self.val.dataset.transform = self.transform\n",
    "        self.test.dataset.transform = self.transform\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size, num_workers=2)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwY6LvI2VfrT"
   },
   "source": [
    "## LightingModule - Define the System\n",
    "\n",
    "Let us look at the model definition to see how transfer learning can be used with PyTorch Lightning.\n",
    "In the `LitModel` class, we can use the pre-trained model provided by Torchvision as a feature extractor for our classification model. Here we are using ResNet-18. A list of pre-trained models provided by PyTorch Lightning can be found here.\n",
    "- When `pretrained=True`, we use the pre-trained weights; otherwise, the weights are initialized randomly.\n",
    "- If `.eval()` is used, then the layers are frozen. \n",
    "- A single `Linear` layer is used as the output layer. We can have multiple layers stacked over the `feature_extractor`.\n",
    "\n",
    "Setting the `transfer` argument to `True` will enable transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T19:32:45.550966Z",
     "iopub.status.busy": "2023-03-08T19:32:45.549966Z",
     "iopub.status.idle": "2023-03-08T19:32:45.580497Z",
     "shell.execute_reply": "2023-03-08T19:32:45.579488Z",
     "shell.execute_reply.started": "2023-03-08T19:32:45.550966Z"
    },
    "id": "qu0xf25aUckF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, input_shape, num_classes, learning_rate=2e-4, transfer=False, acc_task='multiclass'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # log hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dim = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.acc_task = acc_task\n",
    "        \n",
    "        if transfer:\n",
    "            # transfer learning if pretrained=True\n",
    "            self.feature_extractor = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1, progress=True) #models.maxvit_t(weights=models.MaxVit_T_Weights.IMAGENET1K_V1, progress=True)\n",
    "            # layers are frozen by using eval()\n",
    "            self.feature_extractor.eval()\n",
    "            # freeze params\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            self.feature_extractor = models.maxvit_t(pretrained=transfer)\n",
    "        \n",
    "        n_sizes = self._get_conv_output(input_shape)\n",
    "\n",
    "        self.classifier = nn.Linear(n_sizes, num_classes)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=self.acc_task, num_classes=self.num_classes)\n",
    "  \n",
    "    # returns the size of the output tensor going into the Linear layer from the conv block.\n",
    "    def _get_conv_output(self, shape):\n",
    "        batch_size = 1\n",
    "        tmp_input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "\n",
    "        output_feat = self._forward_features(tmp_input) \n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "        \n",
    "    # returns the feature tensor from the conv block\n",
    "    def _forward_features(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x\n",
    "    \n",
    "    # will be used during inference\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        batch, gt = batch[0], batch[1]\n",
    "        out = self.forward(batch)\n",
    "        loss = self.criterion(out, gt)\n",
    "\n",
    "        acc = self.accuracy(out, gt)\n",
    "\n",
    "        self.log(\"train/loss\", loss)\n",
    "        self.log(\"train/acc\", acc)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch, gt = batch[0], batch[1]\n",
    "        out = self.forward(batch)\n",
    "        loss = self.criterion(out, gt)\n",
    "\n",
    "        self.log(\"val/loss\", loss)\n",
    "\n",
    "        acc = self.accuracy(out, gt)\n",
    "        self.log(\"val/acc\", acc)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        batch, gt = batch[0], batch[1]\n",
    "        out = self.forward(batch)\n",
    "        loss = self.criterion(out, gt)\n",
    "        \n",
    "        return {\"loss\": loss, \"outputs\": out, \"gt\": gt}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        output = torch.cat([x['outputs'] for x in outputs], dim=0)\n",
    "        \n",
    "        gts = torch.cat([x['gt'] for x in outputs], dim=0)\n",
    "        \n",
    "        self.log(\"test/loss\", loss)\n",
    "        acc = self.accuracy(output, gts)\n",
    "        self.log(\"test/acc\", acc)\n",
    "        \n",
    "        self.test_gts = gts\n",
    "        self.test_output = output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCGQbJq7Wx6o"
   },
   "source": [
    "## Train your Model 🏋️‍♂️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erqTHdpDVf6S"
   },
   "source": [
    "To train the model, we instantiate the `StanfordCarsDataModule` and the `LitModel` along with the PyTorch Lightning Trainer. To the `Trainer`, we will pass the `WandbLogger` as the logger to use W&B to track the metrics during model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T19:32:45.582499Z",
     "iopub.status.busy": "2023-03-08T19:32:45.581494Z",
     "iopub.status.idle": "2023-03-08T19:32:47.343232Z",
     "shell.execute_reply": "2023-03-08T19:32:47.342234Z",
     "shell.execute_reply.started": "2023-03-08T19:32:45.581494Z"
    },
    "id": "Oq4R84NbWcH6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20230308_143246-3i7b8e4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/TransferLearning/runs/3i7b8e4i' target=\"_blank\">misty-water-5</a></strong> to <a href='https://wandb.ai/christopher-marais/TransferLearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/christopher-marais/TransferLearning' target=\"_blank\">https://wandb.ai/christopher-marais/TransferLearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/christopher-marais/TransferLearning/runs/3i7b8e4i' target=\"_blank\">https://wandb.ai/christopher-marais/TransferLearning/runs/3i7b8e4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "dm = StanfordCarsDataModule(batch_size=32)\n",
    "model = LitModel((3, 300, 300), 196, transfer=True)\n",
    "trainer = pl.Trainer(logger=WandbLogger(project=\"TransferLearning\"), max_epochs=1, accelerator=\"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fovvADjjYH9y"
   },
   "source": [
    "We are good to go! Let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2023-03-08T19:32:47.345247Z",
     "iopub.status.busy": "2023-03-08T19:32:47.345247Z",
     "iopub.status.idle": "2023-03-08T19:33:45.859424Z",
     "shell.execute_reply": "2023-03-08T19:33:45.858429Z",
     "shell.execute_reply.started": "2023-03-08T19:32:47.345247Z"
    },
    "id": "InyEyFeqXcck",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | feature_extractor | ResNet             | 11.7 M\n",
      "1 | classifier        | Linear             | 196 K \n",
      "2 | criterion         | CrossEntropyLoss   | 0     \n",
      "3 | accuracy          | MulticlassAccuracy | 0     \n",
      "---------------------------------------------------------\n",
      "196 K     Trainable params\n",
      "11.7 M    Non-trainable params\n",
      "11.9 M    Total params\n",
      "47.543    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gcmar\\.conda\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\gcmar\\.conda\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35f367cb2b4411eb3346b90b1211057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TncKz6NkYZfk"
   },
   "source": [
    "Now that the model is trained, let's see how it performs on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T19:33:45.861426Z",
     "iopub.status.busy": "2023-03-08T19:33:45.861426Z"
    },
    "id": "EO6mVfIeYdiW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\gcmar\\.conda\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982803323c074f35931a2e2cfd1c6718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-gUHf58YeKp"
   },
   "source": [
    "Let's close our W&B run, so we call `wandb.finish()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvvnbOLmwArI"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6KWT8j8Yywo"
   },
   "source": [
    "The workspace generated to compare training the model from scratch vs using transfer learning is available [here](https://wandb.ai/manan-goel/StanfordCars). The conclusions that can be drawn from this are explained in detail in [this report](https://wandb.ai/wandb/wandb-lightning/reports/Transfer-Learning-Using-PyTorch-Lightning--VmlldzoyMzMxMzk4/edit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9xNuqshZMzD"
   },
   "source": [
    "To learn more about transfer learning check out these resources:\n",
    "- [Gotchas of transfer learning for image classification](https://docs.google.com/presentation/d/1s29WOQoQvBD5KoPUzE5TPcavjqno8ZgnZaSljHGGHVU/edit?usp=sharing) by Sayak Paul.\n",
    "- [Transfer Learning with Keras and Deep Learning by PyImageSearch.](https://www.pyimagesearch.com/2019/05/20/transfer-learning-with-keras-and-deep-learning/)\n",
    "- [Transfer Learning - Machine Learning's Next Frontier](https://ruder.io/transfer-learning/) by Sebastian Ruder.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
