{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdce738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "import wandb\n",
    "import fastai\n",
    "from fastai.callback.wandb import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394d2be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2.7.11'"
     ]
    }
   ],
   "source": [
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76af5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open(\n",
    "#     urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
    "\n",
    "# model = timm.create_model('maxvit_xlarge_tf_512.in21k_ft_in1k', pretrained=True) # hf-hub:timm/\n",
    "# model = model.eval()\n",
    "\n",
    "# # get model specific transforms (normalization, resize)\n",
    "# data_config = timm.data.resolve_model_data_config(model)\n",
    "# transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "# output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
    "\n",
    "# top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ed3793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maxvit_base_tf_224.in1k',\n",
      " 'maxvit_base_tf_384.in1k',\n",
      " 'maxvit_base_tf_384.in21k_ft_in1k',\n",
      " 'maxvit_base_tf_512.in1k',\n",
      " 'maxvit_base_tf_512.in21k_ft_in1k',\n",
      " 'maxvit_large_tf_224.in1k',\n",
      " 'maxvit_large_tf_384.in1k',\n",
      " 'maxvit_large_tf_384.in21k_ft_in1k',\n",
      " 'maxvit_large_tf_512.in1k',\n",
      " 'maxvit_large_tf_512.in21k_ft_in1k',\n",
      " 'maxvit_nano_rw_256.sw_in1k',\n",
      " 'maxvit_rmlp_base_rw_224.sw_in12k',\n",
      " 'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k',\n",
      " 'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k',\n",
      " 'maxvit_rmlp_nano_rw_256.sw_in1k',\n",
      " 'maxvit_rmlp_pico_rw_256.sw_in1k',\n",
      " 'maxvit_rmlp_small_rw_224.sw_in1k',\n",
      " 'maxvit_rmlp_tiny_rw_256.sw_in1k',\n",
      " 'maxvit_small_tf_224.in1k',\n",
      " 'maxvit_small_tf_384.in1k',\n",
      " 'maxvit_small_tf_512.in1k',\n",
      " 'maxvit_tiny_rw_224.sw_in1k',\n",
      " 'maxvit_tiny_tf_224.in1k',\n",
      " 'maxvit_tiny_tf_384.in1k',\n",
      " 'maxvit_tiny_tf_512.in1k',\n",
      " 'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
      " 'maxvit_xlarge_tf_512.in21k_ft_in1k',\n",
      " 'maxxvit_rmlp_nano_rw_256.sw_in1k',\n",
      " 'maxxvit_rmlp_small_rw_256.sw_in1k',\n",
      " 'maxxvitv2_nano_rw_256.sw_in1k',\n",
      " 'maxxvitv2_rmlp_base_rw_224.sw_in12k',\n",
      " 'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k',\n",
      " 'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k']"
     ]
    }
   ],
   "source": [
    "timm.list_models('*max*', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a969e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "def get_pets(batch_size, img_size, seed):\n",
    "    \"The dog breeds pets datasets\"\n",
    "    dataset_path = untar_data(URLs.PETS)\n",
    "    files = get_image_files(dataset_path/\"images\")\n",
    "    dls = ImageDataLoaders.from_name_re(dataset_path, files, \n",
    "                                        pat=r'(^[a-zA-Z]+_*[a-zA-Z]+)', \n",
    "                                        valid_pct=0.2, \n",
    "                                        seed=seed, \n",
    "                                        bs=batch_size,\n",
    "                                        item_tfms=Resize(img_size)) \n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bb5b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\gcmar\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\wandb\\run-20230309_165457-wo19rtpe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe' target=\"_blank\">feasible-frost-4</a></strong> to <a href='https://wandb.ai/christopher-marais/PROJECT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/christopher-marais/PROJECT' target=\"_blank\">https://wandb.ai/christopher-marais/PROJECT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe' target=\"_blank\">https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/92 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = SimpleNamespace(\n",
    "    batch_size=64,\n",
    "    img_size=224,\n",
    "    seed=42,\n",
    "    pretrained=False,\n",
    "    model_name=\"maxvit_nano_rw_256.sw_in1k\", # try with maxvit_nano_rw_256.sw_in1k # regnetx_040\n",
    "    epochs=1)\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    \"Train the model using the supplied config\"\n",
    "    dls = get_pets(config.batch_size, config.img_size, config.seed)\n",
    "    with wandb.init(project=\"PROJECT\", group='ambrosia_symbiosis', job_type='test_training', config=config):\n",
    "        cbs = [MixedPrecision(), WandbCallback(log_preds=False)]\n",
    "        learn = vision_learner(dls, config.model_name, metrics=error_rate, \n",
    "                               cbs=cbs, pretrained=config.pretrained)\n",
    "        learn.fine_tune(config.epochs)\n",
    "\n",
    "\n",
    "train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
