diff --git a/Train/.ipynb_checkpoints/timm_FastAI_WANDB-checkpoint.ipynb b/Train/.ipynb_checkpoints/timm_FastAI_WANDB-checkpoint.ipynb
index ad480ea..4cd4cc0 100644
--- a/Train/.ipynb_checkpoints/timm_FastAI_WANDB-checkpoint.ipynb
+++ b/Train/.ipynb_checkpoints/timm_FastAI_WANDB-checkpoint.ipynb
@@ -6,11 +6,11 @@
    "id": "436f1771-7aa6-45e7-abae-56c8112da143",
    "metadata": {
     "execution": {
-     "iopub.execute_input": "2023-03-09T17:03:52.351536Z",
-     "iopub.status.busy": "2023-03-09T17:03:52.351536Z",
-     "iopub.status.idle": "2023-03-09T17:03:57.156787Z",
-     "shell.execute_reply": "2023-03-09T17:03:57.155788Z",
-     "shell.execute_reply.started": "2023-03-09T17:03:52.351536Z"
+     "iopub.execute_input": "2023-03-09T21:54:47.601565Z",
+     "iopub.status.busy": "2023-03-09T21:54:47.600571Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.454515Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.453511Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:47.601565Z"
     },
     "tags": []
    },
@@ -18,7 +18,11 @@
    "source": [
     "from urllib.request import urlopen\n",
     "from PIL import Image\n",
-    "import timm"
+    "import timm\n",
+    "import torch\n",
+    "import wandb\n",
+    "import fastai\n",
+    "from fastai.callback.wandb import WandbCallback"
    ]
   },
   {
@@ -27,11 +31,11 @@
    "id": "5a8980b5-f22c-4e50-bb9f-8ade0c9583cc",
    "metadata": {
     "execution": {
-     "iopub.execute_input": "2023-03-09T17:03:57.158793Z",
-     "iopub.status.busy": "2023-03-09T17:03:57.158793Z",
-     "iopub.status.idle": "2023-03-09T17:03:57.172789Z",
-     "shell.execute_reply": "2023-03-09T17:03:57.171788Z",
-     "shell.execute_reply.started": "2023-03-09T17:03:57.158793Z"
+     "iopub.execute_input": "2023-03-09T21:54:52.456511Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.455499Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.470498Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.469486Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.456511Z"
     },
     "tags": []
    },
@@ -39,7 +43,7 @@
     {
      "data": {
       "text/plain": [
-       "'0.8.15dev0'"
+       "'2.7.11'"
       ]
      },
      "execution_count": 2,
@@ -48,1972 +52,51 @@
     }
    ],
    "source": [
-    "timm.__version__"
+    "fastai.__version__"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
-   "id": "ce6dcd5a-212a-4a26-ab87-8ab81952aec4",
-   "metadata": {
-    "collapsed": true,
-    "execution": {
-     "iopub.execute_input": "2023-03-09T17:06:06.091074Z",
-     "iopub.status.busy": "2023-03-09T17:06:06.091074Z",
-     "iopub.status.idle": "2023-03-09T17:37:25.330060Z",
-     "shell.execute_reply": "2023-03-09T17:37:25.329053Z",
-     "shell.execute_reply.started": "2023-03-09T17:06:06.091074Z"
-    },
-    "jupyter": {
-     "outputs_hidden": true
-    },
-    "tags": []
-   },
-   "outputs": [
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "6028ebec30904d63a0b5577fa37e86e5",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/plain": [
-       "MaxxVit(\n",
-       "  (stem): Stem(\n",
-       "    (conv1): Conv2dSame(3, 192, kernel_size=(3, 3), stride=(2, 2))\n",
-       "    (norm1): BatchNormAct2d(\n",
-       "      192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "      (drop): Identity()\n",
-       "      (act): GELUTanh()\n",
-       "    )\n",
-       "    (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
-       "  )\n",
-       "  (stages): Sequential(\n",
-       "    (0): MaxxVitStage(\n",
-       "      (blocks): Sequential(\n",
-       "        (0): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Downsample2d(\n",
-       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
-       "              (expand): Identity()\n",
-       "            )\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2dSame(768, 768, kernel_size=(3, 3), stride=(2, 2), groups=768, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (1): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "    (1): MaxxVitStage(\n",
-       "      (blocks): Sequential(\n",
-       "        (0): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Downsample2d(\n",
-       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
-       "              (expand): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            )\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(192, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2dSame(1536, 1536, kernel_size=(3, 3), stride=(2, 2), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (1): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (2): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (3): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (4): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (5): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "    (2): MaxxVitStage(\n",
-       "      (blocks): Sequential(\n",
-       "        (0): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Downsample2d(\n",
-       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
-       "              (expand): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            )\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2dSame(3072, 3072, kernel_size=(3, 3), stride=(2, 2), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (1): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (2): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (3): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (4): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (5): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (6): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (7): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (8): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (9): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (10): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (11): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (12): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (13): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "    (3): MaxxVitStage(\n",
-       "      (blocks): Sequential(\n",
-       "        (0): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Downsample2d(\n",
-       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
-       "              (expand): Conv2d(768, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            )\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              6144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2dSame(6144, 6144, kernel_size=(3, 3), stride=(2, 2), groups=6144, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              6144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(6144, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(384, 6144, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(6144, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (1): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(1536, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              6144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=6144, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              6144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(6144, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(384, 6144, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(6144, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "  )\n",
-       "  (norm): Identity()\n",
-       "  (head): NormMlpClassifierHead(\n",
-       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
-       "    (norm): LayerNorm2d((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
-       "    (pre_logits): Sequential(\n",
-       "      (fc): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "      (act): Tanh()\n",
-       "    )\n",
-       "    (drop): Dropout(p=0.0, inplace=False)\n",
-       "    (fc): Linear(in_features=1536, out_features=1000, bias=True)\n",
-       "  )\n",
-       ")"
-      ]
-     },
-     "execution_count": 4,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "timm.create_model(\"maxvit_xlarge_tf_512.in21k_ft_in1k\", pretrained=True)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 3,
    "id": "af673a5e-c94b-4ff2-ad5a-af508a10ca18",
    "metadata": {
     "execution": {
-     "iopub.execute_input": "2023-03-09T17:46:56.585226Z",
-     "iopub.status.busy": "2023-03-09T17:46:56.585226Z"
+     "iopub.execute_input": "2023-03-09T21:54:52.474510Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.473510Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.487037Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.485507Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.474510Z"
     },
     "tags": []
    },
    "outputs": [],
    "source": [
-    "img = Image.open(\n",
-    "    urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
+    "# img = Image.open(\n",
+    "#     urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
     "\n",
-    "model = timm.create_model('maxvit_xlarge_tf_512.in21k_ft_in1k', pretrained=True) # hf-hub:timm/\n",
-    "model = model.eval()\n",
+    "# model = timm.create_model('maxvit_xlarge_tf_512.in21k_ft_in1k', pretrained=True) # hf-hub:timm/\n",
+    "# model = model.eval()\n",
     "\n",
-    "# get model specific transforms (normalization, resize)\n",
-    "data_config = timm.data.resolve_model_data_config(model)\n",
-    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
+    "# # get model specific transforms (normalization, resize)\n",
+    "# data_config = timm.data.resolve_model_data_config(model)\n",
+    "# transforms = timm.data.create_transform(**data_config, is_training=False)\n",
     "\n",
-    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
+    "# output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
     "\n",
-    "top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)"
+    "# top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 4,
    "id": "56818f26-f610-4fe1-89a0-450d6c5e93b0",
    "metadata": {
     "execution": {
-     "iopub.execute_input": "2023-03-09T17:04:02.134887Z",
-     "iopub.status.busy": "2023-03-09T17:04:02.134887Z",
-     "iopub.status.idle": "2023-03-09T17:04:02.159457Z",
-     "shell.execute_reply": "2023-03-09T17:04:02.158456Z",
-     "shell.execute_reply.started": "2023-03-09T17:04:02.134887Z"
+     "iopub.execute_input": "2023-03-09T21:54:52.489037Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.488043Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.502563Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.501568Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.489037Z"
     },
     "tags": []
    },
@@ -2056,7 +139,7 @@
        " 'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k']"
       ]
      },
-     "execution_count": 3,
+     "execution_count": 4,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2067,9 +150,17 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 5,
    "id": "fd1e36e7-6131-4ad7-a7cc-3e825b391108",
-   "metadata": {},
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2023-03-09T21:54:52.504569Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.503569Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.550078Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.549090Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.504569Z"
+    }
+   },
    "outputs": [],
    "source": [
     "from fastai.vision.all import *\n",
@@ -2090,24 +181,278 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 6,
    "id": "28c79cc5-2685-4d40-8b07-7a79e00ade9c",
-   "metadata": {},
-   "outputs": [],
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2023-03-09T21:54:52.552075Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.551079Z",
+     "iopub.status.idle": "2023-03-09T21:55:25.952196Z",
+     "shell.execute_reply": "2023-03-09T21:55:25.950177Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.552075Z"
+    },
+    "tags": []
+   },
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
+       " $ pip install wandb --upgrade"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Tracking run with wandb version 0.13.10"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Run data is saved locally in <code>C:\\Users\\gcmar\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\wandb\\run-20230309_165457-wo19rtpe</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe' target=\"_blank\">feasible-frost-4</a></strong> to <a href='https://wandb.ai/christopher-marais/PROJECT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View project at <a href='https://wandb.ai/christopher-marais/PROJECT' target=\"_blank\">https://wandb.ai/christopher-marais/PROJECT</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run at <a href='https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe' target=\"_blank\">https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "C:\\Users\\gcmar\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
+      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "\n",
+       "<style>\n",
+       "    /* Turns off some styling */\n",
+       "    progress {\n",
+       "        /* gets rid of default border in Firefox and Opera. */\n",
+       "        border: none;\n",
+       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
+       "        background-size: auto;\n",
+       "    }\n",
+       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
+       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
+       "    }\n",
+       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
+       "        background: #F44336;\n",
+       "    }\n",
+       "</style>\n"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "\n",
+       "    <div>\n",
+       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
+       "      0.00% [0/1 00:00&lt;?]\n",
+       "    </div>\n",
+       "    \n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: left;\">\n",
+       "      <th>epoch</th>\n",
+       "      <th>train_loss</th>\n",
+       "      <th>valid_loss</th>\n",
+       "      <th>error_rate</th>\n",
+       "      <th>time</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "  </tbody>\n",
+       "</table><p>\n",
+       "\n",
+       "    <div>\n",
+       "      <progress value='0' class='' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
+       "      0.00% [0/92 00:00&lt;?]\n",
+       "    </div>\n",
+       "    "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "8b029004fd12455ba2368a5fc49ac0f8",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "VBox(children=(Label(value='2.996 MB of 11.439 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.26193â€¦"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run <strong style=\"color:#cdcd00\">feasible-frost-4</strong> at: <a href='https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe' target=\"_blank\">https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Find logs at: <code>.\\wandb\\run-20230309_165457-wo19rtpe\\logs</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "ename": "AssertionError",
+     "evalue": "height (28) must be divisible by window (8)",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
+      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m         learn \u001b[38;5;241m=\u001b[39m vision_learner(dls, config\u001b[38;5;241m.\u001b[39mmodel_name, metrics\u001b[38;5;241m=\u001b[39merror_rate, \n\u001b[0;32m     16\u001b[0m                                cbs\u001b[38;5;241m=\u001b[39mcbs, pretrained\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretrained)\n\u001b[0;32m     17\u001b[0m         learn\u001b[38;5;241m.\u001b[39mfine_tune(config\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
+      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     14\u001b[0m cbs \u001b[38;5;241m=\u001b[39m [MixedPrecision(), WandbCallback(log_preds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[0;32m     15\u001b[0m learn \u001b[38;5;241m=\u001b[39m vision_learner(dls, config\u001b[38;5;241m.\u001b[39mmodel_name, metrics\u001b[38;5;241m=\u001b[39merror_rate, \n\u001b[0;32m     16\u001b[0m                        cbs\u001b[38;5;241m=\u001b[39mcbs, pretrained\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretrained)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\callback\\schedule.py:165\u001b[0m, in \u001b[0;36mfine_tune\u001b[1;34m(self, epochs, base_lr, freeze_epochs, lr_mult, pct_start, div, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_one_cycle(freeze_epochs, \u001b[38;5;28mslice\u001b[39m(base_lr), pct_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m base_lr \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfreeze()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\callback\\schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[1;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[0;32m    116\u001b[0m lr_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[0;32m    117\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[38;5;241m/\u001b[39mdiv, lr_max, lr_max\u001b[38;5;241m/\u001b[39mdiv_final),\n\u001b[0;32m    118\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmom\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoms \u001b[38;5;28;01mif\u001b[39;00m moms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m moms))}\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[1;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[1;34m(self, i, b)\u001b[0m\n\u001b[0;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:216\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_one_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb):\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\vision\\learner.py:177\u001b[0m, in \u001b[0;36mTimmBody.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_pool \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:1257\u001b[0m, in \u001b[0;36mMaxxVit.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m   1256\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[1;32m-> 1257\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1258\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:1065\u001b[0m, in \u001b[0;36mMaxxVitStage.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1065\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:955\u001b[0m, in \u001b[0;36mMaxxVitBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    953\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# to NHWC (channels-last)\u001b[39;00m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 955\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_grid(x)\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnchw_attn:\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:739\u001b[0m, in \u001b[0;36mPartitionAttentionCl.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 739\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partition_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    740\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:726\u001b[0m, in \u001b[0;36mPartitionAttentionCl._partition_attn\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    724\u001b[0m img_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_block:\n\u001b[1;32m--> 726\u001b[0m     partitioned \u001b[38;5;241m=\u001b[39m \u001b[43mwindow_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     partitioned \u001b[38;5;241m=\u001b[39m grid_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_size)\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:635\u001b[0m, in \u001b[0;36mwindow_partition\u001b[1;34m(x, window_size)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwindow_partition\u001b[39m(x, window_size: List[\u001b[38;5;28mint\u001b[39m]):\n\u001b[0;32m    634\u001b[0m     B, H, W, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 635\u001b[0m     \u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheight (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) must be divisible by window (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m     _assert(W \u001b[38;5;241m%\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    637\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B, H \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m0\u001b[39m], window_size[\u001b[38;5;241m0\u001b[39m], W \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m], window_size[\u001b[38;5;241m1\u001b[39m], C)\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\__init__.py:853\u001b[0m, in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[1;32m--> 853\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
+      "\u001b[1;31mAssertionError\u001b[0m: height (28) must be divisible by window (8)"
+     ]
+    }
+   ],
    "source": [
     "config = SimpleNamespace(\n",
     "    batch_size=64,\n",
     "    img_size=224,\n",
     "    seed=42,\n",
     "    pretrained=False,\n",
-    "    model_name=\"regnetx_040\",\n",
-    "    epochs=5)\n",
+    "    model_name=\"maxvit_tiny_rw_224.sw_in1k\", # try with maxvit_nano_rw_256.sw_in1k # regnetx_040\n",
+    "    epochs=1)\n",
     "\n",
     "\n",
     "def train(config):\n",
     "    \"Train the model using the supplied config\"\n",
     "    dls = get_pets(config.batch_size, config.img_size, config.seed)\n",
-    "    with wandb.init(project=PROJECT, group=GROUP, job_type=JOB_TYPE, config=config):\n",
+    "    with wandb.init(project=\"PROJECT\", group='ambrosia_symbiosis', job_type='test_training', config=config):\n",
     "        cbs = [MixedPrecision(), WandbCallback(log_preds=False)]\n",
     "        learn = vision_learner(dls, config.model_name, metrics=error_rate, \n",
     "                               cbs=cbs, pretrained=config.pretrained)\n",
@@ -2121,7 +466,15 @@
    "cell_type": "code",
    "execution_count": null,
    "id": "450bfc9b-6527-41e0-803b-633e82230dea",
-   "metadata": {},
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2023-03-09T21:55:25.952196Z",
+     "iopub.status.idle": "2023-03-09T21:55:25.953194Z",
+     "shell.execute_reply": "2023-03-09T21:55:25.953194Z",
+     "shell.execute_reply.started": "2023-03-09T21:55:25.953194Z"
+    },
+    "tags": []
+   },
    "outputs": [],
    "source": [
     "def get_planets(batch_size=64, img_size=224, seed=42):\n",
@@ -2137,6 +490,24 @@
     "                                    item_tfms=Resize(img_size))\n",
     "    return dls"
    ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "dff8becf-8e4b-4b07-911c-d030ab8c12dc",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2023-03-09T21:55:25.955191Z",
+     "iopub.status.idle": "2023-03-09T21:55:25.955191Z",
+     "shell.execute_reply": "2023-03-09T21:55:25.955191Z",
+     "shell.execute_reply.started": "2023-03-09T21:55:25.955191Z"
+    },
+    "tags": []
+   },
+   "outputs": [],
+   "source": [
+    "get_planets()"
+   ]
   }
  ],
  "metadata": {
diff --git a/Train/timm_FastAI_WANDB.ipynb b/Train/timm_FastAI_WANDB.ipynb
index e6b260f..4cd4cc0 100644
--- a/Train/timm_FastAI_WANDB.ipynb
+++ b/Train/timm_FastAI_WANDB.ipynb
@@ -6,11 +6,11 @@
    "id": "436f1771-7aa6-45e7-abae-56c8112da143",
    "metadata": {
     "execution": {
-     "iopub.execute_input": "2023-03-09T17:03:52.351536Z",
-     "iopub.status.busy": "2023-03-09T17:03:52.351536Z",
-     "iopub.status.idle": "2023-03-09T17:03:57.156787Z",
-     "shell.execute_reply": "2023-03-09T17:03:57.155788Z",
-     "shell.execute_reply.started": "2023-03-09T17:03:52.351536Z"
+     "iopub.execute_input": "2023-03-09T21:54:47.601565Z",
+     "iopub.status.busy": "2023-03-09T21:54:47.600571Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.454515Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.453511Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:47.601565Z"
     },
     "tags": []
    },
@@ -18,7 +18,11 @@
    "source": [
     "from urllib.request import urlopen\n",
     "from PIL import Image\n",
-    "import timm"
+    "import timm\n",
+    "import torch\n",
+    "import wandb\n",
+    "import fastai\n",
+    "from fastai.callback.wandb import WandbCallback"
    ]
   },
   {
@@ -27,11 +31,11 @@
    "id": "5a8980b5-f22c-4e50-bb9f-8ade0c9583cc",
    "metadata": {
     "execution": {
-     "iopub.execute_input": "2023-03-09T17:03:57.158793Z",
-     "iopub.status.busy": "2023-03-09T17:03:57.158793Z",
-     "iopub.status.idle": "2023-03-09T17:03:57.172789Z",
-     "shell.execute_reply": "2023-03-09T17:03:57.171788Z",
-     "shell.execute_reply.started": "2023-03-09T17:03:57.158793Z"
+     "iopub.execute_input": "2023-03-09T21:54:52.456511Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.455499Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.470498Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.469486Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.456511Z"
     },
     "tags": []
    },
@@ -39,7 +43,7 @@
     {
      "data": {
       "text/plain": [
-       "'0.8.15dev0'"
+       "'2.7.11'"
       ]
      },
      "execution_count": 2,
@@ -48,1987 +52,51 @@
     }
    ],
    "source": [
-    "timm.__version__"
+    "fastai.__version__"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
-   "id": "ce6dcd5a-212a-4a26-ab87-8ab81952aec4",
-   "metadata": {
-    "collapsed": true,
-    "execution": {
-     "iopub.execute_input": "2023-03-09T17:06:06.091074Z",
-     "iopub.status.busy": "2023-03-09T17:06:06.091074Z",
-     "iopub.status.idle": "2023-03-09T17:37:25.330060Z",
-     "shell.execute_reply": "2023-03-09T17:37:25.329053Z",
-     "shell.execute_reply.started": "2023-03-09T17:06:06.091074Z"
-    },
-    "jupyter": {
-     "outputs_hidden": true
-    },
-    "tags": []
-   },
-   "outputs": [
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "6028ebec30904d63a0b5577fa37e86e5",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/plain": [
-       "MaxxVit(\n",
-       "  (stem): Stem(\n",
-       "    (conv1): Conv2dSame(3, 192, kernel_size=(3, 3), stride=(2, 2))\n",
-       "    (norm1): BatchNormAct2d(\n",
-       "      192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "      (drop): Identity()\n",
-       "      (act): GELUTanh()\n",
-       "    )\n",
-       "    (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
-       "  )\n",
-       "  (stages): Sequential(\n",
-       "    (0): MaxxVitStage(\n",
-       "      (blocks): Sequential(\n",
-       "        (0): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Downsample2d(\n",
-       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
-       "              (expand): Identity()\n",
-       "            )\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2dSame(768, 768, kernel_size=(3, 3), stride=(2, 2), groups=768, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (1): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "    (1): MaxxVitStage(\n",
-       "      (blocks): Sequential(\n",
-       "        (0): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Downsample2d(\n",
-       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
-       "              (expand): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            )\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(192, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2dSame(1536, 1536, kernel_size=(3, 3), stride=(2, 2), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (1): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (2): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (3): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (4): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (5): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "    (2): MaxxVitStage(\n",
-       "      (blocks): Sequential(\n",
-       "        (0): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Downsample2d(\n",
-       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
-       "              (expand): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            )\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(384, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2dSame(3072, 3072, kernel_size=(3, 3), stride=(2, 2), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (1): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (2): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (3): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (4): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (5): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (6): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (7): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (8): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (9): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (10): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (11): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (12): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (13): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "    (3): MaxxVitStage(\n",
-       "      (blocks): Sequential(\n",
-       "        (0): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Downsample2d(\n",
-       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
-       "              (expand): Conv2d(768, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            )\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(768, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              6144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2dSame(6144, 6144, kernel_size=(3, 3), stride=(2, 2), groups=6144, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              6144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(6144, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(384, 6144, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(6144, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "        (1): MaxxVitBlock(\n",
-       "          (conv): MbConvBlock(\n",
-       "            (shortcut): Identity()\n",
-       "            (pre_norm): BatchNormAct2d(\n",
-       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): Identity()\n",
-       "            )\n",
-       "            (down): Identity()\n",
-       "            (conv1_1x1): Conv2d(1536, 6144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
-       "            (norm1): BatchNormAct2d(\n",
-       "              6144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (conv2_kxk): Conv2d(6144, 6144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=6144, bias=False)\n",
-       "            (norm2): BatchNormAct2d(\n",
-       "              6144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
-       "              (drop): Identity()\n",
-       "              (act): GELUTanh()\n",
-       "            )\n",
-       "            (se): SEModule(\n",
-       "              (fc1): Conv2d(6144, 384, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (bn): Identity()\n",
-       "              (act): SiLU(inplace=True)\n",
-       "              (fc2): Conv2d(384, 6144, kernel_size=(1, 1), stride=(1, 1))\n",
-       "              (gate): Sigmoid()\n",
-       "            )\n",
-       "            (conv3_1x1): Conv2d(6144, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
-       "            (drop_path): Identity()\n",
-       "          )\n",
-       "          (attn_block): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "          (attn_grid): PartitionAttentionCl(\n",
-       "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (attn): AttentionCl(\n",
-       "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
-       "              (rel_pos): RelPosBiasTf()\n",
-       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
-       "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls1): Identity()\n",
-       "            (drop_path1): Identity()\n",
-       "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "            (mlp): Mlp(\n",
-       "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
-       "              (act): GELUTanh()\n",
-       "              (drop1): Dropout(p=0.0, inplace=False)\n",
-       "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
-       "              (drop2): Dropout(p=0.0, inplace=False)\n",
-       "            )\n",
-       "            (ls2): Identity()\n",
-       "            (drop_path2): Identity()\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "  )\n",
-       "  (norm): Identity()\n",
-       "  (head): NormMlpClassifierHead(\n",
-       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
-       "    (norm): LayerNorm2d((1536,), eps=1e-05, elementwise_affine=True)\n",
-       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
-       "    (pre_logits): Sequential(\n",
-       "      (fc): Linear(in_features=1536, out_features=1536, bias=True)\n",
-       "      (act): Tanh()\n",
-       "    )\n",
-       "    (drop): Dropout(p=0.0, inplace=False)\n",
-       "    (fc): Linear(in_features=1536, out_features=1000, bias=True)\n",
-       "  )\n",
-       ")"
-      ]
-     },
-     "execution_count": 4,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "timm.create_model(\"maxvit_xlarge_tf_512.in21k_ft_in1k\", pretrained=True)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 3,
    "id": "af673a5e-c94b-4ff2-ad5a-af508a10ca18",
    "metadata": {
     "execution": {
-     "iopub.execute_input": "2023-03-09T17:46:56.585226Z",
-     "iopub.status.busy": "2023-03-09T17:46:56.585226Z",
-     "iopub.status.idle": "2023-03-09T17:47:24.408985Z",
-     "shell.execute_reply": "2023-03-09T17:47:24.407985Z",
-     "shell.execute_reply.started": "2023-03-09T17:46:56.585226Z"
+     "iopub.execute_input": "2023-03-09T21:54:52.474510Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.473510Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.487037Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.485507Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.474510Z"
     },
     "tags": []
    },
-   "outputs": [
-    {
-     "ename": "NameError",
-     "evalue": "name 'torch' is not defined",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
-      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m transforms \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcreate_transform(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_config, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(transforms(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# unsqueeze single image into batch of 1\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m top5_probabilities, top5_class_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtopk(output\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
-      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
-    "img = Image.open(\n",
-    "    urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
+    "# img = Image.open(\n",
+    "#     urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\n",
     "\n",
-    "model = timm.create_model('maxvit_xlarge_tf_512.in21k_ft_in1k', pretrained=True) # hf-hub:timm/\n",
-    "model = model.eval()\n",
+    "# model = timm.create_model('maxvit_xlarge_tf_512.in21k_ft_in1k', pretrained=True) # hf-hub:timm/\n",
+    "# model = model.eval()\n",
     "\n",
-    "# get model specific transforms (normalization, resize)\n",
-    "data_config = timm.data.resolve_model_data_config(model)\n",
-    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
+    "# # get model specific transforms (normalization, resize)\n",
+    "# data_config = timm.data.resolve_model_data_config(model)\n",
+    "# transforms = timm.data.create_transform(**data_config, is_training=False)\n",
     "\n",
-    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
+    "# output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
     "\n",
-    "top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)"
+    "# top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 4,
    "id": "56818f26-f610-4fe1-89a0-450d6c5e93b0",
    "metadata": {
     "execution": {
-     "iopub.execute_input": "2023-03-09T17:04:02.134887Z",
-     "iopub.status.busy": "2023-03-09T17:04:02.134887Z",
-     "iopub.status.idle": "2023-03-09T17:04:02.159457Z",
-     "shell.execute_reply": "2023-03-09T17:04:02.158456Z",
-     "shell.execute_reply.started": "2023-03-09T17:04:02.134887Z"
+     "iopub.execute_input": "2023-03-09T21:54:52.489037Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.488043Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.502563Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.501568Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.489037Z"
     },
     "tags": []
    },
@@ -2071,7 +139,7 @@
        " 'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k']"
       ]
      },
-     "execution_count": 3,
+     "execution_count": 4,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2082,9 +150,17 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 5,
    "id": "fd1e36e7-6131-4ad7-a7cc-3e825b391108",
-   "metadata": {},
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2023-03-09T21:54:52.504569Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.503569Z",
+     "iopub.status.idle": "2023-03-09T21:54:52.550078Z",
+     "shell.execute_reply": "2023-03-09T21:54:52.549090Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.504569Z"
+    }
+   },
    "outputs": [],
    "source": [
     "from fastai.vision.all import *\n",
@@ -2105,24 +181,278 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 6,
    "id": "28c79cc5-2685-4d40-8b07-7a79e00ade9c",
-   "metadata": {},
-   "outputs": [],
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2023-03-09T21:54:52.552075Z",
+     "iopub.status.busy": "2023-03-09T21:54:52.551079Z",
+     "iopub.status.idle": "2023-03-09T21:55:25.952196Z",
+     "shell.execute_reply": "2023-03-09T21:55:25.950177Z",
+     "shell.execute_reply.started": "2023-03-09T21:54:52.552075Z"
+    },
+    "tags": []
+   },
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
+       " $ pip install wandb --upgrade"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Tracking run with wandb version 0.13.10"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Run data is saved locally in <code>C:\\Users\\gcmar\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\wandb\\run-20230309_165457-wo19rtpe</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe' target=\"_blank\">feasible-frost-4</a></strong> to <a href='https://wandb.ai/christopher-marais/PROJECT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View project at <a href='https://wandb.ai/christopher-marais/PROJECT' target=\"_blank\">https://wandb.ai/christopher-marais/PROJECT</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run at <a href='https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe' target=\"_blank\">https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe</a>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "C:\\Users\\gcmar\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
+      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
+     ]
+    },
+    {
+     "data": {
+      "text/html": [
+       "\n",
+       "<style>\n",
+       "    /* Turns off some styling */\n",
+       "    progress {\n",
+       "        /* gets rid of default border in Firefox and Opera. */\n",
+       "        border: none;\n",
+       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
+       "        background-size: auto;\n",
+       "    }\n",
+       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
+       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
+       "    }\n",
+       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
+       "        background: #F44336;\n",
+       "    }\n",
+       "</style>\n"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "\n",
+       "    <div>\n",
+       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
+       "      0.00% [0/1 00:00&lt;?]\n",
+       "    </div>\n",
+       "    \n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: left;\">\n",
+       "      <th>epoch</th>\n",
+       "      <th>train_loss</th>\n",
+       "      <th>valid_loss</th>\n",
+       "      <th>error_rate</th>\n",
+       "      <th>time</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "  </tbody>\n",
+       "</table><p>\n",
+       "\n",
+       "    <div>\n",
+       "      <progress value='0' class='' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
+       "      0.00% [0/92 00:00&lt;?]\n",
+       "    </div>\n",
+       "    "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "application/vnd.jupyter.widget-view+json": {
+       "model_id": "8b029004fd12455ba2368a5fc49ac0f8",
+       "version_major": 2,
+       "version_minor": 0
+      },
+      "text/plain": [
+       "VBox(children=(Label(value='2.996 MB of 11.439 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.26193â€¦"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       " View run <strong style=\"color:#cdcd00\">feasible-frost-4</strong> at: <a href='https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe' target=\"_blank\">https://wandb.ai/christopher-marais/PROJECT/runs/wo19rtpe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/html": [
+       "Find logs at: <code>.\\wandb\\run-20230309_165457-wo19rtpe\\logs</code>"
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "ename": "AssertionError",
+     "evalue": "height (28) must be divisible by window (8)",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
+      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m         learn \u001b[38;5;241m=\u001b[39m vision_learner(dls, config\u001b[38;5;241m.\u001b[39mmodel_name, metrics\u001b[38;5;241m=\u001b[39merror_rate, \n\u001b[0;32m     16\u001b[0m                                cbs\u001b[38;5;241m=\u001b[39mcbs, pretrained\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretrained)\n\u001b[0;32m     17\u001b[0m         learn\u001b[38;5;241m.\u001b[39mfine_tune(config\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
+      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     14\u001b[0m cbs \u001b[38;5;241m=\u001b[39m [MixedPrecision(), WandbCallback(log_preds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[0;32m     15\u001b[0m learn \u001b[38;5;241m=\u001b[39m vision_learner(dls, config\u001b[38;5;241m.\u001b[39mmodel_name, metrics\u001b[38;5;241m=\u001b[39merror_rate, \n\u001b[0;32m     16\u001b[0m                        cbs\u001b[38;5;241m=\u001b[39mcbs, pretrained\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretrained)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\callback\\schedule.py:165\u001b[0m, in \u001b[0;36mfine_tune\u001b[1;34m(self, epochs, base_lr, freeze_epochs, lr_mult, pct_start, div, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_one_cycle(freeze_epochs, \u001b[38;5;28mslice\u001b[39m(base_lr), pct_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m base_lr \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfreeze()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\callback\\schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[1;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[0;32m    116\u001b[0m lr_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[0;32m    117\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[38;5;241m/\u001b[39mdiv, lr_max, lr_max\u001b[38;5;241m/\u001b[39mdiv_final),\n\u001b[0;32m    118\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmom\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoms \u001b[38;5;28;01mif\u001b[39;00m moms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m moms))}\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[1;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[1;34m(self, i, b)\u001b[0m\n\u001b[0;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[1;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\learner.py:216\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_one_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_pred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb):\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\fastai\\vision\\learner.py:177\u001b[0m, in \u001b[0;36mTimmBody.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_pool \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:1257\u001b[0m, in \u001b[0;36mMaxxVit.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m   1256\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[1;32m-> 1257\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1258\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:1065\u001b[0m, in \u001b[0;36mMaxxVitStage.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1065\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:955\u001b[0m, in \u001b[0;36mMaxxVitBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    953\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# to NHWC (channels-last)\u001b[39;00m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 955\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_grid(x)\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnchw_attn:\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:739\u001b[0m, in \u001b[0;36mPartitionAttentionCl.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 739\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partition_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    740\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:726\u001b[0m, in \u001b[0;36mPartitionAttentionCl._partition_attn\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    724\u001b[0m img_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_block:\n\u001b[1;32m--> 726\u001b[0m     partitioned \u001b[38;5;241m=\u001b[39m \u001b[43mwindow_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    728\u001b[0m     partitioned \u001b[38;5;241m=\u001b[39m grid_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_size)\n",
+      "File \u001b[1;32m~\\Desktop\\GIT_REPOS\\LAB\\Beetle_classifier\\Train\\pytorch-image-models\\timm\\models\\maxxvit.py:635\u001b[0m, in \u001b[0;36mwindow_partition\u001b[1;34m(x, window_size)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwindow_partition\u001b[39m(x, window_size: List[\u001b[38;5;28mint\u001b[39m]):\n\u001b[0;32m    634\u001b[0m     B, H, W, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 635\u001b[0m     \u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheight (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) must be divisible by window (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m     _assert(W \u001b[38;5;241m%\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    637\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B, H \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m0\u001b[39m], window_size[\u001b[38;5;241m0\u001b[39m], W \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m], window_size[\u001b[38;5;241m1\u001b[39m], C)\n",
+      "File \u001b[1;32m~\\.conda\\envs\\BC_310\\lib\\site-packages\\torch\\__init__.py:853\u001b[0m, in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[1;32m--> 853\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
+      "\u001b[1;31mAssertionError\u001b[0m: height (28) must be divisible by window (8)"
+     ]
+    }
+   ],
    "source": [
     "config = SimpleNamespace(\n",
     "    batch_size=64,\n",
     "    img_size=224,\n",
     "    seed=42,\n",
     "    pretrained=False,\n",
-    "    model_name=\"regnetx_040\",\n",
-    "    epochs=5)\n",
+    "    model_name=\"maxvit_tiny_rw_224.sw_in1k\", # try with maxvit_nano_rw_256.sw_in1k # regnetx_040\n",
+    "    epochs=1)\n",
     "\n",
     "\n",
     "def train(config):\n",
     "    \"Train the model using the supplied config\"\n",
     "    dls = get_pets(config.batch_size, config.img_size, config.seed)\n",
-    "    with wandb.init(project=PROJECT, group=GROUP, job_type=JOB_TYPE, config=config):\n",
+    "    with wandb.init(project=\"PROJECT\", group='ambrosia_symbiosis', job_type='test_training', config=config):\n",
     "        cbs = [MixedPrecision(), WandbCallback(log_preds=False)]\n",
     "        learn = vision_learner(dls, config.model_name, metrics=error_rate, \n",
     "                               cbs=cbs, pretrained=config.pretrained)\n",
@@ -2136,7 +466,15 @@
    "cell_type": "code",
    "execution_count": null,
    "id": "450bfc9b-6527-41e0-803b-633e82230dea",
-   "metadata": {},
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2023-03-09T21:55:25.952196Z",
+     "iopub.status.idle": "2023-03-09T21:55:25.953194Z",
+     "shell.execute_reply": "2023-03-09T21:55:25.953194Z",
+     "shell.execute_reply.started": "2023-03-09T21:55:25.953194Z"
+    },
+    "tags": []
+   },
    "outputs": [],
    "source": [
     "def get_planets(batch_size=64, img_size=224, seed=42):\n",
@@ -2152,6 +490,24 @@
     "                                    item_tfms=Resize(img_size))\n",
     "    return dls"
    ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "dff8becf-8e4b-4b07-911c-d030ab8c12dc",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2023-03-09T21:55:25.955191Z",
+     "iopub.status.idle": "2023-03-09T21:55:25.955191Z",
+     "shell.execute_reply": "2023-03-09T21:55:25.955191Z",
+     "shell.execute_reply.started": "2023-03-09T21:55:25.955191Z"
+    },
+    "tags": []
+   },
+   "outputs": [],
+   "source": [
+    "get_planets()"
+   ]
   }
  ],
  "metadata": {
