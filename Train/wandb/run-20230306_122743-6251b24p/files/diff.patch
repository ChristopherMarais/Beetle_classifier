diff --git a/Train/.ipynb_checkpoints/PyLi_wanb_sweep_CoatNet-checkpoint.ipynb b/Train/.ipynb_checkpoints/PyLi_wanb_sweep_CoatNet-checkpoint.ipynb
index cd4d5a1..4db001c 100644
--- a/Train/.ipynb_checkpoints/PyLi_wanb_sweep_CoatNet-checkpoint.ipynb
+++ b/Train/.ipynb_checkpoints/PyLi_wanb_sweep_CoatNet-checkpoint.ipynb
@@ -287,112 +287,112 @@
    },
    "outputs": [],
    "source": [
-    "# class MyModel(LightningModule):\n",
+    "class MyModel(LightningModule):\n",
     "\n",
-    "#     def __init__(self, n_classes=10, acc_task=\"multiclass\", lr=1e-3):\n",
-    "#         super().__init__()\n",
-    "#         \"\"\"\n",
-    "#         The convolutions are arranged in such a way that the image maintain the x and y dimensions. only the channels change\n",
-    "#         \"\"\"\n",
-    "#         self.layer_1 = nn.Conv2d(in_channels = 1,out_channels = 3,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
-    "#         self.layer_2 = nn.Conv2d(in_channels = 3,out_channels = 6,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
-    "#         self.layer_3 = nn.Conv2d(in_channels = 6,out_channels = 12,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
-    "#         self.pool = nn.MaxPool2d(kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
-    "#         self.layer_5 = nn.Linear(12*50*50,1000)#the input dimensions are (Number of dimensions * height * width)\n",
-    "#         self.layer_6 = nn.Linear(1000,100)\n",
-    "#         self.layer_7 = nn.Linear(100,50)\n",
-    "#         self.layer_8 = nn.Linear(50,10)\n",
-    "#         self.layer_9 = nn.Linear(10,10)\n",
-    "#         self.lr = lr\n",
-    "#         # metrics\n",
-    "#         self.acc_task = acc_task\n",
-    "#         self.n_classes = n_classes\n",
-    "#         self.accuracy = torchmetrics.Accuracy(task=self.acc_task, num_classes=self.n_classes)\n",
-    "#         self.class_names = classes_lst\n",
-    "#         self.loss = CrossEntropyLoss()\n",
+    "    def __init__(self, n_classes=10, acc_task=\"multiclass\", lr=1e-3):\n",
+    "        super().__init__()\n",
+    "        \"\"\"\n",
+    "        The convolutions are arranged in such a way that the image maintain the x and y dimensions. only the channels change\n",
+    "        \"\"\"\n",
+    "        self.layer_1 = nn.Conv2d(in_channels = 1,out_channels = 3,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
+    "        self.layer_2 = nn.Conv2d(in_channels = 3,out_channels = 6,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
+    "        self.layer_3 = nn.Conv2d(in_channels = 6,out_channels = 12,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
+    "        self.pool = nn.MaxPool2d(kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
+    "        self.layer_5 = nn.Linear(12*50*50,1000)#the input dimensions are (Number of dimensions * height * width)\n",
+    "        self.layer_6 = nn.Linear(1000,100)\n",
+    "        self.layer_7 = nn.Linear(100,50)\n",
+    "        self.layer_8 = nn.Linear(50,10)\n",
+    "        self.layer_9 = nn.Linear(10,10)\n",
+    "        self.lr = lr\n",
+    "        # metrics\n",
+    "        self.acc_task = acc_task\n",
+    "        self.n_classes = n_classes\n",
+    "        self.accuracy = torchmetrics.Accuracy(task=self.acc_task, num_classes=self.n_classes)\n",
+    "        self.class_names = classes_lst\n",
+    "        self.loss = CrossEntropyLoss()\n",
     "\n",
-    "#         # optional - save hyper-parameters to self.hparams\n",
-    "#         # they will also be automatically logged as config parameters in W&B\n",
-    "#         self.save_hyperparameters()\n",
+    "        # optional - save hyper-parameters to self.hparams\n",
+    "        # they will also be automatically logged as config parameters in W&B\n",
+    "        self.save_hyperparameters()\n",
     "\n",
-    "#     def forward(self,x):\n",
-    "#         \"\"\"\n",
-    "#         x is the input data\n",
-    "#         \"\"\"\n",
-    "#         x = self.layer_1(x)\n",
-    "#         x = self.pool(x)\n",
-    "#         x = self.layer_2(x)\n",
-    "#         x = self.pool(x)\n",
-    "#         x = self.layer_3(x)\n",
-    "#         x = self.pool(x)\n",
-    "#         x = x.view(x.size(0),-1)\n",
-    "#         print(x.size())\n",
-    "#         x = self.layer_5(x)\n",
-    "#         x = self.layer_6(x)\n",
-    "#         x = self.layer_7(x)\n",
-    "#         x = self.layer_8(x)\n",
-    "#         x = self.layer_9(x)\n",
-    "#         return x\n",
+    "    def forward(self,x):\n",
+    "        \"\"\"\n",
+    "        x is the input data\n",
+    "        \"\"\"\n",
+    "        x = self.layer_1(x)\n",
+    "        x = self.pool(x)\n",
+    "        x = self.layer_2(x)\n",
+    "        x = self.pool(x)\n",
+    "        x = self.layer_3(x)\n",
+    "        x = self.pool(x)\n",
+    "        x = x.view(x.size(0),-1)\n",
+    "        print(x.size())\n",
+    "        x = self.layer_5(x)\n",
+    "        x = self.layer_6(x)\n",
+    "        x = self.layer_7(x)\n",
+    "        x = self.layer_8(x)\n",
+    "        x = self.layer_9(x)\n",
+    "        return x\n",
     "\n",
-    "#     def configure_optimizers(self):\n",
-    "#         optimizer = torch.optim.Adam(self.parameters(),lr = self.lr)\n",
-    "#         return optimizer\n",
+    "    def configure_optimizers(self):\n",
+    "        optimizer = torch.optim.Adam(self.parameters(),lr = self.lr)\n",
+    "        return optimizer\n",
     "\n",
-    "# # The Pytorch-Lightning module handles all the iterations of the epoch\n",
+    "# The Pytorch-Lightning module handles all the iterations of the epoch\n",
     "\n",
-    "#     def training_step(self,batch,batch_idx):\n",
-    "#         x,y = batch\n",
-    "#         y_pred = self(x)\n",
-    "#         loss = F.cross_entropy(y_pred,y)\n",
-    "#         # Log training loss\n",
-    "#         self.log('train_loss', loss)\n",
-    "#         # Log metrics\n",
-    "#         self.log('train_acc', self.accuracy(y_pred, y))\n",
-    "#         return loss\n",
+    "    def training_step(self,batch,batch_idx):\n",
+    "        x,y = batch\n",
+    "        y_pred = self(x)\n",
+    "        loss = F.cross_entropy(y_pred,y)\n",
+    "        # Log training loss\n",
+    "        self.log('train_loss', loss)\n",
+    "        # Log metrics\n",
+    "        self.log('train_acc', self.accuracy(y_pred, y))\n",
+    "        return loss\n",
     "\n",
-    "#     def validation_step(self,batch,batch_idx):\n",
-    "#         preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
-    "#         # Log loss and metric\n",
-    "#         self.log('val_loss_alt', loss)\n",
-    "#         self.log('val_accuracy_alt', acc)\n",
+    "    def validation_step(self,batch,batch_idx):\n",
+    "        preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
+    "        # Log loss and metric\n",
+    "        self.log('val_loss_alt', loss)\n",
+    "        self.log('val_accuracy_alt', acc)\n",
     "        \n",
-    "#         x,y = batch\n",
-    "#         y_pred = self(x)\n",
-    "#         loss = F.cross_entropy(y_pred,y)\n",
-    "#         # Log training loss\n",
-    "#         self.log('val_loss', loss)\n",
-    "#         # Log metrics\n",
-    "#         self.log('val_acc', self.accuracy(y_pred, y))\n",
-    "#         self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
-    "#         self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
-    "#         wandb.log({\"val_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
-    "#                         y_true=self.cpu_y, preds=None,\n",
-    "#                         class_names=self.class_names)})\n",
-    "#         return preds\n",
+    "        x,y = batch\n",
+    "        y_pred = self(x)\n",
+    "        loss = F.cross_entropy(y_pred,y)\n",
+    "        # Log training loss\n",
+    "        self.log('val_loss', loss)\n",
+    "        # Log metrics\n",
+    "        self.log('val_acc', self.accuracy(y_pred, y))\n",
+    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
+    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
+    "        wandb.log({\"val_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
+    "                        y_true=self.cpu_y, preds=None,\n",
+    "                        class_names=self.class_names)})\n",
+    "        return preds\n",
     "\n",
-    "#     def test_step(self,batch,batch_idx):\n",
-    "#         x,y = batch\n",
-    "#         y_pred = self(x)\n",
-    "#         loss = F.cross_entropy(y_pred,y)\n",
-    "#         # Log training loss\n",
-    "#         self.log('test_loss', loss)\n",
-    "#         # Log metrics\n",
-    "#         self.log('test_acc', self.accuracy(y_pred, y))\n",
-    "#         self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
-    "#         self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
-    "#         wandb.log({\"test_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
-    "#                         y_true=self.cpu_y, preds=None,\n",
-    "#                         class_names=self.class_names)})\n",
-    "#         return loss\n",
+    "    def test_step(self,batch,batch_idx):\n",
+    "        x,y = batch\n",
+    "        y_pred = self(x)\n",
+    "        loss = F.cross_entropy(y_pred,y)\n",
+    "        # Log training loss\n",
+    "        self.log('test_loss', loss)\n",
+    "        # Log metrics\n",
+    "        self.log('test_acc', self.accuracy(y_pred, y))\n",
+    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
+    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
+    "        wandb.log({\"test_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
+    "                        y_true=self.cpu_y, preds=None,\n",
+    "                        class_names=self.class_names)})\n",
+    "        return loss\n",
     "    \n",
-    "#     def _get_preds_loss_accuracy(self, batch):\n",
-    "#         '''convenience function since train/valid/test steps are similar'''\n",
-    "#         x, y = batch\n",
-    "#         logits = self(x)\n",
-    "#         preds = torch.argmax(logits, dim=1)\n",
-    "#         loss = self.loss(logits, y)\n",
-    "#         acc = accuracy(preds, y, self.acc_task, num_classes=10)\n",
-    "#         return preds, loss, acc"
+    "    def _get_preds_loss_accuracy(self, batch):\n",
+    "        '''convenience function since train/valid/test steps are similar'''\n",
+    "        x, y = batch\n",
+    "        logits = self(x)\n",
+    "        preds = torch.argmax(logits, dim=1)\n",
+    "        loss = self.loss(logits, y)\n",
+    "        acc = accuracy(preds, y, self.acc_task, num_classes=10)\n",
+    "        return preds, loss, acc"
    ]
   },
   {
@@ -410,84 +410,84 @@
    },
    "outputs": [],
    "source": [
-    "class MyModel(LightningModule): \n",
+    "# class MyModel(LightningModule): \n",
     "\n",
-    "    def __init__(self, n_classes=10, n_layer_1=128, n_layer_2=256, lr=1e-3): \n",
-    "        '''method used to define our model parameters'''\n",
-    "        super().__init__()\n",
+    "#     def __init__(self, n_classes=10, n_layer_1=128, n_layer_2=256, lr=1e-3): \n",
+    "#         '''method used to define our model parameters'''\n",
+    "#         super().__init__()\n",
     "\n",
-    "        # mnist images are (1, 28, 28) (channels, width, height\n",
-    "        self.layer_1 = Linear(28 * 28, n_layer_1)\n",
-    "        self.layer_2 = Linear(n_layer_1, n_layer_2)\n",
-    "        self.layer_3 = Linear(n_layer_2, n_classes)\n",
+    "#         # mnist images are (1, 28, 28) (channels, width, height\n",
+    "#         self.layer_1 = Linear(28 * 28, n_layer_1)\n",
+    "#         self.layer_2 = Linear(n_layer_1, n_layer_2)\n",
+    "#         self.layer_3 = Linear(n_layer_2, n_classes)\n",
     "\n",
-    "        # loss\n",
-    "        self.loss = CrossEntropyLoss()\n",
+    "#         # loss\n",
+    "#         self.loss = CrossEntropyLoss()\n",
     "\n",
-    "        # optimizer parameters\n",
-    "        self.lr = lr\n",
+    "#         # optimizer parameters\n",
+    "#         self.lr = lr\n",
     "\n",
-    "        # save hyper-parameters to self.hparams (auto-logged by W&B)\n",
-    "        self.save_hyperparameters() \n",
+    "#         # save hyper-parameters to self.hparams (auto-logged by W&B)\n",
+    "#         self.save_hyperparameters() \n",
     "\n",
-    "    def forward(self, x):\n",
-    "        '''method used for inference input -> output'''\n",
+    "#     def forward(self, x):\n",
+    "#         '''method used for inference input -> output'''\n",
     "\n",
-    "        batch_size, channels, width, height = x.size()\n",
+    "#         batch_size, channels, width, height = x.size()\n",
     "\n",
-    "        # (b, 1, 28, 28) -> (b, 1*28*28)\n",
-    "        x = x.view(batch_size, -1)\n",
+    "#         # (b, 1, 28, 28) -> (b, 1*28*28)\n",
+    "#         x = x.view(batch_size, -1)\n",
     "\n",
-    "        # let's do 3 x (linear + relu)\n",
-    "        x = self.layer_1(x)\n",
-    "        x = F.relu(x)\n",
-    "        x = self.layer_2(x)\n",
-    "        x = F.relu(x)\n",
-    "        x = self.layer_3(x)\n",
+    "#         # let's do 3 x (linear + relu)\n",
+    "#         x = self.layer_1(x)\n",
+    "#         x = F.relu(x)\n",
+    "#         x = self.layer_2(x)\n",
+    "#         x = F.relu(x)\n",
+    "#         x = self.layer_3(x)\n",
     "\n",
-    "        return x\n",
+    "#         return x\n",
     "\n",
-    "    def training_step(self, batch, batch_idx):\n",
-    "        '''needs to return a loss from a single batch'''\n",
-    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
+    "#     def training_step(self, batch, batch_idx):\n",
+    "#         '''needs to return a loss from a single batch'''\n",
+    "#         _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
     "\n",
-    "        # Log loss and metric\n",
-    "        self.log('train_loss', loss)\n",
-    "        self.log('train_accuracy', acc)\n",
+    "#         # Log loss and metric\n",
+    "#         self.log('train_loss', loss)\n",
+    "#         self.log('train_accuracy', acc)\n",
     "\n",
-    "        return loss\n",
+    "#         return loss\n",
     "\n",
-    "    def validation_step(self, batch, batch_idx):\n",
-    "        '''used for logging metrics'''\n",
-    "        preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
+    "#     def validation_step(self, batch, batch_idx):\n",
+    "#         '''used for logging metrics'''\n",
+    "#         preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
     "\n",
-    "        # Log loss and metric\n",
-    "        self.log('val_loss', loss)\n",
-    "        self.log('val_accuracy', acc)\n",
+    "#         # Log loss and metric\n",
+    "#         self.log('val_loss', loss)\n",
+    "#         self.log('val_accuracy', acc)\n",
     "\n",
-    "        # Let's return preds to use it in a custom callback\n",
-    "        return preds\n",
+    "#         # Let's return preds to use it in a custom callback\n",
+    "#         return preds\n",
     "\n",
-    "    def test_step(self, batch, batch_idx):\n",
-    "        '''used for logging metrics'''\n",
-    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
+    "#     def test_step(self, batch, batch_idx):\n",
+    "#         '''used for logging metrics'''\n",
+    "#         _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
     "\n",
-    "        # Log loss and metric\n",
-    "        self.log('test_loss', loss)\n",
-    "        self.log('test_accuracy', acc)\n",
+    "#         # Log loss and metric\n",
+    "#         self.log('test_loss', loss)\n",
+    "#         self.log('test_accuracy', acc)\n",
     "    \n",
-    "    def configure_optimizers(self):\n",
-    "        '''defines model optimizer'''\n",
-    "        return Adam(self.parameters(), lr=self.lr)\n",
+    "#     def configure_optimizers(self):\n",
+    "#         '''defines model optimizer'''\n",
+    "#         return Adam(self.parameters(), lr=self.lr)\n",
     "    \n",
-    "    def _get_preds_loss_accuracy(self, batch):\n",
-    "        '''convenience function since train/valid/test steps are similar'''\n",
-    "        x, y = batch\n",
-    "        logits = self(x)\n",
-    "        preds = torch.argmax(logits, dim=1)\n",
-    "        loss = self.loss(logits, y)\n",
-    "        acc = accuracy(preds, y, 'multiclass', num_classes=10)\n",
-    "        return preds, loss, acc"
+    "#     def _get_preds_loss_accuracy(self, batch):\n",
+    "#         '''convenience function since train/valid/test steps are similar'''\n",
+    "#         x, y = batch\n",
+    "#         logits = self(x)\n",
+    "#         preds = torch.argmax(logits, dim=1)\n",
+    "#         loss = self.loss(logits, y)\n",
+    "#         acc = accuracy(preds, y, 'multiclass', num_classes=10)\n",
+    "#         return preds, loss, acc"
    ]
   },
   {
diff --git a/Train/PyLi_wanb_sweep_CoatNet.ipynb b/Train/PyLi_wanb_sweep_CoatNet.ipynb
index cd4d5a1..4db001c 100644
--- a/Train/PyLi_wanb_sweep_CoatNet.ipynb
+++ b/Train/PyLi_wanb_sweep_CoatNet.ipynb
@@ -287,112 +287,112 @@
    },
    "outputs": [],
    "source": [
-    "# class MyModel(LightningModule):\n",
+    "class MyModel(LightningModule):\n",
     "\n",
-    "#     def __init__(self, n_classes=10, acc_task=\"multiclass\", lr=1e-3):\n",
-    "#         super().__init__()\n",
-    "#         \"\"\"\n",
-    "#         The convolutions are arranged in such a way that the image maintain the x and y dimensions. only the channels change\n",
-    "#         \"\"\"\n",
-    "#         self.layer_1 = nn.Conv2d(in_channels = 1,out_channels = 3,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
-    "#         self.layer_2 = nn.Conv2d(in_channels = 3,out_channels = 6,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
-    "#         self.layer_3 = nn.Conv2d(in_channels = 6,out_channels = 12,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
-    "#         self.pool = nn.MaxPool2d(kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
-    "#         self.layer_5 = nn.Linear(12*50*50,1000)#the input dimensions are (Number of dimensions * height * width)\n",
-    "#         self.layer_6 = nn.Linear(1000,100)\n",
-    "#         self.layer_7 = nn.Linear(100,50)\n",
-    "#         self.layer_8 = nn.Linear(50,10)\n",
-    "#         self.layer_9 = nn.Linear(10,10)\n",
-    "#         self.lr = lr\n",
-    "#         # metrics\n",
-    "#         self.acc_task = acc_task\n",
-    "#         self.n_classes = n_classes\n",
-    "#         self.accuracy = torchmetrics.Accuracy(task=self.acc_task, num_classes=self.n_classes)\n",
-    "#         self.class_names = classes_lst\n",
-    "#         self.loss = CrossEntropyLoss()\n",
+    "    def __init__(self, n_classes=10, acc_task=\"multiclass\", lr=1e-3):\n",
+    "        super().__init__()\n",
+    "        \"\"\"\n",
+    "        The convolutions are arranged in such a way that the image maintain the x and y dimensions. only the channels change\n",
+    "        \"\"\"\n",
+    "        self.layer_1 = nn.Conv2d(in_channels = 1,out_channels = 3,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
+    "        self.layer_2 = nn.Conv2d(in_channels = 3,out_channels = 6,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
+    "        self.layer_3 = nn.Conv2d(in_channels = 6,out_channels = 12,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
+    "        self.pool = nn.MaxPool2d(kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
+    "        self.layer_5 = nn.Linear(12*50*50,1000)#the input dimensions are (Number of dimensions * height * width)\n",
+    "        self.layer_6 = nn.Linear(1000,100)\n",
+    "        self.layer_7 = nn.Linear(100,50)\n",
+    "        self.layer_8 = nn.Linear(50,10)\n",
+    "        self.layer_9 = nn.Linear(10,10)\n",
+    "        self.lr = lr\n",
+    "        # metrics\n",
+    "        self.acc_task = acc_task\n",
+    "        self.n_classes = n_classes\n",
+    "        self.accuracy = torchmetrics.Accuracy(task=self.acc_task, num_classes=self.n_classes)\n",
+    "        self.class_names = classes_lst\n",
+    "        self.loss = CrossEntropyLoss()\n",
     "\n",
-    "#         # optional - save hyper-parameters to self.hparams\n",
-    "#         # they will also be automatically logged as config parameters in W&B\n",
-    "#         self.save_hyperparameters()\n",
+    "        # optional - save hyper-parameters to self.hparams\n",
+    "        # they will also be automatically logged as config parameters in W&B\n",
+    "        self.save_hyperparameters()\n",
     "\n",
-    "#     def forward(self,x):\n",
-    "#         \"\"\"\n",
-    "#         x is the input data\n",
-    "#         \"\"\"\n",
-    "#         x = self.layer_1(x)\n",
-    "#         x = self.pool(x)\n",
-    "#         x = self.layer_2(x)\n",
-    "#         x = self.pool(x)\n",
-    "#         x = self.layer_3(x)\n",
-    "#         x = self.pool(x)\n",
-    "#         x = x.view(x.size(0),-1)\n",
-    "#         print(x.size())\n",
-    "#         x = self.layer_5(x)\n",
-    "#         x = self.layer_6(x)\n",
-    "#         x = self.layer_7(x)\n",
-    "#         x = self.layer_8(x)\n",
-    "#         x = self.layer_9(x)\n",
-    "#         return x\n",
+    "    def forward(self,x):\n",
+    "        \"\"\"\n",
+    "        x is the input data\n",
+    "        \"\"\"\n",
+    "        x = self.layer_1(x)\n",
+    "        x = self.pool(x)\n",
+    "        x = self.layer_2(x)\n",
+    "        x = self.pool(x)\n",
+    "        x = self.layer_3(x)\n",
+    "        x = self.pool(x)\n",
+    "        x = x.view(x.size(0),-1)\n",
+    "        print(x.size())\n",
+    "        x = self.layer_5(x)\n",
+    "        x = self.layer_6(x)\n",
+    "        x = self.layer_7(x)\n",
+    "        x = self.layer_8(x)\n",
+    "        x = self.layer_9(x)\n",
+    "        return x\n",
     "\n",
-    "#     def configure_optimizers(self):\n",
-    "#         optimizer = torch.optim.Adam(self.parameters(),lr = self.lr)\n",
-    "#         return optimizer\n",
+    "    def configure_optimizers(self):\n",
+    "        optimizer = torch.optim.Adam(self.parameters(),lr = self.lr)\n",
+    "        return optimizer\n",
     "\n",
-    "# # The Pytorch-Lightning module handles all the iterations of the epoch\n",
+    "# The Pytorch-Lightning module handles all the iterations of the epoch\n",
     "\n",
-    "#     def training_step(self,batch,batch_idx):\n",
-    "#         x,y = batch\n",
-    "#         y_pred = self(x)\n",
-    "#         loss = F.cross_entropy(y_pred,y)\n",
-    "#         # Log training loss\n",
-    "#         self.log('train_loss', loss)\n",
-    "#         # Log metrics\n",
-    "#         self.log('train_acc', self.accuracy(y_pred, y))\n",
-    "#         return loss\n",
+    "    def training_step(self,batch,batch_idx):\n",
+    "        x,y = batch\n",
+    "        y_pred = self(x)\n",
+    "        loss = F.cross_entropy(y_pred,y)\n",
+    "        # Log training loss\n",
+    "        self.log('train_loss', loss)\n",
+    "        # Log metrics\n",
+    "        self.log('train_acc', self.accuracy(y_pred, y))\n",
+    "        return loss\n",
     "\n",
-    "#     def validation_step(self,batch,batch_idx):\n",
-    "#         preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
-    "#         # Log loss and metric\n",
-    "#         self.log('val_loss_alt', loss)\n",
-    "#         self.log('val_accuracy_alt', acc)\n",
+    "    def validation_step(self,batch,batch_idx):\n",
+    "        preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
+    "        # Log loss and metric\n",
+    "        self.log('val_loss_alt', loss)\n",
+    "        self.log('val_accuracy_alt', acc)\n",
     "        \n",
-    "#         x,y = batch\n",
-    "#         y_pred = self(x)\n",
-    "#         loss = F.cross_entropy(y_pred,y)\n",
-    "#         # Log training loss\n",
-    "#         self.log('val_loss', loss)\n",
-    "#         # Log metrics\n",
-    "#         self.log('val_acc', self.accuracy(y_pred, y))\n",
-    "#         self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
-    "#         self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
-    "#         wandb.log({\"val_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
-    "#                         y_true=self.cpu_y, preds=None,\n",
-    "#                         class_names=self.class_names)})\n",
-    "#         return preds\n",
+    "        x,y = batch\n",
+    "        y_pred = self(x)\n",
+    "        loss = F.cross_entropy(y_pred,y)\n",
+    "        # Log training loss\n",
+    "        self.log('val_loss', loss)\n",
+    "        # Log metrics\n",
+    "        self.log('val_acc', self.accuracy(y_pred, y))\n",
+    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
+    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
+    "        wandb.log({\"val_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
+    "                        y_true=self.cpu_y, preds=None,\n",
+    "                        class_names=self.class_names)})\n",
+    "        return preds\n",
     "\n",
-    "#     def test_step(self,batch,batch_idx):\n",
-    "#         x,y = batch\n",
-    "#         y_pred = self(x)\n",
-    "#         loss = F.cross_entropy(y_pred,y)\n",
-    "#         # Log training loss\n",
-    "#         self.log('test_loss', loss)\n",
-    "#         # Log metrics\n",
-    "#         self.log('test_acc', self.accuracy(y_pred, y))\n",
-    "#         self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
-    "#         self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
-    "#         wandb.log({\"test_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
-    "#                         y_true=self.cpu_y, preds=None,\n",
-    "#                         class_names=self.class_names)})\n",
-    "#         return loss\n",
+    "    def test_step(self,batch,batch_idx):\n",
+    "        x,y = batch\n",
+    "        y_pred = self(x)\n",
+    "        loss = F.cross_entropy(y_pred,y)\n",
+    "        # Log training loss\n",
+    "        self.log('test_loss', loss)\n",
+    "        # Log metrics\n",
+    "        self.log('test_acc', self.accuracy(y_pred, y))\n",
+    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
+    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
+    "        wandb.log({\"test_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
+    "                        y_true=self.cpu_y, preds=None,\n",
+    "                        class_names=self.class_names)})\n",
+    "        return loss\n",
     "    \n",
-    "#     def _get_preds_loss_accuracy(self, batch):\n",
-    "#         '''convenience function since train/valid/test steps are similar'''\n",
-    "#         x, y = batch\n",
-    "#         logits = self(x)\n",
-    "#         preds = torch.argmax(logits, dim=1)\n",
-    "#         loss = self.loss(logits, y)\n",
-    "#         acc = accuracy(preds, y, self.acc_task, num_classes=10)\n",
-    "#         return preds, loss, acc"
+    "    def _get_preds_loss_accuracy(self, batch):\n",
+    "        '''convenience function since train/valid/test steps are similar'''\n",
+    "        x, y = batch\n",
+    "        logits = self(x)\n",
+    "        preds = torch.argmax(logits, dim=1)\n",
+    "        loss = self.loss(logits, y)\n",
+    "        acc = accuracy(preds, y, self.acc_task, num_classes=10)\n",
+    "        return preds, loss, acc"
    ]
   },
   {
@@ -410,84 +410,84 @@
    },
    "outputs": [],
    "source": [
-    "class MyModel(LightningModule): \n",
+    "# class MyModel(LightningModule): \n",
     "\n",
-    "    def __init__(self, n_classes=10, n_layer_1=128, n_layer_2=256, lr=1e-3): \n",
-    "        '''method used to define our model parameters'''\n",
-    "        super().__init__()\n",
+    "#     def __init__(self, n_classes=10, n_layer_1=128, n_layer_2=256, lr=1e-3): \n",
+    "#         '''method used to define our model parameters'''\n",
+    "#         super().__init__()\n",
     "\n",
-    "        # mnist images are (1, 28, 28) (channels, width, height\n",
-    "        self.layer_1 = Linear(28 * 28, n_layer_1)\n",
-    "        self.layer_2 = Linear(n_layer_1, n_layer_2)\n",
-    "        self.layer_3 = Linear(n_layer_2, n_classes)\n",
+    "#         # mnist images are (1, 28, 28) (channels, width, height\n",
+    "#         self.layer_1 = Linear(28 * 28, n_layer_1)\n",
+    "#         self.layer_2 = Linear(n_layer_1, n_layer_2)\n",
+    "#         self.layer_3 = Linear(n_layer_2, n_classes)\n",
     "\n",
-    "        # loss\n",
-    "        self.loss = CrossEntropyLoss()\n",
+    "#         # loss\n",
+    "#         self.loss = CrossEntropyLoss()\n",
     "\n",
-    "        # optimizer parameters\n",
-    "        self.lr = lr\n",
+    "#         # optimizer parameters\n",
+    "#         self.lr = lr\n",
     "\n",
-    "        # save hyper-parameters to self.hparams (auto-logged by W&B)\n",
-    "        self.save_hyperparameters() \n",
+    "#         # save hyper-parameters to self.hparams (auto-logged by W&B)\n",
+    "#         self.save_hyperparameters() \n",
     "\n",
-    "    def forward(self, x):\n",
-    "        '''method used for inference input -> output'''\n",
+    "#     def forward(self, x):\n",
+    "#         '''method used for inference input -> output'''\n",
     "\n",
-    "        batch_size, channels, width, height = x.size()\n",
+    "#         batch_size, channels, width, height = x.size()\n",
     "\n",
-    "        # (b, 1, 28, 28) -> (b, 1*28*28)\n",
-    "        x = x.view(batch_size, -1)\n",
+    "#         # (b, 1, 28, 28) -> (b, 1*28*28)\n",
+    "#         x = x.view(batch_size, -1)\n",
     "\n",
-    "        # let's do 3 x (linear + relu)\n",
-    "        x = self.layer_1(x)\n",
-    "        x = F.relu(x)\n",
-    "        x = self.layer_2(x)\n",
-    "        x = F.relu(x)\n",
-    "        x = self.layer_3(x)\n",
+    "#         # let's do 3 x (linear + relu)\n",
+    "#         x = self.layer_1(x)\n",
+    "#         x = F.relu(x)\n",
+    "#         x = self.layer_2(x)\n",
+    "#         x = F.relu(x)\n",
+    "#         x = self.layer_3(x)\n",
     "\n",
-    "        return x\n",
+    "#         return x\n",
     "\n",
-    "    def training_step(self, batch, batch_idx):\n",
-    "        '''needs to return a loss from a single batch'''\n",
-    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
+    "#     def training_step(self, batch, batch_idx):\n",
+    "#         '''needs to return a loss from a single batch'''\n",
+    "#         _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
     "\n",
-    "        # Log loss and metric\n",
-    "        self.log('train_loss', loss)\n",
-    "        self.log('train_accuracy', acc)\n",
+    "#         # Log loss and metric\n",
+    "#         self.log('train_loss', loss)\n",
+    "#         self.log('train_accuracy', acc)\n",
     "\n",
-    "        return loss\n",
+    "#         return loss\n",
     "\n",
-    "    def validation_step(self, batch, batch_idx):\n",
-    "        '''used for logging metrics'''\n",
-    "        preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
+    "#     def validation_step(self, batch, batch_idx):\n",
+    "#         '''used for logging metrics'''\n",
+    "#         preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
     "\n",
-    "        # Log loss and metric\n",
-    "        self.log('val_loss', loss)\n",
-    "        self.log('val_accuracy', acc)\n",
+    "#         # Log loss and metric\n",
+    "#         self.log('val_loss', loss)\n",
+    "#         self.log('val_accuracy', acc)\n",
     "\n",
-    "        # Let's return preds to use it in a custom callback\n",
-    "        return preds\n",
+    "#         # Let's return preds to use it in a custom callback\n",
+    "#         return preds\n",
     "\n",
-    "    def test_step(self, batch, batch_idx):\n",
-    "        '''used for logging metrics'''\n",
-    "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
+    "#     def test_step(self, batch, batch_idx):\n",
+    "#         '''used for logging metrics'''\n",
+    "#         _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
     "\n",
-    "        # Log loss and metric\n",
-    "        self.log('test_loss', loss)\n",
-    "        self.log('test_accuracy', acc)\n",
+    "#         # Log loss and metric\n",
+    "#         self.log('test_loss', loss)\n",
+    "#         self.log('test_accuracy', acc)\n",
     "    \n",
-    "    def configure_optimizers(self):\n",
-    "        '''defines model optimizer'''\n",
-    "        return Adam(self.parameters(), lr=self.lr)\n",
+    "#     def configure_optimizers(self):\n",
+    "#         '''defines model optimizer'''\n",
+    "#         return Adam(self.parameters(), lr=self.lr)\n",
     "    \n",
-    "    def _get_preds_loss_accuracy(self, batch):\n",
-    "        '''convenience function since train/valid/test steps are similar'''\n",
-    "        x, y = batch\n",
-    "        logits = self(x)\n",
-    "        preds = torch.argmax(logits, dim=1)\n",
-    "        loss = self.loss(logits, y)\n",
-    "        acc = accuracy(preds, y, 'multiclass', num_classes=10)\n",
-    "        return preds, loss, acc"
+    "#     def _get_preds_loss_accuracy(self, batch):\n",
+    "#         '''convenience function since train/valid/test steps are similar'''\n",
+    "#         x, y = batch\n",
+    "#         logits = self(x)\n",
+    "#         preds = torch.argmax(logits, dim=1)\n",
+    "#         loss = self.loss(logits, y)\n",
+    "#         acc = accuracy(preds, y, 'multiclass', num_classes=10)\n",
+    "#         return preds, loss, acc"
    ]
   },
   {
