C:\Users\gcmar\.conda\envs\BC_310\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\gcmar\.conda\envs\BC_310\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name              | Type               | Params
---------------------------------------------------------
0 | accuracy          | MulticlassAccuracy | 0
1 | loss              | CrossEntropyLoss   | 0
2 | feature_extractor | ResNet             | 11.7 M
3 | classifier        | Linear             | 10.0 K
---------------------------------------------------------
11.7 M    Trainable params
0         Non-trainable params
11.7 M    Total params
46.798    Total estimated model params size (MB)
C:\Users\gcmar\.conda\envs\BC_310\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.
  rank_zero_warn(
C:\Users\gcmar\.conda\envs\BC_310\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
C:\Users\gcmar\.conda\envs\BC_310\lib\site-packages\pytorch_lightning\loggers\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name              | Type               | Params
---------------------------------------------------------
0 | accuracy          | MulticlassAccuracy | 0
1 | loss              | CrossEntropyLoss   | 0
2 | feature_extractor | ResNet             | 11.7 M
3 | classifier        | Linear             | 10.0 K
---------------------------------------------------------
11.7 M    Trainable params
0         Non-trainable params
11.7 M    Total params
46.798    Total estimated model params size (MB)
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 4
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 8
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 8
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 6
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 8
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 2
Feature batch shape: torch.Size([2, 1, 224, 224])
Labels batch shape: torch.Size([2])
Label: 3
Feature batch shape: torch.Size([2, 1, 224, 224])
Labels batch shape: torch.Size([2])
Label: 6
Feature batch shape: torch.Size([2, 1, 224, 224])
Labels batch shape: torch.Size([2])
Label: 8
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 0
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 2
Feature batch shape: torch.Size([10, 1, 256, 256])
Labels batch shape: torch.Size([10])
Label: 3
Feature batch shape: torch.Size([10, 1, 7, 7])
Labels batch shape: torch.Size([10])
Label: 7
Feature batch shape: torch.Size([10, 1, 28, 28])
Labels batch shape: torch.Size([10])
Label: 5
Feature batch shape: torch.Size([10, 1, 28, 28])
Labels batch shape: torch.Size([10])
Label: 6
Feature batch shape: torch.Size([10, 1, 28, 28])
Labels batch shape: torch.Size([10])
Label: 3
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 9
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 6
Feature batch shape: torch.Size([8, 1, 224, 224])
Labels batch shape: torch.Size([8])
Label: 8
Feature batch shape: torch.Size([10, 1, 224, 224])
Labels batch shape: torch.Size([10])
Label: 8
Feature batch shape: torch.Size([8, 1, 224, 224])
Labels batch shape: torch.Size([8])
