{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "675453aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "from random import randint\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Pytorch modules\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "# Pytorch-Lightning\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchmetrics # a new pakage for torchmetrics\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "# pytorch dependent packages\n",
    "import timm # try using maxvit from torchvision it should be just as good if not better\n",
    "\n",
    "# sci-kit learn and scikit-image\n",
    "import sklearn\n",
    "import skimage\n",
    "\n",
    "# Dataset\n",
    "from torchvision.datasets import MNIST ######### not required\n",
    "from torchvision import transforms\n",
    "\n",
    "# to describe model\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "# number of CPUs\n",
    "# cpu_count = 0 if torch.cuda.is_available() else os.cpu_count()\n",
    "\n",
    "# use GPU tensor cores\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046f9bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coatnet_0_rw_224',\n",
      " 'coatnet_1_rw_224',\n",
      " 'coatnet_bn_0_rw_224',\n",
      " 'coatnet_nano_rw_224',\n",
      " 'coatnet_rmlp_1_rw_224',\n",
      " 'coatnet_rmlp_2_rw_224',\n",
      " 'coatnet_rmlp_nano_rw_224']"
     ]
    }
   ],
   "source": [
    "timm.list_models('*coatnet*', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5640f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# model = timm.create_model('coatnet_rmlp_2_rw_224', pretrained=True, exportable=True, num_classes=10)\n",
    "\n",
    "# # see the output head shape\n",
    "# clsfr_shape = model.get_classifier()\n",
    "\n",
    "# # get all node names\n",
    "# nodes,_ = get_graph_node_names(model)\n",
    "\n",
    "# # get all layers\n",
    "# modules = list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed4e500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0,\n",
      " '1': 1,\n",
      " '2': 2,\n",
      " '3': 3,\n",
      " '4': 4,\n",
      " '5': 5,\n",
      " '6': 6,\n",
      " '7': 7,\n",
      " '8': 8,\n",
      " '9': 9}"
     ]
    }
   ],
   "source": [
    "# declaring the path of the train and test folders\n",
    "train_path = \"DATASET_C/TRAIN\"\n",
    "test_path = \"DATASET_C/TEST\"\n",
    "classes_dir_data = os.listdir(train_path)\n",
    "num_of_classes = len(classes_dir_data)\n",
    "print(\"Total Number of Classes :\" , num_of_classes)\n",
    "num = 0\n",
    "classes_dict = {}\n",
    "classes_lst = []\n",
    "num_dict = {}\n",
    "for c in  classes_dir_data:\n",
    "    classes_dict[c] = num\n",
    "    num_dict[num] = c\n",
    "    classes_lst.append(c)\n",
    "    num = num +1\n",
    "\"\"\"\n",
    "num_dict contains a dictionary of the classes numerically and it's corresponding classes.\n",
    "classes_dict contains a dictionary of the classes and the coresponding values numerically.\n",
    "\"\"\"\n",
    "num_of_classes = len(classes_dir_data)\n",
    "\n",
    "classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b30c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dataset\n",
    "\n",
    "#dataset\n",
    "\n",
    "class Image_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self,classes,image_base_dir,transform = None, target_transform = None):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        classes:The classes in the dataset\n",
    "\n",
    "        image_base_dir:The directory of the folders containing the images\n",
    "\n",
    "        transform:The trasformations for the Images\n",
    "\n",
    "        Target_transform:The trasformations for the target\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.img_labels = classes\n",
    "\n",
    "        self.imge_base_dir = image_base_dir\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        img_dir_list = os.listdir(os.path.join(self.imge_base_dir,self.img_labels[idx]))\n",
    "\n",
    "        image_path = img_dir_list[randint(0,len(img_dir_list)-1)]\n",
    "\n",
    "        #print(image_path)\n",
    "\n",
    "        image_path = os.path.join(self.imge_base_dir,self.img_labels[idx],image_path)\n",
    "\n",
    "        image = skimage.io.imread(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            label = self.target_transform(self.img_labels[idx])\n",
    "\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7101d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 50 # need to be the same as what is used in layer_5/ input layer ot the cnn\n",
    "\n",
    "basic_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((size,size)),\n",
    "        transforms.Grayscale(1),\n",
    "    transforms.ToTensor()])\n",
    "training_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((size,size)),\n",
    "    transforms.RandomRotation(degrees = 45),\n",
    "    transforms.RandomHorizontalFlip(p = 0.005),\n",
    "        transforms.Grayscale(1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def target_transformations(x):\n",
    "    return torch.tensor(classes_dict.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82799b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YogaDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self):\n",
    "            super().__init__()            \n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train = Image_Dataset(classes_dir_data,train_path,training_transformations,target_transformations)\n",
    "        self.valid = Image_Dataset(classes_dir_data,test_path,basic_transformations,target_transformations)\n",
    "        self.test = Image_Dataset(classes_dir_data,test_path,basic_transformations,target_transformations)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train,batch_size = 64,shuffle = True)#False, num_workers = cpu_count)\n",
    "\n",
    "    def val_dataloader(self):  \n",
    "        return DataLoader(self.valid,batch_size = 64,shuffle = True)#False, num_workers = cpu_count)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test,batch_size = 64,shuffle = True)#False, num_workers = cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b1f0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "\n",
    "    def __init__(self, n_classes=10, acc_task=\"multiclass\", n_layer_1=128, n_layer_2=256, lr=1e-3):\n",
    "        '''method used to define our model parameters'''\n",
    "        super().__init__()\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, n_layer_1)\n",
    "        self.layer_2 = torch.nn.Linear(n_layer_1, n_layer_2)\n",
    "        self.layer_3 = torch.nn.Linear(n_layer_2, n_classes)\n",
    "        # optimizer parameters\n",
    "        self.lr = lr\n",
    "        # metrics\n",
    "        self.acc_task = acc_task\n",
    "        self.n_classes = n_classes\n",
    "        self.accuracy = torchmetrics.Accuracy(task=self.acc_task, num_classes=self.n_classes)\n",
    "        self.class_names = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "        # optional - save hyper-parameters to self.hparams\n",
    "        # they will also be automatically logged as config parameters in W&B\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''method used for inference input -> output'''\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        # (b, 1, 28, 28) -> (b, 1*28*28)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.layer_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''needs to return a loss from a single batch'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('train_acc', self.accuracy(logits, y))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        # Log validation loss (will be automatically averaged over an epoch)\n",
    "        self.log('valid_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('valid_acc', self.accuracy(logits, y))\n",
    "        self.cpu_logits = logits.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"valid_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_logits,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        # Log test loss\n",
    "        self.log('test_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('test_acc', self.accuracy(logits, y))\n",
    "        self.cpu_logits = logits.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"test_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_logits,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        '''defines model optimizer'''\n",
    "        return Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20acbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YogaModel(LightningModule):\n",
    "\n",
    "    def __init__(self, n_classes=10, acc_task=\"multiclass\", lr=1e-3):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        The convolutions are arranged in such a way that the image maintain the x and y dimensions. only the channels change\n",
    "        \"\"\"\n",
    "        self.layer_1 = nn.Conv2d(in_channels = 1,out_channels = 3,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.layer_2 = nn.Conv2d(in_channels = 3,out_channels = 6,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.layer_3 = nn.Conv2d(in_channels = 6,out_channels = 12,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.layer_5 = nn.Linear(12*50*50,1000)#the input dimensions are (Number of dimensions * height * width)\n",
    "        self.layer_6 = nn.Linear(1000,100)\n",
    "        self.layer_7 = nn.Linear(100,50)\n",
    "        self.layer_8 = nn.Linear(50,10)\n",
    "        self.layer_9 = nn.Linear(10,10)\n",
    "        self.lr = lr\n",
    "        # metrics\n",
    "        self.acc_task = acc_task\n",
    "        self.n_classes = n_classes\n",
    "        self.accuracy = torchmetrics.Accuracy(task=self.acc_task, num_classes=self.n_classes)\n",
    "        self.class_names = classes_lst\n",
    "        # optional - save hyper-parameters to self.hparams\n",
    "        # they will also be automatically logged as config parameters in W&B\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x is the input data\n",
    "        \"\"\"\n",
    "        x = self.layer_1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        print(x.size())\n",
    "        x = self.layer_5(x)\n",
    "        x = self.layer_6(x)\n",
    "        x = self.layer_7(x)\n",
    "        x = self.layer_8(x)\n",
    "        x = self.layer_9(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr = self.lr)\n",
    "        return optimizer\n",
    "\n",
    "# The Pytorch-Lightning module handles all the iterations of the epoch\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred,y)\n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('train_acc', self.accuracy(y_pred, y))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred,y)\n",
    "        # Log training loss\n",
    "        self.log('val_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('val_acc', self.accuracy(y_pred, y))\n",
    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"val_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "        return loss\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred,y)\n",
    "        # Log training loss\n",
    "        self.log('test_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('test_acc', self.accuracy(y_pred, y))\n",
    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"test_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f7d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_acc', mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d5e1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    " \n",
    "class LogPredictionsCallback(Callback):\n",
    "    \n",
    "    def on_validation_batch_end(\n",
    "        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        \"\"\"Called when the validation batch ends.\"\"\"\n",
    " \n",
    "        # `outputs` comes from `LightningModule.validation_step`\n",
    "        # which corresponds to our model predictions in this case\n",
    "        \n",
    "        # Let's log 20 sample image predictions from first batch\n",
    "        if batch_idx == 0:\n",
    "            n = 20\n",
    "            x, y = batch\n",
    "            images = [img for img in x[:n]]\n",
    "            captions = [f'Ground Truth: {y_i} - Prediction: {y_pred}' for y_i, y_pred in zip(y[:n], outputs[:n])]\n",
    "            \n",
    "            # Option 1: log images with `WandbLogger.log_image`\n",
    "            wandb_logger.log_image(key='sample_images', images=images, caption=captions)\n",
    "\n",
    "            # Option 2: log predictions as a Table\n",
    "            columns = ['image', 'ground truth', 'prediction']\n",
    "            data = [[wandb.Image(x_i), y_i, y_pred] for x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]\n",
    "            wandb_logger.log_table(key='sample_table', columns=columns, data=data)\n",
    "\n",
    "log_predictions_callback = LogPredictionsCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a1750e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20230302_205638-htlhzkne</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/computer_vision_test_single/runs/htlhzkne' target=\"_blank\">misty-darkness-5</a></strong> to <a href='https://wandb.ai/christopher-marais/computer_vision_test_single' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/christopher-marais/computer_vision_test_single' target=\"_blank\">https://wandb.ai/christopher-marais/computer_vision_test_single</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/christopher-marais/computer_vision_test_single/runs/htlhzkne' target=\"_blank\">https://wandb.ai/christopher-marais/computer_vision_test_single/runs/htlhzkne</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8617453a48aa4c45818b2aea4ac5130f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb_logger = WandbLogger(project='computer_vision_test_single', log_model='all')\n",
    "\n",
    "# TRAIN\n",
    "# setup data\n",
    "# data = MNISTDataModule()\n",
    "data = YogaDataModule()\n",
    "\n",
    "# setup model - choose different hyperparameters per experiment\n",
    "model = YogaModel(n_classes=num_of_classes)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='gpu', \n",
    "    devices=-1, # use all GPU's (-1)\n",
    "    callbacks=[log_predictions_callback,checkpoint_callback],\n",
    "    logger=wandb_logger,    # W&B integration\n",
    "    max_epochs=3            # number of epochs\n",
    "    )\n",
    "\n",
    "trainer.fit(model, data)\n",
    "\n",
    "trainer.test(model, datamodule=data)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3816c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YogaModel(LightningModule):\n",
    "\n",
    "    def __init__(self, n_classes=10, acc_task=\"multiclass\", lr=1e-3):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        The convolutions are arranged in such a way that the image maintain the x and y dimensions. only the channels change\n",
    "        \"\"\"\n",
    "        self.layer_1 = nn.Conv2d(in_channels = 1,out_channels = 3,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.layer_2 = nn.Conv2d(in_channels = 3,out_channels = 6,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.layer_3 = nn.Conv2d(in_channels = 6,out_channels = 12,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.layer_5 = nn.Linear(12*50*50,1000)#the input dimensions are (Number of dimensions * height * width)\n",
    "        self.layer_6 = nn.Linear(1000,100)\n",
    "        self.layer_7 = nn.Linear(100,50)\n",
    "        self.layer_8 = nn.Linear(50,10)\n",
    "        self.layer_9 = nn.Linear(10,10)\n",
    "        self.lr = lr\n",
    "        # metrics\n",
    "        self.acc_task = acc_task\n",
    "        self.n_classes = n_classes\n",
    "        self.accuracy = torchmetrics.Accuracy(task=self.acc_task, num_classes=self.n_classes)\n",
    "        self.class_names = classes_lst\n",
    "        # optional - save hyper-parameters to self.hparams\n",
    "        # they will also be automatically logged as config parameters in W&B\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x is the input data\n",
    "        \"\"\"\n",
    "        x = self.layer_1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        print(x.size())\n",
    "        x = self.layer_5(x)\n",
    "        x = self.layer_6(x)\n",
    "        x = self.layer_7(x)\n",
    "        x = self.layer_8(x)\n",
    "        x = self.layer_9(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr = self.lr)\n",
    "        return optimizer\n",
    "\n",
    "# The Pytorch-Lightning module handles all the iterations of the epoch\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred,y)\n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('train_acc', self.accuracy(y_pred, y))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "#         preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
    "\n",
    "#         # Log loss and metric\n",
    "#         self.log('val_loss', loss)\n",
    "#         self.log('val_accuracy', acc)\n",
    "        \n",
    "        \n",
    "        x,y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred,y)\n",
    "        # Log training loss\n",
    "        self.log('val_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('val_acc', self.accuracy(y_pred, y))\n",
    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"val_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "        return y_pred\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred,y)\n",
    "        # Log training loss\n",
    "        self.log('test_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('test_acc', self.accuracy(y_pred, y))\n",
    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"test_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e3c94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_acc', mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1fd53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    " \n",
    "class LogPredictionsCallback(Callback):\n",
    "    \n",
    "    def on_validation_batch_end(\n",
    "        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        \"\"\"Called when the validation batch ends.\"\"\"\n",
    " \n",
    "        # `outputs` comes from `LightningModule.validation_step`\n",
    "        # which corresponds to our model predictions in this case\n",
    "        \n",
    "        # Let's log 20 sample image predictions from first batch\n",
    "        if batch_idx == 0:\n",
    "            n = 20\n",
    "            x, y = batch\n",
    "            images = [img for img in x[:n]]\n",
    "            captions = [f'Ground Truth: {y_i} - Prediction: {y_pred}' for y_i, y_pred in zip(y[:n], outputs[:n])]\n",
    "            \n",
    "            # Option 1: log images with `WandbLogger.log_image`\n",
    "            wandb_logger.log_image(key='sample_images', images=images, caption=captions)\n",
    "\n",
    "            # Option 2: log predictions as a Table\n",
    "            columns = ['image', 'ground truth', 'prediction']\n",
    "            data = [[wandb.Image(x_i), y_i, y_pred] for x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]\n",
    "            wandb_logger.log_table(key='sample_table', columns=columns, data=data)\n",
    "\n",
    "log_predictions_callback = LogPredictionsCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be5ab496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64368f777824260a398503156e2f6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b01d4b4cbb644289795b239e5891ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e30fbeb99a3450aaefd782d2362461a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffeceead112d4319a749341046d75ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abede5d12d649ddb18d1e8442f91586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2c5caded8944d8b3864609b82180cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb_logger = WandbLogger(project='computer_vision_test_single', log_model='all')\n",
    "\n",
    "# TRAIN\n",
    "# setup data\n",
    "# data = MNISTDataModule()\n",
    "data = YogaDataModule()\n",
    "\n",
    "# setup model - choose different hyperparameters per experiment\n",
    "model = YogaModel(n_classes=num_of_classes)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='gpu', \n",
    "    devices=-1, # use all GPU's (-1)\n",
    "    callbacks=[log_predictions_callback,checkpoint_callback],\n",
    "    logger=wandb_logger,    # W&B integration\n",
    "    max_epochs=3            # number of epochs\n",
    "    )\n",
    "\n",
    "trainer.fit(model, data)\n",
    "\n",
    "trainer.test(model, datamodule=data)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
