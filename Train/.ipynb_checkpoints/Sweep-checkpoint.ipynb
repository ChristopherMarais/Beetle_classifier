{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4c2bd-a555-44a6-a4c0-371eb17ba22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import wandb\n",
    "import fastai\n",
    "import dill\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.core import *\n",
    "from fastai.text.core import RegexLabeller\n",
    "from fastai.vision.utils import get_image_files\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.data.core import *\n",
    "from fastai.tabular.all import *\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Parameter_Optimization_Sweep.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c5062-b647-4a3e-aaba-07025d1e6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define configs and parameters\n",
    "# define static parameters\n",
    "meta_config = SimpleNamespace(\n",
    "    dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/selected_images/train_data\",\n",
    "    img_size=224,\n",
    "    seed=42,\n",
    "    project=\"Ambrosia_Symbiosis\",\n",
    "    # group=\"Beetle_classifier\",\n",
    "    # job_type=\"parameter_optimization\"\n",
    "    )\n",
    "\n",
    "# define parameter optimization config\n",
    "sweep_config = {\n",
    "    'name': 'Beetle_Classifier_Sweep',\n",
    "    'project':meta_config.project,\n",
    "    'method': 'bayes',\n",
    "    'run_cap':10,\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'validation_loss'\n",
    "        },\n",
    "    # 'early_terminate':{\n",
    "    #     'type': 'hyperband',\n",
    "    #     'min_iter': 1,\n",
    "    #     'max_iter': 100,\n",
    "    #     'eta': 3,\n",
    "    #     's': 2\n",
    "    # },\n",
    "    'parameters': {\n",
    "        'pretrained':{'values': [True, False]},\n",
    "        'model_name':{'values': [\"maxvit_rmlp_small_rw_224.sw_in1k\"]},\n",
    "        'batch_size': {'values': [128, 64, 256]},\n",
    "        'epochs': {'values': [2, 5, 3]},\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3aff0-b845-479e-a58e-8aee3565b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions required for sweep\n",
    "def get_images(dataset_path, batch_size, img_size, seed):\n",
    "    \"The beetles dataset\"\n",
    "    files = get_image_files(path=dataset_path, recurse=True, folders=('train','valid'))\n",
    "    dblock = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                       get_items = get_image_files,\n",
    "                       splitter = GrandparentSplitter(train_name='train', valid_name='valid'),\n",
    "                       get_y = parent_label,\n",
    "                       item_tfms = Resize(img_size, ResizeMethod.Pad, pad_mode='zeros'))\n",
    "    dls = dblock.dataloaders(dataset_path, bs = batch_size)\n",
    "    return dls\n",
    "\n",
    "# def train(meta_config):\n",
    "#     \"Train the model using the supplied configs\"\n",
    "#     run = wandb.init(project=meta_config.project) # , job_type=meta_config.job_type, group =meta_config.group, \n",
    "#     dls = get_images(dataset_path=meta_config.dataset_path, img_size=meta_config.img_size, seed=meta_config.seed, batch_size=wandb.config.batch_size)\n",
    "#     cbs = [MixedPrecision(), ShowGraphCallback(), SaveModelCallback(), WandbCallback(log='all')] \n",
    "#     learn = vision_learner(dls, \n",
    "#                            wandb.config.model_name,\n",
    "#                            loss_func=LabelSmoothingCrossEntropyFlat(),\n",
    "#                            metrics=[error_rate, \n",
    "#                                     accuracy, \n",
    "#                                     top_k_accuracy], \n",
    "#                            cbs=cbs, \n",
    "#                            pretrained=wandb.config.pretrained)\n",
    "#     learn.fine_tune(wandb.config.epochs)\n",
    "#     run.finish()\n",
    "\n",
    "def train(config, dataset_path, subfolders=('train','valid')):\n",
    "    \"Train the model using the supplied config\"\n",
    "    dls = get_images(dataset_path=dataset_path, batch_size=config.batch_size, img_size=config.img_size, seed=config.seed, subfolders=subfolders)\n",
    "    labels = np.array([str(x).split('/')[-2] for x in dls.items])\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weights = {c: w for c, w in zip(classes, weights)}\n",
    "    weights = tensor([class_weights[c] for c in dls.vocab]).to(dls.device)\n",
    "    wandb.init(project=config.wandb_project, group=config.wandb_group, job_type=config.job_type, config=config) # it is a good idea to keep these functions out of the training function due to some exporting issues\n",
    "    cbs = [MixedPrecision(), ShowGraphCallback(), SaveModelCallback(), WandbCallback(log='gradients')] # (all, parameters, gradients or None) parameters and all does nto work currently wandb needs to be updated\n",
    "    learn = vision_learner(dls, \n",
    "                           config.model_name, \n",
    "                           loss_func=LabelSmoothingCrossEntropy(weight=weights), # this fucntion is used for class imbalance it is a regularization technique # LabelSmoothingCrossEntropyFlat is used for multi dimensional data\n",
    "                           metrics=[error_rate, \n",
    "                                    accuracy, \n",
    "                                    top_k_accuracy], \n",
    "                           cbs=cbs, \n",
    "                           pretrained=config.pretrained)\n",
    "    learn.fine_tune(config.epochs)\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    interp.plot_confusion_matrix()\n",
    "    interp.plot_top_losses(config.top_k_losses, nrows=config.top_k_losses)\n",
    "    wandb.finish() # it is a good idea to keep these functions out of the training function due to some exporting issues\n",
    "    # return learn\n",
    "    \n",
    "# Prepare training wrapper based on configs\n",
    "def train_wrapper():\n",
    "    train(meta_config = meta_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12602698-224e-4370-a039-3f0ba589feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sweep    \n",
    "sweep_id = wandb.sweep(sweep_config)\n",
    "wandb.agent(sweep_id, function=train_wrapper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BC_310",
   "language": "python",
   "name": "bc_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
