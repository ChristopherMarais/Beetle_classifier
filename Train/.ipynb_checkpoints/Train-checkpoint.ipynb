{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaf7e05-ae68-4000-935e-c765b5e930a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T17:52:27.272269Z",
     "iopub.status.busy": "2023-03-06T17:52:27.271766Z",
     "iopub.status.idle": "2023-03-06T17:52:30.362663Z",
     "shell.execute_reply": "2023-03-06T17:52:30.361663Z",
     "shell.execute_reply.started": "2023-03-06T17:52:27.272269Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa7082e-4f18-4572-a1ca-81e89e6655f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T17:52:30.364663Z",
     "iopub.status.busy": "2023-03-06T17:52:30.364163Z",
     "iopub.status.idle": "2023-03-06T17:52:30.378167Z",
     "shell.execute_reply": "2023-03-06T17:52:30.377163Z",
     "shell.execute_reply.started": "2023-03-06T17:52:30.364663Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self, dataset, transform_list=None):\n",
    "        [data_X, data_y] = dataset\n",
    "        X_tensor, y_tensor = torch.Tensor(data_X), torch.Tensor(data_y)\n",
    "        tensors = (X_tensor, y_tensor)\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transforms = transform_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "        if self.transforms:\n",
    "          #for transform in self.transforms: \n",
    "          #  x = transform(x)\n",
    "            x = self.transforms(x)\n",
    "        y = self.tensors[1][index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e964502e-dc18-44ae-8709-706e2acc2bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T17:52:30.381669Z",
     "iopub.status.busy": "2023-03-06T17:52:30.381166Z",
     "iopub.status.idle": "2023-03-06T17:52:30.701637Z",
     "shell.execute_reply": "2023-03-06T17:52:30.701135Z",
     "shell.execute_reply.started": "2023-03-06T17:52:30.381669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to preprocess data and prepare for training\n",
    "def preprocess_data(data_path, labels_path, batch_size, split_test_size=0.3, split_random_state=42):\n",
    "    # obtain workers number\n",
    "    core_count = 8 #len(os.sched_getaffinity(0))\n",
    "    print('CPU cores: ', core_count)\n",
    "    \n",
    "    # Loading Data\n",
    "    data = np.load(data_path)\n",
    "    labels = np.load(labels_path)\n",
    "\n",
    "    # reshape data\n",
    "    reshaped_data = data.reshape((300,300,3,data.shape[1]))\n",
    "    # normalize parameters\n",
    "    stds = reshaped_data.std(axis=(0,1,3))\n",
    "    means = reshaped_data.mean(axis=(0,1,3))\n",
    "    # move axis on data\n",
    "    reshaped_data = np.moveaxis(reshaped_data, source=[0, 1, 2, 3], destination=[2, 3, 1, 0])\n",
    "    labels = np.array(labels,dtype=int)\n",
    "    \n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(degrees=(0, 360)),\n",
    "            transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "            transforms.GaussianBlur(kernel_size=(5, 9), \n",
    "                                    sigma=(0.1, 5)),\n",
    "#             transforms.ColorJitter(brightness=(0.75,1.25), \n",
    "#                                    contrast=(0,2), \n",
    "#                                    saturation=(0,2), \n",
    "#                                    hue=None),\n",
    "#             transforms.RandomPerspective(),\n",
    "#             transforms.RandomAffine(degrees=(0, 360), \n",
    "#                                     translate=(0.1, 0.3), \n",
    "#                                     scale=(0.5, 0.75), \n",
    "#                                     shear=(0, 0.2, 0, 0.2)),\n",
    "            transforms.Normalize(means, stds)\n",
    "            ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means, stds)\n",
    "            ]),\n",
    "    }\n",
    "    \n",
    "    # split data into training and validation sets\n",
    "    x_train, x_val, t_train, t_val = train_test_split(reshaped_data, labels, test_size=split_test_size, random_state=split_random_state)\n",
    "    train_set = [x_train,t_train]\n",
    "    val_set = [x_val,t_val]\n",
    "    \n",
    "    #dataset class and dataloaders\n",
    "    image_datasets = {'train': CustomTensorDataset(dataset=train_set, transform_list=data_transforms['train']), 'val': CustomTensorDataset(dataset=val_set, transform_list=data_transforms['train'])}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                                  batch_size=batch_size, \n",
    "                                                  shuffle=True, \n",
    "                                                  num_workers=core_count) for x in ['train', 'val']}\n",
    "    \n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    \n",
    "    return dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36b7b33-5cff-40e4-8901-d71b8de09849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T17:52:30.703635Z",
     "iopub.status.busy": "2023-03-06T17:52:30.703137Z",
     "iopub.status.idle": "2023-03-06T17:52:31.321197Z",
     "shell.execute_reply": "2023-03-06T17:52:31.320195Z",
     "shell.execute_reply.started": "2023-03-06T17:52:30.703635Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to describe training process of the neural network\n",
    "def model_function(model,  criterion, optimizer, scheduler, dataset_sizes, dataloaders, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    # use GPU when available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    train_dict = {'train':[[],[]], 'val':[[],[]]}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                labels = labels.type(torch.LongTensor) \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            train_dict[phase][0].append(epoch_loss)\n",
    "            train_dict[phase][1].append(epoch_acc.cpu().item())\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89cc8cd8-769d-4d79-a204-78bdf59c49cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T17:52:31.323199Z",
     "iopub.status.busy": "2023-03-06T17:52:31.322698Z",
     "iopub.status.idle": "2023-03-06T17:52:31.583395Z",
     "shell.execute_reply": "2023-03-06T17:52:31.582393Z",
     "shell.execute_reply.started": "2023-03-06T17:52:31.323199Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model and train data\n",
    "def train_model(dataloaders, epochs, dataset_sizes, gamma, step_size, learning_rate, class_num=10):\n",
    "    \n",
    "    # use GPU when available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # get device info\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # Print GPU name\n",
    "    print('GPU: ', torch.cuda.get_device_name(device=device))\n",
    "    \n",
    "    # load model\n",
    "    model_conv = torchvision.models.efficientnet_v2_l(weights='EfficientNet_V2_L_Weights.IMAGENET1K_V1')\n",
    "    for param in model_conv.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "    num_ftrs = model_conv.classifier[1].in_features\n",
    "    model_conv.classifier = nn.Linear(num_ftrs, class_num)\n",
    "    model_conv = model_conv.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Only parameters of final layer are being optimized\n",
    "    optimizer_conv = optim.Adam(model_conv.classifier.parameters(), lr=learning_rate)\n",
    "    # Decay LR by a factor of gamma every step_size epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=step_size, gamma=gamma)\n",
    "    # train model\n",
    "    best_model, train_dict_fixed = model_function(model_conv, \n",
    "                                               criterion,\n",
    "                                               optimizer_conv, \n",
    "                                               exp_lr_scheduler,\n",
    "                                               dataset_sizes=dataset_sizes, \n",
    "                                               num_epochs=epochs,\n",
    "                                               dataloaders=dataloaders)\n",
    "    return best_model, train_dict_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c98f2e-e371-49c0-a9fd-6e8e0a39e901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T17:52:31.585394Z",
     "iopub.status.busy": "2023-03-06T17:52:31.584896Z",
     "iopub.status.idle": "2023-03-06T17:52:31.845920Z",
     "shell.execute_reply": "2023-03-06T17:52:31.844919Z",
     "shell.execute_reply.started": "2023-03-06T17:52:31.585394Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full train function that saves training progress and best model to disk\n",
    "def Train(data_path, labels_path, batch_size, epoch_count, gamma, step_size, learning_rate, split_test_size=0.3, split_random_state=42, class_count=10):\n",
    "    \n",
    "    # current working directory\n",
    "    wd = os.getcwd()\n",
    "    print('Working directory: ', wd)\n",
    "\n",
    "    # start time\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "    print(current_time)\n",
    "    \n",
    "    dataloaders, dataset_sizes = preprocess_data(data_path=data_path, labels_path=labels_path, batch_size=batch_size, split_test_size=0.3, split_random_state=42)\n",
    "    \n",
    "    best_model, train_dict_fixed = train_model(dataloaders=dataloaders, \n",
    "                                     epochs=epoch_count,\n",
    "                                     dataset_sizes=dataset_sizes,\n",
    "                                     gamma=gamma, \n",
    "                                     step_size=step_size, \n",
    "                                     learning_rate=learning_rate, \n",
    "                                     class_num=class_count)\n",
    "                                               \n",
    "    # save weights state dictionary of best model to disk in working directory\n",
    "    torch.save(best_model.state_dict(), wd + '/training_best_model_state_dict.pt')\n",
    "    \n",
    "    # save training epoch data to disk\n",
    "    df = pd.DataFrame()\n",
    "    df['train_loss'] = train_dict_fixed['train'][0]\n",
    "    df['val_loss'] = train_dict_fixed['val'][0]\n",
    "    df['train_acc'] = train_dict_fixed['train'][1]\n",
    "    df['val_acc'] = train_dict_fixed['val'][1]\n",
    "    df.to_csv(wd+'/training_accuracy_and_loss.csv')\n",
    "    \n",
    "    print('Saved model to Disk','\\n','Finished Training')\n",
    "    \n",
    "    return best_model, train_dict_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222de1a0-1c8e-4c55-a917-a48f574ab387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T17:52:31.847922Z",
     "iopub.status.busy": "2023-03-06T17:52:31.846924Z",
     "iopub.status.idle": "2023-03-06T17:52:33.239066Z",
     "shell.execute_reply": "2023-03-06T17:52:33.237566Z",
     "shell.execute_reply.started": "2023-03-06T17:52:31.847922Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory:  E:\\GIT_REPOS\\final-project-rpjc_ml\n",
      "12:52:32\n",
      "CPU cores:  8\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nodup_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_model, train_dict_fixed \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnodup_data.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlabels_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnodup_labels.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43msplit_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43msplit_random_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepoch_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m     \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mTrain\u001b[1;34m(data_path, labels_path, batch_size, epoch_count, gamma, step_size, learning_rate, split_test_size, split_random_state, class_count)\u001b[0m\n\u001b[0;32m     10\u001b[0m current_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m, t)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(current_time)\n\u001b[1;32m---> 13\u001b[0m dataloaders, dataset_sizes \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_random_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m best_model, train_dict_fixed \u001b[38;5;241m=\u001b[39m train_model(dataloaders\u001b[38;5;241m=\u001b[39mdataloaders, \n\u001b[0;32m     16\u001b[0m                                  epochs\u001b[38;5;241m=\u001b[39mepoch_count,\n\u001b[0;32m     17\u001b[0m                                  dataset_sizes\u001b[38;5;241m=\u001b[39mdataset_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m                                  learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, \n\u001b[0;32m     21\u001b[0m                                  class_num\u001b[38;5;241m=\u001b[39mclass_count)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# save weights state dictionary of best model to disk in working directory\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(data_path, labels_path, batch_size, split_test_size, split_random_state)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU cores: \u001b[39m\u001b[38;5;124m'\u001b[39m, core_count)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Loading Data\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(labels_path)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# reshape data\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nodup_data.npy'"
     ]
    }
   ],
   "source": [
    "best_model, train_dict_fixed = Train(data_path='nodup_data.npy', \n",
    "      labels_path='nodup_labels.npy',\n",
    "      batch_size=64,\n",
    "      split_test_size=0.3, \n",
    "      split_random_state=42, \n",
    "      epoch_count=1, \n",
    "      gamma=0.1, \n",
    "      step_size=5, \n",
    "      learning_rate=0.01\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff43614e-b024-40e2-9b51-0bb3ab429cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
