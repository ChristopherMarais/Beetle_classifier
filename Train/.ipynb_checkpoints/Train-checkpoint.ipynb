{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c403f8d-3cfd-48f5-8a4a-500a03fff3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import wandb\n",
    "import fastai\n",
    "import dill\n",
    "import re\n",
    "import random\n",
    "import PIL\n",
    "import numpy as np\n",
    "from fastai.vision.augment import cutout_gaussian\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.core import *\n",
    "from fastai.text.core import RegexLabeller\n",
    "from fastai.vision.utils import get_image_files\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.data.core import *\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import notebook_login, push_to_hub_fastai, from_pretrained_fastai\n",
    "from torchvision.transforms import GaussianBlur\n",
    "# os.environ['WANDB_WATCH'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2baae5-bf1c-4f7a-bae0-98bb038d313f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    batch_size=8,  #16, #256,\n",
    "    epochs=15,\n",
    "    lr=2e-3,\n",
    "    img_size=256, # 224\n",
    "    seed=42,\n",
    "    pretrained=True,\n",
    "    top_k_losses=5,\n",
    "    model_name=\"maxvit_nano_rw_256.sw_in1k\",# \"maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k\",# try with maxvit_nano_rw_256.sw_in1k # regnetx_040 coatnet_bn_0_rw_224.sw_in1k\n",
    "    wandb_project=\"Beetle_classifier\", \n",
    "    wandb_group=\"ambrosia_symbiosis\",\n",
    "    job_type=\"training\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e633d2-718e-4940-9e6e-94ea772337e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a custom transform for Gaussian blur\n",
    "def gaussian_blur(x, p=0.5, kernel_size_min=3, kernel_size_max=20, sigma_min=0.1, sigma_max=3):\n",
    "    if x.ndim == 4:\n",
    "        for i in range(x.shape[0]):\n",
    "            if random.random() < p:\n",
    "                kernel_size = random.randrange(kernel_size_min, kernel_size_max + 1, 2)\n",
    "                sigma = random.uniform(sigma_min, sigma_max)\n",
    "                x[i] = GaussianBlur(kernel_size=kernel_size, sigma=sigma)(x[i])\n",
    "    return x\n",
    "\n",
    "def get_images(dataset_path, batch_size, img_size, seed, subfolders=('train','valid')):\n",
    "    \"The beetles dataset\"\n",
    "    files = get_image_files(path=dataset_path, recurse=True, folders=subfolders)\n",
    "    transforms = aug_transforms(    # transformatiosn that are only applied ot training and not inference\n",
    "                           batch=False,\n",
    "                           pad_mode='zeros',\n",
    "                           size=img_size,\n",
    "                           p_affine=0.8,\n",
    "                           p_lighting=0.8,\n",
    "                           max_rotate=360.0,\n",
    "                           mult=1.0, \n",
    "                           do_flip=True, \n",
    "                           flip_vert=False,\n",
    "                           min_zoom=1.0,\n",
    "                           max_zoom=1.1, \n",
    "                           max_lighting=0.75,\n",
    "                           max_warp=0.2, \n",
    "                           mode='bilinear', \n",
    "                           align_corners=True,\n",
    "                           min_scale=1.0,\n",
    "                           xtra_tfms=[RandomErasing(p=0.8, max_count=5, sh=0.25)]) # this adds random erasing to entire batches\n",
    "    transforms.append(partial(gaussian_blur, p=0.8))\n",
    "    dblock = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                       get_items = get_image_files,\n",
    "                       splitter = GrandparentSplitter(train_name=subfolders[0], valid_name=subfolders[1]),\n",
    "                       get_y = parent_label,\n",
    "                       item_tfms = Resize(img_size, ResizeMethod.Pad, pad_mode='zeros'), # resize trasnformation is applied during inference too                                    \n",
    "                       batch_tfms = transforms)\n",
    "    dls = dblock.dataloaders(dataset_path, bs = batch_size)\n",
    "    return dls\n",
    "\n",
    "def train(config, dataset_path, subfolders=('train','valid')):\n",
    "    \"Train the model using the supplied config\"\n",
    "    dls = get_images(dataset_path=dataset_path, batch_size=config.batch_size, img_size=config.img_size, seed=config.seed, subfolders=subfolders)\n",
    "    labels = np.array([re.split(r'/|\\\\', str(x))[-2] for x in dls.items])\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weights = {c: w for c, w in zip(classes, weights)}\n",
    "    weights = tensor([class_weights[c] for c in dls.vocab]).to(dls.device)\n",
    "    # wandb.init(project=config.wandb_project, group=config.wandb_group, job_type=config.job_type, config=config) # it is a good idea to keep these functions out of the training function due to some exporting issues\n",
    "    cbs = [MixedPrecision(), ShowGraphCallback(), SaveModelCallback(), WandbCallback(log='gradients')] # (all, parameters, gradients or None) parameters and all does nto work currently wandb needs to be updated\n",
    "    learn = vision_learner(dls, \n",
    "                           config.model_name, \n",
    "                           loss_func=LabelSmoothingCrossEntropy(weight=weights), # this fucntion is used for class imbalance it is a regularization technique # LabelSmoothingCrossEntropyFlat is used for multi dimensional data\n",
    "                           metrics=[error_rate, \n",
    "                                    accuracy, \n",
    "                                    top_k_accuracy], \n",
    "                           cbs=cbs, \n",
    "                           pretrained=config.pretrained)\n",
    "    learn.fine_tune(config.epochs, base_lr=config.lr)\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    interp.plot_confusion_matrix()\n",
    "    interp.plot_top_losses(config.top_k_losses, nrows=config.top_k_losses)\n",
    "    # wandb.finish() # it is a good idea to keep these functions out of the training function due to some exporting issues\n",
    "    return learn\n",
    "\n",
    "# this function only describes how much a singular value in al ist stands out.\n",
    "# if all values in the lsit are high or low this is 1\n",
    "# the smaller the proportiopn of number of disimilar vlaues are to other more similar values the lower this number\n",
    "# the larger the gap between the dissimilar numbers and the simialr number the smaller this number\n",
    "# only able to interpret probabilities or values between 0 and 1\n",
    "# this function outputs an estimate an inverse of the classification confidence based on the probabilities of all the classes.\n",
    "# the wedge threshold splits the data on a threshold with a magnitude of a positive int to force a ledge/peak in the data\n",
    "def unkown_prob_calc(probs, wedge_threshold, wedge_magnitude=1, wedge='strict'):\n",
    "    if wedge =='strict':\n",
    "        increase_var = (1/(wedge_magnitude))\n",
    "        decrease_var = (wedge_magnitude)\n",
    "    if wedge =='dynamic': # this allows pointsthat are furhter from the threshold ot be moved less and points clsoer to be moved more\n",
    "        increase_var = (1/(wedge_magnitude*((1-np.abs(probs-wedge_threshold)))))\n",
    "        decrease_var = (wedge_magnitude*((1-np.abs(probs-wedge_threshold))))\n",
    "    # else:\n",
    "    #     print(\"Error: use 'strict' (default) or 'dynamic' as options for the wedge parameter!\")\n",
    "    probs = np.where(probs>=wedge_threshold , probs**increase_var, probs)\n",
    "    probs = np.where(probs<=wedge_threshold , probs**decrease_var, probs)\n",
    "    diff_matrix = np.abs(probs[:, np.newaxis] - probs)\n",
    "    diff_matrix_sum = np.sum(diff_matrix)\n",
    "    probs_sum = np.sum(probs)\n",
    "    class_val = (diff_matrix_sum/probs_sum)\n",
    "    max_class_val = ((len(probs)-1)*2)\n",
    "    kown_prob = class_val/max_class_val\n",
    "    unknown_prob = 1-kown_prob\n",
    "    return(unknown_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f127d-ce29-44cb-8f02-19e5454ef5cb",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d677984-84cd-4344-8929-6deb33e8aac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/blue/hulcr/gmarais/Beetle_classifier/Train/wandb/run-20230419_132016-r6k1rbn1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/Beetle_classifier/runs/r6k1rbn1' target=\"_blank\">curious-grass-41</a></strong> to <a href='https://wandb.ai/christopher-marais/Beetle_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/christopher-marais/Beetle_classifier' target=\"_blank\">https://wandb.ai/christopher-marais/Beetle_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/christopher-marais/Beetle_classifier/runs/r6k1rbn1' target=\"_blank\">https://wandb.ai/christopher-marais/Beetle_classifier/runs/r6k1rbn1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBeetle_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mselected_images\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mwandb_project, group\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mwandb_group, job_type\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mjob_type, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m----> 5\u001b[0m learn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config, dataset_path, subfolders)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(config, dataset_path, subfolders\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain the model using the supplied config\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m     dls \u001b[38;5;241m=\u001b[39m \u001b[43mget_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/|\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(x))[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m dls\u001b[38;5;241m.\u001b[39mitems])\n\u001b[1;32m     46\u001b[0m     classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(labels)\n",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m, in \u001b[0;36mget_images\u001b[0;34m(dataset_path, batch_size, img_size, seed, subfolders)\u001b[0m\n\u001b[1;32m     32\u001b[0m transforms\u001b[38;5;241m.\u001b[39mappend(partial(gaussian_blur, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m))\n\u001b[1;32m     33\u001b[0m dblock \u001b[38;5;241m=\u001b[39m DataBlock(blocks \u001b[38;5;241m=\u001b[39m (ImageBlock, CategoryBlock),\n\u001b[1;32m     34\u001b[0m                    get_items \u001b[38;5;241m=\u001b[39m get_image_files,\n\u001b[1;32m     35\u001b[0m                    splitter \u001b[38;5;241m=\u001b[39m GrandparentSplitter(train_name\u001b[38;5;241m=\u001b[39msubfolders[\u001b[38;5;241m0\u001b[39m], valid_name\u001b[38;5;241m=\u001b[39msubfolders[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m     36\u001b[0m                    get_y \u001b[38;5;241m=\u001b[39m parent_label,\n\u001b[1;32m     37\u001b[0m                    item_tfms \u001b[38;5;241m=\u001b[39m Resize(img_size, ResizeMethod\u001b[38;5;241m.\u001b[39mPad, pad_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;66;03m# resize trasnformation is applied during inference too                                    \u001b[39;00m\n\u001b[1;32m     38\u001b[0m                    batch_tfms \u001b[38;5;241m=\u001b[39m transforms)\n\u001b[0;32m---> 39\u001b[0m dls \u001b[38;5;241m=\u001b[39m \u001b[43mdblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dls\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/BC_310/lib/python3.10/site-packages/fastai/data/block.py:155\u001b[0m, in \u001b[0;36mDataBlock.dataloaders\u001b[0;34m(self, source, path, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataloaders\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    150\u001b[0m     source, \u001b[38;5;66;03m# The data source\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     path:\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# Data source and default `Learner` path \u001b[39;00m\n\u001b[1;32m    152\u001b[0m     verbose:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# Show verbose messages\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataLoaders:\n\u001b[0;32m--> 155\u001b[0m     dsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m: verbose}\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dsets\u001b[38;5;241m.\u001b[39mdataloaders(path\u001b[38;5;241m=\u001b[39mpath, after_item\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_tfms, after_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_tfms, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/BC_310/lib/python3.10/site-packages/fastai/data/block.py:147\u001b[0m, in \u001b[0;36mDataBlock.datasets\u001b[0;34m(self, source, verbose)\u001b[0m\n\u001b[1;32m    145\u001b[0m splits \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter \u001b[38;5;129;01mor\u001b[39;00m RandomSplitter())(items)\n\u001b[1;32m    146\u001b[0m pv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(splits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m datasets of sizes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s))\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39ms\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39msplits])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDatasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_type_tfms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdl_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_inp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/BC_310/lib/python3.10/site-packages/fastai/data/core.py:454\u001b[0m, in \u001b[0;36mDatasets.__init__\u001b[0;34m(self, items, tfms, tls, n_inp, dl_type, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    446\u001b[0m     items:\u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of items to create `Datasets`\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     tfms:MutableSequence\u001b[38;5;241m|\u001b[39mPipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of `Transform`(s) or `Pipeline` to apply\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    452\u001b[0m ):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dl_type\u001b[38;5;241m=\u001b[39mdl_type)\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls \u001b[38;5;241m=\u001b[39m L(tls \u001b[38;5;28;01mif\u001b[39;00m tls \u001b[38;5;28;01melse\u001b[39;00m [TfmdLists(items, t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m L(ifnone(tfms,[\u001b[38;5;28;01mNone\u001b[39;00m]))])\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inp \u001b[38;5;241m=\u001b[39m ifnone(n_inp, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/BC_310/lib/python3.10/site-packages/fastai/data/core.py:454\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    446\u001b[0m     items:\u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of items to create `Datasets`\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     tfms:MutableSequence\u001b[38;5;241m|\u001b[39mPipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of `Transform`(s) or `Pipeline` to apply\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    452\u001b[0m ):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dl_type\u001b[38;5;241m=\u001b[39mdl_type)\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls \u001b[38;5;241m=\u001b[39m L(tls \u001b[38;5;28;01mif\u001b[39;00m tls \u001b[38;5;28;01melse\u001b[39;00m [\u001b[43mTfmdLists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m L(ifnone(tfms,[\u001b[38;5;28;01mNone\u001b[39;00m]))])\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inp \u001b[38;5;241m=\u001b[39m ifnone(n_inp, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/BC_310/lib/python3.10/site-packages/fastcore/foundation.py:98\u001b[0m, in \u001b[0;36m_L_Meta.__call__\u001b[0;34m(cls, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x,\u001b[38;5;28mcls\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/BC_310/lib/python3.10/site-packages/fastai/data/core.py:368\u001b[0m, in \u001b[0;36mTfmdLists.__init__\u001b[0;34m(self, items, tfms, use_list, do_setup, split_idx, train_setup, splits, types, verbose, dl_type)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_setup:\n\u001b[1;32m    367\u001b[0m     pv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting up \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_setup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_setup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/BC_310/lib/python3.10/site-packages/fastai/data/core.py:397\u001b[0m, in \u001b[0;36mTfmdLists.setup\u001b[0;34m(self, train_setup)\u001b[0m\n\u001b[1;32m    395\u001b[0m         x \u001b[38;5;241m=\u001b[39m f(x)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(x))\n\u001b[0;32m--> 397\u001b[0m types \u001b[38;5;241m=\u001b[39m L(t \u001b[38;5;28;01mif\u001b[39;00m is_listy(t) \u001b[38;5;28;01melse\u001b[39;00m [t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypes)\u001b[38;5;241m.\u001b[39mconcat()\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "# dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/selected_images/train_data\"\n",
    "dataset_path = r\"F:\\Beetle_data\\selected_images\\train_data\"\n",
    "wandb.init(project=config.wandb_project, group=config.wandb_group, job_type=config.job_type, config=config)\n",
    "learn = train(config=config, dataset_path=dataset_path)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba4082-af8d-432d-9c55-372a77332a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn.remove_cb(WandbCallback)\n",
    "lrs = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n",
    "plt.savefig('Learning_rate_find.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450bc27-0910-4b5e-b6df-95cc93085dea",
   "metadata": {},
   "source": [
    "Training images: 32.469 (~25000 training and ~7500 validation [80/20 split])\n",
    "Testing Images: 4610"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045802a9-049d-4be7-9c10-105e30a7c5ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save Model Locally and to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec139e7c-a844-49fe-b507-c77931492ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model to disk for inference\n",
    "# learn.path = Path(\"/blue/hulcr/gmarais/Beetle_classifier/Models\")\n",
    "learn.path = Path(\"F:\\Beetle_data\\Models\")\n",
    "learn.remove_cb(WandbCallback) # remove WandbCallbacks to allow prediction model to be applied without wandb\n",
    "# wandb.unwatch(learn.model)\n",
    "learn.export('beetle_classifier.pkl')#, pickle_module=dill) # use learn.save to save model and continue training later\n",
    "\n",
    "# load to huggingface hub\n",
    "repo_id=\"ChristopherMarais/beetle-model\"\n",
    "# repo_id=\"ChristopherMarais/Andrew_Alpha_model\"\n",
    "push_to_hub_fastai(learner=learn, repo_id=repo_id, token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea085049-1c89-45c8-9a9b-c61cdef22f6d",
   "metadata": {},
   "source": [
    "# ADD CROSS VALIDATION OF MODEL TO EVALUATE MODEL MORE ACCURATELY ON VALIDATION DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6928116-f083-4831-8120-5995715ef049",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd667977-042e-4100-a8bf-922e12c202d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from huggingface Hub\n",
    "repo_id=\"ChristopherMarais/beetle-model\"\n",
    "learn = from_pretrained_fastai(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e80506-d4f8-428f-94e4-563865cfdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import testing data\n",
    "# dataset_path=r\"/blue/hulcr/gmarais/Beetle_data/selected_images\"\n",
    "dataset_path=r\"F:\\Beetle_data\\selected_images\"\n",
    "files = get_image_files(path=dataset_path, recurse=True, folders=('test_data')) # get files from directory\n",
    "test_dl = learn.dls.test_dl(files, with_labels=True) # load data as a dataloader\n",
    "preds, targets = learn.get_preds(dl=test_dl)\n",
    "\n",
    "# get names of classes\n",
    "class_lst = list(learn.dls.vocab)\n",
    "class_lst.append(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f948b-a7ed-4d75-96d7-efb760771d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate ROC curve for classifier\n",
    "threshold_resolution = 0.001\n",
    "wedge_magnitude=2\n",
    "fpr_lst = []\n",
    "tpr_lst = []\n",
    "threshold_arr = np.arange(0,1+threshold_resolution,threshold_resolution)\n",
    "for i in threshold_arr:\n",
    "    # add unknown class probability\n",
    "    unknown_preds = np.apply_along_axis(unkown_prob_calc, axis=1, arr=np.array(preds), wedge_threshold=i, wedge_magnitude=wedge_magnitude) # calculate unknown class probability\n",
    "    full_preds = torch.from_numpy(np.concatenate((np.array(preds), unknown_preds[:, np.newaxis]), axis=1)) # add probability to estimates\n",
    "    preds_probs, preds_class = torch.max(full_preds, axis=1)\n",
    "    cnf_matrix = confusion_matrix(y_true=np.array(targets), y_pred=np.array(preds_class))\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    FPR = FP/(FP+TN)\n",
    "    fpr_lst.append(FPR)\n",
    "    TPR = TP/(TP+FN + 1e-6) # add 1e-6 to avoid the Nan value that happends when class not in targets\n",
    "    tpr_lst.append(TPR)\n",
    "\n",
    "# create dataframe with information in it\n",
    "# get column names for each dataframe\n",
    "FPR_class_lst = [x + \"_FPR\" for x in class_lst]\n",
    "TPR_class_lst = [x + \"_TPR\" for x in class_lst]\n",
    "df_fpr = pd.DataFrame(fpr_lst, columns=FPR_class_lst)\n",
    "df_tpr = pd.DataFrame(tpr_lst, columns=TPR_class_lst)\n",
    "df_ROC = pd.concat([df_tpr, df_fpr], axis=1) # get all data in one df\n",
    "df_ROC['threshold'] = threshold_arr # add threshold column\n",
    "ROC_lst = FPR_class_lst + TPR_class_lst\n",
    "df_ROC = df_ROC.drop_duplicates(subset=ROC_lst) # get rid of all unecessary duplicates\n",
    "df_ROC = df_ROC.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bea8da-6ab3-4981-b256-0388dadded23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100c53b-702b-4f53-8585-cfcb84b065eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize ROC curve\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Create an empty figure, and iteratively add new lines\n",
    "# every time we compute a new class\n",
    "fig = go.Figure()\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "\n",
    "for i in class_lst:\n",
    "    # get data from dataframe\n",
    "    fpr = df_ROC[i+\"_FPR\"].tolist()\n",
    "    fpr.insert(0, 0)\n",
    "    fpr.insert(-1, 1)\n",
    "    fpr.sort()\n",
    "    tpr = df_ROC[i+\"_TPR\"].tolist()\n",
    "    tpr.insert(0, 0)\n",
    "    tpr.insert(-1, 1)\n",
    "    tpr.sort()\n",
    "    \n",
    "    auc_score = auc(fpr, tpr)\n",
    "\n",
    "    name = f\"{i} (AUC={auc_score:.2f})\"\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=i, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    width=1000, height=1000\n",
    ")\n",
    "fig.show()\n",
    "# Save the figure as an HTML file\n",
    "pyo.plot(fig, filename='ROC_curve.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0583842-fd50-4680-83d3-e8b1ee06220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting a threshold is practically irrelevant but os far the bst one is at ~0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ee2f2-e70a-46f0-8c65-13504865dc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import model again to test\n",
    "# learn = load_learner(Path(\"/blue/hulcr/gmarais/Beetle_classifier/Models\") / 'beetle_classifier.pkl', cpu=False, pickle_module=dill)\n",
    "# print(learn.dls.vocab) # print all possible classes of model\n",
    "\n",
    "# import testing data\n",
    "# dataset_path=r\"/blue/hulcr/gmarais/Beetle_data/selected_images\"\n",
    "dataset_path=r\"F:\\Beetle_data\\selected_images\"\n",
    "files = get_image_files(path=dataset_path, recurse=True, folders=('test_data')) # get files from directory\n",
    "test_dl = learn.dls.test_dl(files, with_labels=True) # load data as a dataloader\n",
    "\n",
    "preds, targets = learn.get_preds(dl=test_dl)\n",
    "val_out = learn.validate(dl=test_dl)\n",
    "print(\" Loss: \"+str(val_out[0])+\"\\n\",\n",
    "      \"Error Rate: \"+str(val_out[1])+\"\\n\",\n",
    "      \"Accuracy: \"+str(val_out[2])+\"\\n\",\n",
    "      \"Top k(5) Accuracy: \"+str(val_out[3])+\"\\n\")\n",
    "\n",
    "# add unknown class probability\n",
    "unknown_preds = np.apply_along_axis(unkown_prob_calc, axis=1, arr=np.array(preds), wedge_threshold=0.5) # calculate unknown class probability\n",
    "full_preds = torch.from_numpy(np.concatenate((np.array(preds), unknown_preds[:, np.newaxis]), axis=1)) # add probability to estimates\n",
    "preds_probs, preds_class = torch.max(full_preds, axis=1)\n",
    "\n",
    "\n",
    "# plot confusion matrix\n",
    "arr_cm = confusion_matrix(y_true=np.array(targets), y_pred=np.array(preds_class))\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(arr_cm, cmap='Blues')\n",
    "ax.set_xticks(range(0,11))\n",
    "ax.set_xticklabels(class_lst)\n",
    "ax.set_yticks(range(0,11))\n",
    "ax.set_yticklabels(class_lst)\n",
    "# Get the colormap colors\n",
    "my_cmap = im.cmap(im.norm(arr_cm))\n",
    "for i in range(arr_cm.shape[0]):\n",
    "    for j in range(arr_cm.shape[1]):\n",
    "        # Get the RGB color of the cell\n",
    "        rgba = my_cmap[i, j]\n",
    "        # If the cell is dark, use white text; otherwise, use black text\n",
    "        text_color = 'w' if rgba[:3].mean() < 0.5 else 'k'\n",
    "        ax.text(j, i, arr_cm[i, j], ha='center', va='center', color=text_color)\n",
    "plt.title(\"Test Confusion Matrix\")        \n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BC_310",
   "language": "python",
   "name": "bc_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
