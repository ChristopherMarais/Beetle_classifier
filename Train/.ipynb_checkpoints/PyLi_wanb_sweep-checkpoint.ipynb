{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b471f1-f5fa-4eb1-b9ae-2d67ed8838df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T06:37:25.109874Z",
     "iopub.status.busy": "2023-02-11T06:37:25.109374Z",
     "iopub.status.idle": "2023-02-11T06:37:29.510754Z",
     "shell.execute_reply": "2023-02-11T06:37:29.509751Z",
     "shell.execute_reply.started": "2023-02-11T06:37:25.109874Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "from random import randint\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Pytorch modules\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "# Pytorch-Lightning\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchmetrics # a new pakage for torchmetrics\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "# sci-kit learn and scikit-image\n",
    "import sklearn\n",
    "import skimage\n",
    "\n",
    "# Dataset\n",
    "from torchvision.datasets import MNIST ######### not required\n",
    "from torchvision import transforms\n",
    "\n",
    "# number of CPUs\n",
    "# cpu_count = 0 if torch.cuda.is_available() else os.cpu_count()\n",
    "\n",
    "# use GPU tensor cores\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb5a33-290c-4449-8127-0a8a1ed4c6f0",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b386300d-6603-456e-b250-9fa334f9defa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T06:37:29.512255Z",
     "iopub.status.busy": "2023-02-11T06:37:29.511754Z",
     "iopub.status.idle": "2023-02-11T06:37:29.525758Z",
     "shell.execute_reply": "2023-02-11T06:37:29.525254Z",
     "shell.execute_reply.started": "2023-02-11T06:37:29.512255Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir='./', batch_size=256):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        '''called only once and on 1 GPU'''\n",
    "        # download data\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''called on each GPU separately - stage defines if we are at fit or test step'''\n",
    "        # we set up only relevant datasets when stage is specified (automatically set by Pytorch-Lightning)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_train = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        '''returns training dataloader'''\n",
    "        mnist_train = DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle = True)#, num_workers = cpu_count)\n",
    "        return mnist_train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        '''returns validation dataloader'''\n",
    "        mnist_val = DataLoader(self.mnist_val, batch_size=self.batch_size, shuffle = True)#, num_workers = cpu_count)\n",
    "        return mnist_val\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        '''returns test dataloader'''\n",
    "        mnist_test = DataLoader(self.mnist_test, batch_size=self.batch_size, shuffle = True)#, num_workers = cpu_count)\n",
    "        return mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c28e3c-850d-48b6-8db9-3a8804e0c9ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T06:37:29.527255Z",
     "iopub.status.busy": "2023-02-11T06:37:29.527255Z",
     "iopub.status.idle": "2023-02-11T06:37:29.819627Z",
     "shell.execute_reply": "2023-02-11T06:37:29.818625Z",
     "shell.execute_reply.started": "2023-02-11T06:37:29.527255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Classes : 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declaring the path of the train and test folders\n",
    "train_path = \"DATASET/TRAIN\"\n",
    "test_path = \"DATASET/TEST\"\n",
    "classes_dir_data = os.listdir(train_path)\n",
    "num_of_classes = len(classes_dir_data)\n",
    "print(\"Total Number of Classes :\" , num_of_classes)\n",
    "num = 0\n",
    "classes_dict = {}\n",
    "classes_lst = []\n",
    "num_dict = {}\n",
    "for c in  classes_dir_data:\n",
    "    classes_dict[c] = num\n",
    "    num_dict[num] = c\n",
    "    classes_lst.append(c)\n",
    "    num = num +1\n",
    "\"\"\"\n",
    "num_dict contains a dictionary of the classes numerically and it's corresponding classes.\n",
    "classes_dict contains a dictionary of the classes and the coresponding values numerically.\n",
    "\"\"\"\n",
    "num_of_classes = len(classes_dir_data)\n",
    "\n",
    "classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9e722b-cda4-4956-a0c1-8eb31023f765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T06:37:29.822125Z",
     "iopub.status.busy": "2023-02-11T06:37:29.822125Z",
     "iopub.status.idle": "2023-02-11T06:37:30.374831Z",
     "shell.execute_reply": "2023-02-11T06:37:30.373830Z",
     "shell.execute_reply.started": "2023-02-11T06:37:29.822125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating the dataset\n",
    "\n",
    "#dataset\n",
    "\n",
    "class Image_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self,classes,image_base_dir,transform = None, target_transform = None):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        classes:The classes in the dataset\n",
    "\n",
    "        image_base_dir:The directory of the folders containing the images\n",
    "\n",
    "        transform:The trasformations for the Images\n",
    "\n",
    "        Target_transform:The trasformations for the target\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.img_labels = classes\n",
    "\n",
    "        self.imge_base_dir = image_base_dir\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        img_dir_list = os.listdir(os.path.join(self.imge_base_dir,self.img_labels[idx]))\n",
    "\n",
    "        image_path = img_dir_list[randint(0,len(img_dir_list)-1)]\n",
    "\n",
    "        #print(image_path)\n",
    "\n",
    "        image_path = os.path.join(self.imge_base_dir,self.img_labels[idx],image_path)\n",
    "\n",
    "        image = skimage.io.imread(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            label = self.target_transform(self.img_labels[idx])\n",
    "\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbe4b19-9805-40b4-a513-ea35715a0c64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T06:37:30.375830Z",
     "iopub.status.busy": "2023-02-11T06:37:30.375830Z",
     "iopub.status.idle": "2023-02-11T06:37:30.652675Z",
     "shell.execute_reply": "2023-02-11T06:37:30.652174Z",
     "shell.execute_reply.started": "2023-02-11T06:37:30.375830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = 50 # need to be the same as what is used in layer_5/ input layer ot the cnn\n",
    "\n",
    "basic_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((size,size)),\n",
    "        transforms.Grayscale(1),\n",
    "    transforms.ToTensor()])\n",
    "training_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((size,size)),\n",
    "    transforms.RandomRotation(degrees = 45),\n",
    "    transforms.RandomHorizontalFlip(p = 0.005),\n",
    "        transforms.Grayscale(1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def target_transformations(x):\n",
    "    return torch.tensor(classes_dict.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7539d3a5-ca4f-4ba3-a462-05875d4b9a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T06:37:30.654177Z",
     "iopub.status.busy": "2023-02-11T06:37:30.653680Z",
     "iopub.status.idle": "2023-02-11T06:37:31.223520Z",
     "shell.execute_reply": "2023-02-11T06:37:31.223019Z",
     "shell.execute_reply.started": "2023-02-11T06:37:30.654177Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YogaDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self):\n",
    "            super().__init__()            \n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train = Image_Dataset(classes_dir_data,train_path,training_transformations,target_transformations)\n",
    "        self.valid = Image_Dataset(classes_dir_data,test_path,basic_transformations,target_transformations)\n",
    "        self.test = Image_Dataset(classes_dir_data,test_path,basic_transformations,target_transformations)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train,batch_size = 64,shuffle = True)#False, num_workers = cpu_count)\n",
    "\n",
    "    def val_dataloader(self):  \n",
    "        return DataLoader(self.valid,batch_size = 64,shuffle = True)#False, num_workers = cpu_count)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test,batch_size = 64,shuffle = True)#False, num_workers = cpu_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e41f1-2dd9-481f-ae64-3bf0f7e1bbee",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40361447-e7c3-47f2-9e04-a9058b596861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T06:37:31.225521Z",
     "iopub.status.busy": "2023-02-11T06:37:31.225021Z",
     "iopub.status.idle": "2023-02-11T06:37:31.501716Z",
     "shell.execute_reply": "2023-02-11T06:37:31.500718Z",
     "shell.execute_reply.started": "2023-02-11T06:37:31.225021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "\n",
    "    def __init__(self, n_classes=10, acc_task=\"multiclass\", n_layer_1=128, n_layer_2=256, lr=1e-3):\n",
    "        '''method used to define our model parameters'''\n",
    "        super().__init__()\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, n_layer_1)\n",
    "        self.layer_2 = torch.nn.Linear(n_layer_1, n_layer_2)\n",
    "        self.layer_3 = torch.nn.Linear(n_layer_2, n_classes)\n",
    "        # optimizer parameters\n",
    "        self.lr = lr\n",
    "        # metrics\n",
    "        self.acc_task = acc_task\n",
    "        self.n_classes = n_classes\n",
    "        self.accuracy = torchmetrics.Accuracy(task=self.acc_task, num_classes=self.n_classes)\n",
    "        self.class_names = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "        # optional - save hyper-parameters to self.hparams\n",
    "        # they will also be automatically logged as config parameters in W&B\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''method used for inference input -> output'''\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        # (b, 1, 28, 28) -> (b, 1*28*28)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.layer_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''needs to return a loss from a single batch'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('train_acc', self.accuracy(logits, y))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        # Log validation loss (will be automatically averaged over an epoch)\n",
    "        self.log('valid_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('valid_acc', self.accuracy(logits, y))\n",
    "        self.cpu_logits = logits.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"valid_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_logits,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        '''used for logging metrics'''\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        # Log test loss\n",
    "        self.log('test_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('test_acc', self.accuracy(logits, y))\n",
    "        self.cpu_logits = logits.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"test_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_logits,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        '''defines model optimizer'''\n",
    "        return Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb34aed-792a-4022-a878-f8d6c4ffa97c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T06:37:31.503216Z",
     "iopub.status.busy": "2023-02-11T06:37:31.502716Z",
     "iopub.status.idle": "2023-02-11T06:37:31.794324Z",
     "shell.execute_reply": "2023-02-11T06:37:31.793824Z",
     "shell.execute_reply.started": "2023-02-11T06:37:31.503216Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YogaModel(LightningModule):\n",
    "\n",
    "    def __init__(self, n_classes=10, acc_task=\"multiclass\", lr=1e-3):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        The convolutions are arranged in such a way that the image maintain the x and y dimensions. only the channels change\n",
    "        \"\"\"\n",
    "        self.layer_1 = nn.Conv2d(in_channels = 1,out_channels = 3,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.layer_2 = nn.Conv2d(in_channels = 3,out_channels = 6,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.layer_3 = nn.Conv2d(in_channels = 6,out_channels = 12,kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size = (3,3),padding = (1,1),stride = (1,1))\n",
    "        self.layer_5 = nn.Linear(12*50*50,1000)#the input dimensions are (Number of dimensions * height * width)\n",
    "        self.layer_6 = nn.Linear(1000,100)\n",
    "        self.layer_7 = nn.Linear(100,50)\n",
    "        self.layer_8 = nn.Linear(50,10)\n",
    "        self.layer_9 = nn.Linear(10,5)\n",
    "        self.lr = lr\n",
    "        # metrics\n",
    "        self.acc_task = acc_task\n",
    "        self.n_classes = n_classes\n",
    "        self.accuracy = torchmetrics.Accuracy(task=self.acc_task, num_classes=self.n_classes)\n",
    "        self.class_names = classes_lst\n",
    "        # optional - save hyper-parameters to self.hparams\n",
    "        # they will also be automatically logged as config parameters in W&B\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x is the input data\n",
    "        \"\"\"\n",
    "        x = self.layer_1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        print(x.size())\n",
    "        x = self.layer_5(x)\n",
    "        x = self.layer_6(x)\n",
    "        x = self.layer_7(x)\n",
    "        x = self.layer_8(x)\n",
    "        x = self.layer_9(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr = self.lr)\n",
    "        return optimizer\n",
    "\n",
    "# The Pytorch-Lightning module handles all the iterations of the epoch\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred,y)\n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('train_acc', self.accuracy(y_pred, y))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred,y)\n",
    "        # Log training loss\n",
    "        self.log('val_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('val_acc', self.accuracy(y_pred, y))\n",
    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"val_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "        return loss\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred,y)\n",
    "        # Log training loss\n",
    "        self.log('test_loss', loss)\n",
    "        # Log metrics\n",
    "        self.log('test_acc', self.accuracy(y_pred, y))\n",
    "        self.cpu_pred = y_pred.to(\"cpu\").detach().numpy()\n",
    "        self.cpu_y = y.to(\"cpu\").detach().numpy()\n",
    "        wandb.log({\"test_conf_mat\" : wandb.plot.confusion_matrix(probs=self.cpu_pred,\n",
    "                        y_true=self.cpu_y, preds=None,\n",
    "                        class_names=self.class_names)})\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561a2e3-1272-4e67-9322-dd9b7534fed3",
   "metadata": {},
   "source": [
    "# Single training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762279ca-26d9-4310-992f-b6f8912da7fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T06:37:31.795823Z",
     "iopub.status.busy": "2023-02-11T06:37:31.795325Z",
     "iopub.status.idle": "2023-02-11T06:37:39.030479Z",
     "shell.execute_reply": "2023-02-11T06:37:39.022978Z",
     "shell.execute_reply.started": "2023-02-11T06:37:31.795823Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristopher-marais\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20230211_013735-6rx24818</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/christopher-marais/MNIST/runs/6rx24818' target=\"_blank\">charmed-dust-167</a></strong> to <a href='https://wandb.ai/christopher-marais/MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/christopher-marais/MNIST' target=\"_blank\">https://wandb.ai/christopher-marais/MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/christopher-marais/MNIST/runs/6rx24818' target=\"_blank\">https://wandb.ai/christopher-marais/MNIST/runs/6rx24818</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\GCM\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | layer_1  | Conv2d             | 30    \n",
      "1 | layer_2  | Conv2d             | 168   \n",
      "2 | layer_3  | Conv2d             | 660   \n",
      "3 | pool     | MaxPool2d          | 0     \n",
      "4 | layer_5  | Linear             | 30.0 M\n",
      "5 | layer_6  | Linear             | 100 K \n",
      "6 | layer_7  | Linear             | 5.0 K \n",
      "7 | layer_8  | Linear             | 510   \n",
      "8 | layer_9  | Linear             | 55    \n",
      "9 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "30.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.1 M    Total params\n",
      "120.430   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23cfcddd4154820b07abc3479a609b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'cpu_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m YogaModel(n_classes\u001b[38;5;241m=\u001b[39mnum_of_classes)\n\u001b[0;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     14\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# devices=-1, # use all GPU's (-1)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     logger\u001b[38;5;241m=\u001b[39mwandb_logger,    \u001b[38;5;66;03m# W&B integration\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m            \u001b[38;5;66;03m# number of epochs\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     )\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, datamodule\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m     24\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m )\n\u001b[1;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1103\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1103\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1182\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1182\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1195\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1260\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;66;03m# reload dataloaders\u001b[39;00m\n\u001b[1;32m-> 1260\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reload_evaluation_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_sanity_val_batches \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_sanity_val_steps, val_batches) \u001b[38;5;28;01mfor\u001b[39;00m val_batches \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_val_batches\n\u001b[0;32m   1263\u001b[0m ]\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationLoop._reload_evaluation_dataloaders\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtest_dataloaders\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mval_dataloaders \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39m_should_reload_val_dl:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_val_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mval_dataloaders\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataloaders \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1640\u001b[0m, in \u001b[0;36mTrainer.reset_val_dataloader\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   1635\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanity_checking \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_should_check_val_epoch())\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanity_checking\n\u001b[0;32m   1637\u001b[0m ):\n\u001b[0;32m   1638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_val_dl_reload_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch\n\u001b[1;32m-> 1640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_val_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRunningStage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVALIDATING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpl_module\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:357\u001b[0m, in \u001b[0;36mDataConnector._reset_eval_dataloader\u001b[1;34m(self, mode, model)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mode\u001b[38;5;241m.\u001b[39mevaluating \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m RunningStage\u001b[38;5;241m.\u001b[39mPREDICTING\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# always get the loaders first so we can count how many there are\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moverfit_batches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    360\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_overfit_batches(dataloaders, mode)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:446\u001b[0m, in \u001b[0;36mDataConnector._request_dataloader\u001b[1;34m(self, stage)\u001b[0m\n\u001b[0;32m    439\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;241m.\u001b[39mdataloader_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dataloader_source\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;66;03m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;66;03m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataloader, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    448\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dataloader)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BC_310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:524\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance, pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[0;32m    523\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mYogaDataModule.val_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mval_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid,batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m,shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_workers \u001b[38;5;241m=\u001b[39m \u001b[43mcpu_count\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cpu_count' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb_logger = WandbLogger(project='MNIST')\n",
    "\n",
    "# TRAIN\n",
    "# setup data\n",
    "# data = MNISTDataModule()\n",
    "data = YogaDataModule()\n",
    "\n",
    "# setup model - choose different hyperparameters per experiment\n",
    "# model = LitMNIST(n_layer_1=128, n_layer_2=256, lr=1e-3, n_classes=10)\n",
    "model = YogaModel(n_classes=num_of_classes)\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cpu', \n",
    "    # devices=-1, # use all GPU's (-1)\n",
    "    logger=wandb_logger,    # W&B integration\n",
    "    max_epochs=3            # number of epochs\n",
    "    )\n",
    "\n",
    "trainer.fit(model, data)\n",
    "\n",
    "trainer.test(model, datamodule=data)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956e1be-a0ff-4f71-af46-2af5a8bca845",
   "metadata": {},
   "source": [
    "# Parameter tuning sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde1018-ddfa-43f2-b9c6-f2c1d4b19736",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-11T06:37:39.031481Z",
     "iopub.status.idle": "2023-02-11T06:37:39.031983Z",
     "shell.execute_reply": "2023-02-11T06:37:39.031983Z",
     "shell.execute_reply.started": "2023-02-11T06:37:39.031983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"project\": \"mnist_bc\",\n",
    "    \"method\": \"bayes\",   # Random search\n",
    "    \"metric\": {           # We want to maximize val_acc\n",
    "        \"name\": \"val_acc\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"run_cap\": 5, #terminates the sweep after a number of runs\n",
    "    \"early_terminate\": { # only terminates a run early not the sweep (reduces computation time)\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 3\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        # \"n_layer_1\": {\n",
    "        #     # Choose from pre-defined values\n",
    "        #     \"values\": [32, 64, 128, 256, 512]\n",
    "        # },\n",
    "        # \"n_layer_2\": {\n",
    "        #     # Choose from pre-defined values\n",
    "        #     \"values\": [32, 64, 128, 256, 512, 1024]\n",
    "        # },\n",
    "        \"lr\": {\n",
    "            # log uniform distribution between exp(min) and exp(max)\n",
    "            \"distribution\": \"log_uniform\",\n",
    "            \"min\": -9.21,   # exp(-9.21) = 1e-4\n",
    "            \"max\": -4.61    # exp(-4.61) = 1e-2\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aeb8a0-9ffd-4439-808c-2951d1f74b36",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-11T06:37:39.032981Z",
     "iopub.status.idle": "2023-02-11T06:37:39.033483Z",
     "shell.execute_reply": "2023-02-11T06:37:39.033483Z",
     "shell.execute_reply.started": "2023-02-11T06:37:39.033483Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sweep_iteration():\n",
    "    # set up W&B logger\n",
    "    wandb.init()    # required to have access to `wandb.config`\n",
    "    wandb_logger = WandbLogger()\n",
    "\n",
    "    # setup data\n",
    "    # data = MNISTDataModule()\n",
    "    data = YogaDataModule()\n",
    "\n",
    "    # setup model - note how we refer to sweep parameters with wandb.config\n",
    "    # model = LitMNIST(\n",
    "    #     n_layer_1=wandb.config.n_layer_1,\n",
    "    #     n_layer_2=wandb.config.n_layer_2,\n",
    "    #     lr=wandb.config.lr\n",
    "    # )\n",
    "    model = YogaModel(lr=wandb.config.lr, n_classes=num_of_classes)\n",
    "\n",
    "    # setup Trainer\n",
    "    trainer = Trainer(\n",
    "        logger=wandb_logger,    # W&B integration\n",
    "        gpus=-1,                # use all GPU's\n",
    "        max_epochs=3            # number of epochs\n",
    "        )\n",
    "\n",
    "    # train\n",
    "    trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b0393-21cf-4fba-8efd-7678a852b697",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-11T06:37:39.034980Z",
     "iopub.status.idle": "2023-02-11T06:37:39.035482Z",
     "shell.execute_reply": "2023-02-11T06:37:39.035482Z",
     "shell.execute_reply.started": "2023-02-11T06:37:39.035482Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config)\n",
    "wandb.agent(sweep_id, function=sweep_iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
