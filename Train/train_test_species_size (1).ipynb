{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c2bbccd-6625-45aa-a798-2e80b0edf73d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold-1-Data length1:  [139, 120, 139, 138, 231, 60, 199, 312, 224, 269, 182, 70]\n",
      "fold-1-Data length2:  [323, 216, 278, 336, 490, 153, 340, 535, 417, 534, 401, 311]\n",
      "fold-1-Data length3:  [478, 336, 414, 463, 753, 241, 551, 755, 584, 912, 562, 513]\n",
      "fold-1-Data length4:  [652, 446, 575, 616, 1009, 316, 771, 1030, 817, 1137, 678, 757]\n",
      "fold-1-Data length5:  [862, 537, 723, 793, 1273, 406, 929, 1245, 982, 1383, 834, 987]\n",
      "fold-1-Data length6:  [1041, 607, 827, 961, 1578, 482, 1146, 1420, 1133, 1649, 1002, 1249]\n",
      "fold-1-Data length7:  [1217, 727, 827, 1114, 1865, 570, 1146, 1672, 1285, 1933, 1232, 1357]\n",
      "fold-1-Data length8:  [1433, 828, 827, 1219, 2203, 643, 1146, 1672, 1458, 2147, 1454, 1598]\n",
      "fold-1-Data length9:  [1615, 924, 827, 1379, 2455, 725, 1146, 1672, 1625, 2327, 1645, 1815]\n",
      "fold-1-Data length10:  [1771, 1064, 827, 1563, 2776, 825, 1146, 1672, 1625, 2562, 1645, 1906]\n",
      "fold-1-Data length11:  [1946, 1183, 827, 1701, 3086, 925, 1146, 1672, 1625, 2703, 1645, 1997]\n",
      "fold-1-Data length12:  [2146, 1309, 827, 1888, 3086, 1023, 1146, 1672, 1625, 2827, 1645, 2110]\n",
      "fold-1-Data length13:  [2318, 1309, 827, 2073, 3086, 1093, 1146, 1672, 1625, 2956, 1645, 2174]\n",
      "fold-1-Data length14:  [2479, 1309, 827, 2233, 3086, 1181, 1146, 1672, 1625, 3191, 1645, 2345]\n",
      "fold-1-Data length15:  [2680, 1309, 827, 2413, 3086, 1264, 1146, 1672, 1625, 3191, 1645, 2532]\n",
      "fold-1-Data length16:  [2852, 1309, 827, 2544, 3086, 1364, 1146, 1672, 1625, 3191, 1645, 2848]\n",
      "fold-1-Data length17:  [3059, 1309, 827, 2681, 3086, 1364, 1146, 1672, 1625, 3191, 1645, 2848]\n",
      "fold-1-Data length18:  [3280, 1309, 827, 2805, 3086, 1364, 1146, 1672, 1625, 3191, 1645, 2848]\n",
      "fold-1-Data length19:  [3280, 1309, 827, 2962, 3086, 1364, 1146, 1672, 1625, 3191, 1645, 2848]\n",
      "------------FINISHED----------\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import wandb\n",
    "import fastai\n",
    "import dill\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import PIL\n",
    "import numpy as np\n",
    "from fastai.vision.augment import cutout_gaussian\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.core import *\n",
    "from fastai.text.core import RegexLabeller\n",
    "from fastai.vision.utils import get_image_files\n",
    "from fastai.data.block import DataBlock\n",
    "from fastai.data.core import *\n",
    "from fastai.tabular.all import *\n",
    "from fastcore.foundation import L\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import notebook_login, push_to_hub_fastai, from_pretrained_fastai\n",
    "from torchvision.transforms import GaussianBlur\n",
    "# os.environ['WANDB_WATCH'] = 'false'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Train.ipynb'\n",
    "\n",
    "\n",
    "SPLIT_NUM = 20\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    batch_size=64,  #16, #256,\n",
    "    epochs=1,\n",
    "    lr=3e-3,\n",
    "    img_size=224, # 224, 256 for small model on huggingface\n",
    "    seed=42,\n",
    "    pretrained=True,\n",
    "    top_k_losses=5,\n",
    "    model_name=\"maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k\",# \"maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k\",# maxvit_nano_rw_256.sw_in1k for HF spaces\n",
    "    wandb_project=\"Beetle_classifier\", \n",
    "    wandb_group=\"ambrosia_symbiosis\",\n",
    "    job_type=\"training_cv_5fold_ooc\"\n",
    "    )\n",
    "\n",
    "# \n",
    "species_lst = ['Coccotypes_dactyliperda', 'Hylesinus_varius', 'Monarthrum_fasciatum',\n",
    "                'Phloeosinus_dentatus', 'Pityophthorus_juglandis', 'Platypus_cylindrus',\n",
    "                'Pycnarthrum_hispidium', 'Scolotodes_schwarzi', 'Xyleborinus_saxesenii',\n",
    "                'Xyleborus_affinis', 'Xylosandrus_compactus',\n",
    "                'Xylosandrus_crassiusculus']\n",
    "\n",
    "# Define a custom transform for Gaussian blur\n",
    "def gaussian_blur(x, p=0.5, kernel_size_min=3, kernel_size_max=20, sigma_min=0.1, sigma_max=3):\n",
    "    if x.ndim == 4:\n",
    "        for i in range(x.shape[0]):\n",
    "            if random.random() < p:\n",
    "                kernel_size = random.randrange(kernel_size_min, kernel_size_max + 1, 2)\n",
    "                sigma = random.uniform(sigma_min, sigma_max)\n",
    "                x[i] = GaussianBlur(kernel_size=kernel_size, sigma=sigma)(x[i])\n",
    "    return x\n",
    "\n",
    "# def custom_parent_label(fname, known_categories):\n",
    "#     category = parent_label(fname)\n",
    "#     if category not in known_categories:\n",
    "#         return \"Unknown\"\n",
    "#     else:\n",
    "#         return category\n",
    "\n",
    "# def get_categories(fnames):\n",
    "#     categories = []\n",
    "#     for fname in fnames:\n",
    "#         category = parent_label(fname)\n",
    "#         if category not in categories:\n",
    "#             categories.append(category)\n",
    "#     return categories\n",
    "\n",
    "def get_image_files_exclude(path, folders=('train','valid'), exclude_folder=None, data_len=None):\n",
    "    files = get_image_files(path=path, recurse=True, folders=folders)\n",
    "    if exclude_folder:\n",
    "            files = L([f for f in files if (exclude_folder not in str(f))])\n",
    "    if data_len:\n",
    "        # filter files by size of data\n",
    "        files_1 = [f for f in files if ('Coccotypes_dactyliperda' not in str(f))]\n",
    "        files_temp = [f for f in files if ('Coccotypes_dactyliperda' in str(f))]\n",
    "        vial_subset_lst = list(set([re.search(r'.*_([^_]+_.*[^_]+)_.*_.*', str(path)).group(1) for path in files_temp]))\n",
    "        random.seed(config.seed)\n",
    "        random.shuffle(vial_subset_lst)\n",
    "        files_2 = []\n",
    "        for vial_sub in vial_subset_lst[:data_len]:\n",
    "            files_vial_sub = [f for f in files if (vial_sub in str(f))]\n",
    "            files_2 = files_2 + files_vial_sub\n",
    "        print(\"Number of images: \", len(files_2))\n",
    "        files =  L(files_1 + files_2)\n",
    "    return files\n",
    "    \n",
    "\n",
    "def get_images(dataset_path, batch_size, img_size, seed, subfolders=('train','valid'), exclude_folder=None, data_len=None):\n",
    "    \"The beetles dataset\"\n",
    "    files = get_image_files_exclude(path=dataset_path, folders=subfolders, exclude_folder=exclude_folder, data_len=data_len)\n",
    "    # files = get_image_files(path=dataset_path, recurse=True, folders=subfolders)\n",
    "    transforms = aug_transforms(    # transformatiosn that are only applied ot training and not inference\n",
    "                           batch=False,\n",
    "                           pad_mode='zeros',\n",
    "                           size=img_size,\n",
    "                           p_affine=0.8,\n",
    "                           p_lighting=0.8,\n",
    "                           max_rotate=360.0,\n",
    "                           mult=1.0, \n",
    "                           do_flip=True, \n",
    "                           flip_vert=False,\n",
    "                           min_zoom=1.0,\n",
    "                           max_zoom=1.1, \n",
    "                           max_lighting=0.75,\n",
    "                           max_warp=0.2, \n",
    "                           mode='bilinear', \n",
    "                           align_corners=True,\n",
    "                           min_scale=1.0,\n",
    "                           xtra_tfms=[RandomErasing(p=0.8, max_count=5, sh=0.25)]) # this adds random erasing to entire batches\n",
    "    transforms.append(partial(gaussian_blur, p=0.8))\n",
    "    transforms.append(Normalize.from_stats(*imagenet_stats))\n",
    "    # categories = get_categories(files)\n",
    "    dblock = DataBlock(blocks = (ImageBlock, CategoryBlock),#(vocab=categories+[\"Unknown\"])),\n",
    "                       get_items = partial(get_image_files_exclude, \n",
    "                                           folders=subfolders, \n",
    "                                           exclude_folder=exclude_folder),\n",
    "                       # get_items = get_image_files,\n",
    "                       splitter = GrandparentSplitter(train_name=subfolders[0], valid_name=subfolders[1]),\n",
    "                       get_y = parent_label, # partial(custom_parent_label, known_categories=categories),\n",
    "                       item_tfms = Resize(img_size, ResizeMethod.Pad, pad_mode='zeros'), # resize trasnformation is applied during inference too                                    \n",
    "                       batch_tfms = transforms)\n",
    "    dls = dblock.dataloaders(dataset_path, bs = batch_size, num_workers=4)\n",
    "    return dls\n",
    "\n",
    "def train(config, dataset_path, subfolders=('train','valid'), exclude_folder=None, data_len=None):\n",
    "    \"Train the model using the supplied config\"\n",
    "    dls = get_images(dataset_path=dataset_path, \n",
    "                     batch_size=config.batch_size, \n",
    "                     img_size=config.img_size, \n",
    "                     seed=config.seed, \n",
    "                     subfolders=subfolders, \n",
    "                     exclude_folder=exclude_folder,\n",
    "                     data_len=data_len)\n",
    "    labels = np.array([re.split(r'/|\\\\', str(x))[-2] for x in dls.items])\n",
    "    classes = np.unique(labels)# for label in labels if label != \"Unknown\"])\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weights = {c: w for c, w in zip(classes, weights)}\n",
    "    weights = tensor([class_weights[c] for c in dls.vocab]).to(dls.device)\n",
    "    # wandb.init(project=config.wandb_project, group=config.wandb_group, job_type=config.job_type, config=config) # it is a good idea to keep these functions out of the training function due to some exporting issues\n",
    "    cbs = [MixedPrecision(), ShowGraphCallback(), SaveModelCallback(), WandbCallback(log='gradients')] # (all, parameters, gradients or None) parameters and all does nto work currently wandb needs to be updated\n",
    "    learn = vision_learner(dls, \n",
    "                           config.model_name, \n",
    "                           loss_func=LabelSmoothingCrossEntropy(weight=weights), # this fucntion is used for class imbalance it is a regularization technique # LabelSmoothingCrossEntropyFlat is used for multi dimensional data\n",
    "                           metrics=[error_rate, \n",
    "                                    accuracy, \n",
    "                                    top_k_accuracy], \n",
    "                           cbs=cbs, \n",
    "                           pretrained=config.pretrained)\n",
    "    learn.fine_tune(config.epochs, base_lr=config.lr)\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    interp.plot_confusion_matrix()\n",
    "    interp.plot_top_losses(config.top_k_losses, nrows=config.top_k_losses)\n",
    "    # wandb.finish() # it is a good idea to keep these functions out of the training function due to some exporting issues\n",
    "    return learn\n",
    "\n",
    "# this function only describes how much a singular value in al ist stands out.\n",
    "# if all values in the lsit are high or low this is 1\n",
    "# the smaller the proportiopn of number of disimilar vlaues are to other more similar values the lower this number\n",
    "# the larger the gap between the dissimilar numbers and the simialr number the smaller this number\n",
    "# only able to interpret probabilities or values between 0 and 1\n",
    "# this function outputs an estimate an inverse of the classification confidence based on the probabilities of all the classes.\n",
    "# the wedge threshold splits the data on a threshold with a magnitude of a positive int to force a ledge/peak in the data\n",
    "def unknown_prob_calc(probs, wedge_threshold, wedge_magnitude=1, wedge='strict'):\n",
    "    if wedge =='strict':\n",
    "        increase_var = (1/(wedge_magnitude))\n",
    "        decrease_var = (wedge_magnitude)\n",
    "    if wedge =='dynamic': # this allows pointsthat are furhter from the threshold ot be moved less and points clsoer to be moved more\n",
    "        increase_var = (1/(wedge_magnitude*((1-np.abs(probs-wedge_threshold)))))\n",
    "        decrease_var = (wedge_magnitude*((1-np.abs(probs-wedge_threshold))))\n",
    "    # else:\n",
    "    #     print(\"Error: use 'strict' (default) or 'dynamic' as options for the wedge parameter!\")\n",
    "    probs = np.where(probs>=wedge_threshold , probs**increase_var, probs)\n",
    "    probs = np.where(probs<=wedge_threshold , probs**decrease_var, probs)\n",
    "    diff_matrix = np.abs(probs[:, np.newaxis] - probs)\n",
    "    diff_matrix_sum = np.sum(diff_matrix)\n",
    "    probs_sum = np.sum(probs)\n",
    "    class_val = (diff_matrix_sum/probs_sum)\n",
    "    max_class_val = ((len(probs)-1)*2)\n",
    "    kown_prob = class_val/max_class_val\n",
    "    unknown_prob = 1-kown_prob\n",
    "    return(unknown_prob)\n",
    "\n",
    "########################################################################################################################################\n",
    "# # save the number of images used in each \n",
    "# results_df = pd.DataFrame()\n",
    "# for i in range(1,6):\n",
    "#     img_num_lst = []\n",
    "#     for data_len in range(1,20):\n",
    "#         dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/kfold_images/train_data\"\n",
    "#         files = get_image_files_exclude(path=dataset_path, folders=('train_'+str(i)), exclude_folder=None)\n",
    "\n",
    "#         # filter files by size of data\n",
    "#         files_1 = [f for f in files if ('Coccotypes_dactyliperda' not in str(f))]\n",
    "#         files_temp = [f for f in files if ('Coccotypes_dactyliperda' in str(f))]\n",
    "#         vial_subset_lst = list(set([re.search(r'.*_([^_]+_.*[^_]+)_.*_.*', str(path)).group(1) for path in files_temp]))\n",
    "#         random.seed(42)\n",
    "#         random.shuffle(vial_subset_lst)\n",
    "#         files_2 = []\n",
    "#         for vial_sub in vial_subset_lst[:data_len]:\n",
    "#             files_vial_sub = [f for f in files if (vial_sub in str(f))]\n",
    "#             files_2 = set(files_2 + files_vial_sub)\n",
    "#             files_2 = list(files_2)\n",
    "#         img_num_lst.append(len(files_2))\n",
    "#         files =  L(files_1 + files_2)\n",
    "#     results_df[\"fold-\"+str(i)+\"_image_number\"] = img_num_lst\n",
    "# results_df.to_csv(\"data_size_results.csv\")\n",
    "\n",
    "# save the number of images used in each \n",
    "dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/kfold_images/train_data\"\n",
    "results_df = pd.DataFrame()\n",
    "fold_results_dict = {}\n",
    "files_1 = []\n",
    "for i in range(1,2):\n",
    "    for data_len in range(1,SPLIT_NUM):\n",
    "        img_num_lst = []\n",
    "        files_1 = []\n",
    "        for species in species_lst:\n",
    "            files_ext = get_image_files_exclude(path=dataset_path, folders=('train_'+str(i)), exclude_folder=None)\n",
    "            # filter files by size of data\n",
    "            files_temp = [f for f in files_ext if (species in str(f))]\n",
    "            vial_subset_lst = list(set([re.search(r'.*_([^_]+_.*[^_]+)_.*_.*', str(path)).group(1) for path in files_temp]))\n",
    "            random.seed(42)\n",
    "            random.shuffle(vial_subset_lst)\n",
    "            files_2 = []\n",
    "            for vial_sub in vial_subset_lst[:data_len]:\n",
    "                files_vial_sub = [f for f in files_ext if (vial_sub+\"_\" in str(f))]\n",
    "                files_2 = set(files_2 + files_vial_sub)\n",
    "                files_2 = list(files_2)\n",
    "            img_num_lst.append(len(files_2))\n",
    "            files_1 = files_1 + files_2\n",
    "        files = L(files_1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"fold-\"+str(i)+\"-Data length\"+str(data_len)+\": \", img_num_lst)    \n",
    "        results_df[\"data_len_-\"+str(data_len)] = img_num_lst\n",
    "    results_df['species'] = species_lst\n",
    "    fold_results_dict['fold_'+str(i)] = results_df\n",
    "    results_df.to_csv(\"data_size_results_fold-\"+str(i)+\".csv\")\n",
    "# save dict as pickle file        \n",
    "with open('fold_results_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(fold_results_dict, f)\n",
    "\n",
    "# Training\n",
    "# for i in range(1,6):\n",
    "#     for data_len in range(1, SPLIT_NUM):\n",
    "#         print(\"Training: \", str(i))\n",
    "#         print(\"Data length: \", str(data_len))\n",
    "#         if not os.path.isfile(str(data_len)+\"_Testing_prediction_probabilities_fold-\"+str(i)+\".csv\"):\n",
    "#             print(\"Not tested\")\n",
    "#             # Train Model\n",
    "#             dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/kfold_images/train_data\"\n",
    "#             # dataset_path = r\"F:\\Beetle_data\\kfold_images\\train_data\"\n",
    "#             wandb.init(project=config.wandb_project, group=config.wandb_group, job_type=config.job_type, config=config)\n",
    "#             learn = train(config=config, \n",
    "#                           dataset_path=dataset_path, \n",
    "#                           subfolders=('train_'+str(i),'valid_'+str(i)),\n",
    "#                           exclude_folder=None, \n",
    "#                           data_len=data_len)\n",
    "#             wandb.finish()\n",
    "\n",
    "#             # get predicstions and labels\n",
    "#             print(\"Testing: \", str(i))\n",
    "#             learn.remove_cb(WandbCallback)\n",
    "#             # Get validation labels and predictions\n",
    "#             files = get_image_files_exclude(path=dataset_path, folders=('valid_'+str(i)), exclude_folder=None, data_len=None) \n",
    "#             test_dl = learn.dls.test_dl(files, with_labels=True) # load data as a dataloader\n",
    "#             preds, targets = learn.get_preds(dl=test_dl)\n",
    "#             pred_df = pd.DataFrame(preds.cpu().numpy(), columns=learn.dls.vocab)\n",
    "#             pred_df.to_csv(str(data_len)+\"_Validation_prediction_probabilities_fold-\"+str(i)+\".csv\")\n",
    "\n",
    "#             # testing data\n",
    "#             test_dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/kfold_images\"\n",
    "#             files = get_image_files_exclude(path=test_dataset_path, folders=('test_data'), exclude_folder=None, data_len=None) \n",
    "#             test_dl = learn.dls.test_dl(files, with_labels=True) # load data as a dataloader\n",
    "#             preds, targets = learn.get_preds(dl=test_dl)\n",
    "#             pred_df = pd.DataFrame(preds.cpu().numpy(), columns=learn.dls.vocab)\n",
    "#             # pred_df.to_csv(str(data_len)+\"_Testing_prediction_probabilities_fold-\"+str(i)+\".csv\")\n",
    "#         else:\n",
    "#             print(\"Already tested\")\n",
    "print(\"------------FINISHED----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4bccbae-8478-4e01-a62d-ef89267e3753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = r\"/blue/hulcr/gmarais/Beetle_data/kfold_images\"#/train_data\"\n",
    "# files = get_image_files_exclude(path=dataset_path, folders=('train_'+str(1)), exclude_folder=None, data_len=None) \n",
    "files = get_image_files_exclude(path=dataset_path, folders=('test_data'), exclude_folder=None, data_len=None) \n",
    "files_temp = [f for f in files if ('Coccotypes_dactyliperda' in str(f))]\n",
    "len(files_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11f736b2-b197-4187-a539-783995a63d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24955"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BC_310",
   "language": "python",
   "name": "bc_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
